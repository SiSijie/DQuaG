{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b91df9ab-79c4-46e0-a477-ac62672be489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 688736 entries, 0 to 688735\n",
      "Data columns (total 18 columns):\n",
      " #   Column  Non-Null Count   Dtype  \n",
      "---  ------  --------------   -----  \n",
      " 0   0       688736 non-null  float64\n",
      " 1   1       688736 non-null  float64\n",
      " 2   2       688736 non-null  float64\n",
      " 3   3       688736 non-null  float64\n",
      " 4   4       688736 non-null  float64\n",
      " 5   5       688736 non-null  float64\n",
      " 6   6       688736 non-null  float64\n",
      " 7   7       688736 non-null  float64\n",
      " 8   8       688736 non-null  float64\n",
      " 9   9       688736 non-null  float64\n",
      " 10  10      688736 non-null  float64\n",
      " 11  11      688736 non-null  float64\n",
      " 12  12      688736 non-null  float64\n",
      " 13  13      688736 non-null  float64\n",
      " 14  14      688736 non-null  float64\n",
      " 15  15      688736 non-null  float64\n",
      " 16  16      688736 non-null  float64\n",
      " 17  17      688736 non-null  float64\n",
      "dtypes: float64(18)\n",
      "memory usage: 94.6 MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# 加载数据\n",
    "file_path_clean = '/home/sdong/data/chicago_bicycle/data_pr_cleaned_embeddings.csv'\n",
    "file_path_origi = '/home/sdong/data/chicago_bicycle/data_pr_raw_embeddings.csv'\n",
    "data = pd.read_csv(file_path_clean)\n",
    "data_dirty = pd.read_csv(file_path_origi)\n",
    "# 设置显示所有列和部分行\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 10)  \n",
    "# 显示数据集的前几行和数据结构\n",
    "\n",
    "print(data_dirty.info())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27dc078d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean data values are within [0, 1]: False\n",
      "Dirty data values are within [0, 1]: False\n",
      "Clean data contains values out of range [0, 1].\n",
      "Dirty data contains values out of range [0, 1].\n",
      "Out of range values in clean data:\n",
      "          0   1   2   3   4   5   6   7   8   9  10  11  12  13   14   15  16  \\\n",
      "60400   1.0 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  NaN  NaN NaN   \n",
      "109362  NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  1.0  NaN NaN   \n",
      "244595  NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  NaN  1.0 NaN   \n",
      "\n",
      "        17  \n",
      "60400  NaN  \n",
      "109362 NaN  \n",
      "244595 NaN  \n",
      "Out of range values in dirty data:\n",
      "         0    1   2   3   4   5   6   7   8    9  10   11  12  13  14  15  16  \\\n",
      "145205 NaN  NaN NaN NaN NaN NaN NaN NaN NaN  NaN NaN  1.0 NaN NaN NaN NaN NaN   \n",
      "257843 NaN  1.0 NaN NaN NaN NaN NaN NaN NaN  NaN NaN  NaN NaN NaN NaN NaN NaN   \n",
      "560525 NaN  NaN NaN NaN NaN NaN NaN NaN NaN  1.0 NaN  NaN NaN NaN NaN NaN NaN   \n",
      "\n",
      "        17  \n",
      "145205 NaN  \n",
      "257843 NaN  \n",
      "560525 NaN  \n"
     ]
    }
   ],
   "source": [
    "# 检查两个数据集中是否所有值都在0到1之间\n",
    "def check_values_in_range(df, lower=0, upper=1):\n",
    "    return ((df >= lower) & (df <= upper)).all().all()\n",
    "\n",
    "is_data_in_range = check_values_in_range(data)\n",
    "is_data_dirty_in_range = check_values_in_range(data_dirty)\n",
    "\n",
    "print(f\"Clean data values are within [0, 1]: {is_data_in_range}\")\n",
    "print(f\"Dirty data values are within [0, 1]: {is_data_dirty_in_range}\")\n",
    "\n",
    "if not is_data_in_range:\n",
    "    print(\"Clean data contains values out of range [0, 1].\")\n",
    "\n",
    "if not is_data_dirty_in_range:\n",
    "    print(\"Dirty data contains values out of range [0, 1].\")\n",
    "\n",
    "# 如果需要，打印出不在范围内的值和对应的索引\n",
    "def find_out_of_range_values(df, lower=0, upper=1):\n",
    "    out_of_range = df[(df < lower) | (df > upper)]\n",
    "    return out_of_range.dropna(how='all')\n",
    "\n",
    "if not is_data_in_range:\n",
    "    out_of_range_clean = find_out_of_range_values(data)\n",
    "    print(\"Out of range values in clean data:\")\n",
    "    print(out_of_range_clean)\n",
    "\n",
    "if not is_data_dirty_in_range:\n",
    "    out_of_range_dirty = find_out_of_range_values(data_dirty)\n",
    "    print(\"Out of range values in dirty data:\")\n",
    "    print(out_of_range_dirty)\n",
    "    \n",
    "    # 获取不在范围内的值的索引\n",
    "out_of_range_clean_indices = out_of_range_clean.index\n",
    "out_of_range_dirty_indices = out_of_range_dirty.index\n",
    "\n",
    "# 删除不在范围内的行\n",
    "data = data.drop(out_of_range_clean_indices)\n",
    "data_dirty = data_dirty.drop(out_of_range_dirty_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14966a57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 688733 entries, 0 to 688735\n",
      "Data columns (total 18 columns):\n",
      " #   Column  Non-Null Count   Dtype  \n",
      "---  ------  --------------   -----  \n",
      " 0   0       688733 non-null  float64\n",
      " 1   1       688733 non-null  float64\n",
      " 2   2       688733 non-null  float64\n",
      " 3   3       688733 non-null  float64\n",
      " 4   4       688733 non-null  float64\n",
      " 5   5       688733 non-null  float64\n",
      " 6   6       688733 non-null  float64\n",
      " 7   7       688733 non-null  float64\n",
      " 8   8       688733 non-null  float64\n",
      " 9   9       688733 non-null  float64\n",
      " 10  10      688733 non-null  float64\n",
      " 11  11      688733 non-null  float64\n",
      " 12  12      688733 non-null  float64\n",
      " 13  13      688733 non-null  float64\n",
      " 14  14      688733 non-null  float64\n",
      " 15  15      688733 non-null  float64\n",
      " 16  16      688733 non-null  float64\n",
      " 17  17      688733 non-null  float64\n",
      "dtypes: float64(18)\n",
      "memory usage: 99.8 MB\n"
     ]
    }
   ],
   "source": [
    "data_dirty.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5887b650",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 假设 data 已经是一个经过预处理的 DataFrame\n",
    "data_array = data.values.astype(np.float32)  # 转换为浮点数类型的 NumPy 数组\n",
    "\n",
    "# 分割数据为训练集和临时测试集（包括真正的测试集和验证集）\n",
    "train_data, val_test_data = train_test_split(data_array, test_size=0.5, random_state=42)\n",
    "\n",
    "# 将训练验证集进一步分割为训练集和验证集\n",
    "val_data, test_data = train_test_split(val_test_data, test_size=0.5, random_state=42)  # 0.25 x 0.8 = 0.2\n",
    "\n",
    "\n",
    "# 转换为PyTorch张量\n",
    "\n",
    "\n",
    "# 创建数据加载器\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "batch_size = 32  # 或者任何适合你GPU的大小\n",
    "\n",
    "train_tensor = torch.tensor(train_data) #0.6\n",
    "train_dataset = TensorDataset(train_tensor, train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "val_tensor = torch.tensor(val_data) #0.2\n",
    "val_dataset = TensorDataset(val_tensor, val_tensor)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "test_tensor = torch.tensor(test_data)  #20%\n",
    "test_dataset = TensorDataset(test_tensor, test_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)  # 50个批次\n",
    "#len(test_dataset)// 50\n",
    "\n",
    "data_dirty_array = data_dirty.values.astype(np.float32)  # 转换为浮点数类型的 NumPy 数组\n",
    "test_dirty_tensor = torch.tensor(data_dirty_array)  #20%\n",
    "test_dirty_dataset = TensorDataset(test_dirty_tensor, test_dirty_tensor)\n",
    "test_dirty_loader = DataLoader(test_dirty_dataset, batch_size=1, shuffle=False)  # 50个批次\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3d31c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92722c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 检查数据中是否有NaN或无穷大的值\n",
    "# if torch.isnan(train_tensor).any() or torch.isinf(train_tensor).any():\n",
    "#     print(\"Data contains NaNs or Infs.\")\n",
    "# # 检查数据中是否有NaN或无穷大的值\n",
    "# if torch.isnan(test_dirty_tensor).any() or torch.isinf(test_dirty_tensor).any():\n",
    "#     print(\"Data contains NaNs or Infs.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63a8f2af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAE(\n",
      "  (fc1): Linear(in_features=18, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc31): Linear(in_features=64, out_features=20, bias=True)\n",
      "  (fc32): Linear(in_features=64, out_features=20, bias=True)\n",
      "  (fc4): Linear(in_features=20, out_features=64, bias=True)\n",
      "  (fc5): Linear(in_features=64, out_features=128, bias=True)\n",
      "  (fc6): Linear(in_features=128, out_features=18, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "        # Encoder\n",
    "        self.fc1 = nn.Linear(18, 128)  # Input layer\n",
    "        self.fc2 = nn.Linear(128, 64)  # Hidden layer\n",
    "        self.fc31 = nn.Linear(64, 20)  # Output layer for mu\n",
    "        self.fc32 = nn.Linear(64, 20)  # Output layer for logvar\n",
    "\n",
    "        # Decoder\n",
    "        self.fc4 = nn.Linear(20, 64)   # Input layer\n",
    "        self.fc5 = nn.Linear(64, 128)  # Hidden layer\n",
    "        self.fc6 = nn.Linear(128, 18)  # Output layer\n",
    "\n",
    "    def encode(self, x):\n",
    "        h1 = F.relu(self.fc1(x))\n",
    "        h2 = F.relu(self.fc2(h1))\n",
    "        return self.fc31(h2), self.fc32(h2)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar) + 1e-8  # Adding a small constant for numerical stability\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z):\n",
    "        h3 = F.relu(self.fc4(z))\n",
    "        h4 = F.relu(self.fc5(h3))\n",
    "        return torch.sigmoid(self.fc6(h4))  # Use sigmoid to ensure output is between 0 and 1\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n",
    "\n",
    "# Instantiate the model\n",
    "model = VAE()\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1d03c39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sdong/miniconda3/envs/mainenv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#device = torch.device(\"cpu\")\n",
    "model = VAE().to(device)\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "# 设置优化器\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# 定义损失函数\n",
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    # 确保目标张量也是浮点类型且维度匹配\n",
    "    recon_x = torch.clamp(recon_x, 0, 1)  # 确保输出值在[0, 1]范围内\n",
    "    BCE = F.binary_cross_entropy(recon_x, x.view(-1, 18).float(), reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return BCE, KLD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c4a9e966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/237379 (0%)]\tLoss: 12.641626\n",
      "Train Epoch: 1 [3200/237379 (1%)]\tLoss: 11.768671\n",
      "Train Epoch: 1 [6400/237379 (3%)]\tLoss: 11.752696\n",
      "Train Epoch: 1 [9600/237379 (4%)]\tLoss: 11.735062\n",
      "Train Epoch: 1 [12800/237379 (5%)]\tLoss: 11.865826\n",
      "Train Epoch: 1 [16000/237379 (7%)]\tLoss: 11.696664\n",
      "Train Epoch: 1 [19200/237379 (8%)]\tLoss: 11.676301\n",
      "Train Epoch: 1 [22400/237379 (9%)]\tLoss: 11.756909\n",
      "Train Epoch: 1 [25600/237379 (11%)]\tLoss: 11.834959\n",
      "Train Epoch: 1 [28800/237379 (12%)]\tLoss: 11.841856\n",
      "Train Epoch: 1 [32000/237379 (13%)]\tLoss: 11.634074\n",
      "Train Epoch: 1 [35200/237379 (15%)]\tLoss: 11.895435\n",
      "Train Epoch: 1 [38400/237379 (16%)]\tLoss: 11.775717\n",
      "Train Epoch: 1 [41600/237379 (18%)]\tLoss: 11.761427\n",
      "Train Epoch: 1 [44800/237379 (19%)]\tLoss: 11.709703\n",
      "Train Epoch: 1 [48000/237379 (20%)]\tLoss: 11.833692\n",
      "Train Epoch: 1 [51200/237379 (22%)]\tLoss: 11.925179\n",
      "Train Epoch: 1 [54400/237379 (23%)]\tLoss: 11.657171\n",
      "Train Epoch: 1 [57600/237379 (24%)]\tLoss: 11.573464\n",
      "Train Epoch: 1 [60800/237379 (26%)]\tLoss: 11.870288\n",
      "Train Epoch: 1 [64000/237379 (27%)]\tLoss: 11.707285\n",
      "Train Epoch: 1 [67200/237379 (28%)]\tLoss: 11.778245\n",
      "Train Epoch: 1 [70400/237379 (30%)]\tLoss: 11.755736\n",
      "Train Epoch: 1 [73600/237379 (31%)]\tLoss: 11.978827\n",
      "Train Epoch: 1 [76800/237379 (32%)]\tLoss: 11.791167\n",
      "Train Epoch: 1 [80000/237379 (34%)]\tLoss: 11.702696\n",
      "Train Epoch: 1 [83200/237379 (35%)]\tLoss: 11.844002\n",
      "Train Epoch: 1 [86400/237379 (36%)]\tLoss: 11.744327\n",
      "Train Epoch: 1 [89600/237379 (38%)]\tLoss: 11.872304\n",
      "Train Epoch: 1 [92800/237379 (39%)]\tLoss: 11.705982\n",
      "Train Epoch: 1 [96000/237379 (40%)]\tLoss: 11.615985\n",
      "Train Epoch: 1 [99200/237379 (42%)]\tLoss: 11.660406\n",
      "Train Epoch: 1 [102400/237379 (43%)]\tLoss: 11.775317\n",
      "Train Epoch: 1 [105600/237379 (44%)]\tLoss: 11.696765\n",
      "Train Epoch: 1 [108800/237379 (46%)]\tLoss: 11.637815\n",
      "Train Epoch: 1 [112000/237379 (47%)]\tLoss: 11.690351\n",
      "Train Epoch: 1 [115200/237379 (49%)]\tLoss: 11.755104\n",
      "Train Epoch: 1 [118400/237379 (50%)]\tLoss: 11.725368\n",
      "Train Epoch: 1 [121600/237379 (51%)]\tLoss: 11.722319\n",
      "Train Epoch: 1 [124800/237379 (53%)]\tLoss: 11.666324\n",
      "Train Epoch: 1 [128000/237379 (54%)]\tLoss: 11.695717\n",
      "Train Epoch: 1 [131200/237379 (55%)]\tLoss: 11.755495\n",
      "Train Epoch: 1 [134400/237379 (57%)]\tLoss: 11.819348\n",
      "Train Epoch: 1 [137600/237379 (58%)]\tLoss: 11.776029\n",
      "Train Epoch: 1 [140800/237379 (59%)]\tLoss: 11.761384\n",
      "Train Epoch: 1 [144000/237379 (61%)]\tLoss: 11.844076\n",
      "Train Epoch: 1 [147200/237379 (62%)]\tLoss: 11.814754\n",
      "Train Epoch: 1 [150400/237379 (63%)]\tLoss: 11.785653\n",
      "Train Epoch: 1 [153600/237379 (65%)]\tLoss: 11.773907\n",
      "Train Epoch: 1 [156800/237379 (66%)]\tLoss: 11.776390\n",
      "Train Epoch: 1 [160000/237379 (67%)]\tLoss: 11.776835\n",
      "Train Epoch: 1 [163200/237379 (69%)]\tLoss: 11.857821\n",
      "Train Epoch: 1 [166400/237379 (70%)]\tLoss: 11.655750\n",
      "Train Epoch: 1 [169600/237379 (71%)]\tLoss: 11.682403\n",
      "Train Epoch: 1 [172800/237379 (73%)]\tLoss: 11.711018\n",
      "Train Epoch: 1 [176000/237379 (74%)]\tLoss: 11.716426\n",
      "Train Epoch: 1 [179200/237379 (75%)]\tLoss: 11.694780\n",
      "Train Epoch: 1 [182400/237379 (77%)]\tLoss: 11.764812\n",
      "Train Epoch: 1 [185600/237379 (78%)]\tLoss: 11.903761\n",
      "Train Epoch: 1 [188800/237379 (80%)]\tLoss: 11.786679\n",
      "Train Epoch: 1 [192000/237379 (81%)]\tLoss: 11.746643\n",
      "Train Epoch: 1 [195200/237379 (82%)]\tLoss: 11.732814\n",
      "Train Epoch: 1 [198400/237379 (84%)]\tLoss: 11.739239\n",
      "Train Epoch: 1 [201600/237379 (85%)]\tLoss: 11.763345\n",
      "Train Epoch: 1 [204800/237379 (86%)]\tLoss: 11.823027\n",
      "Train Epoch: 1 [208000/237379 (88%)]\tLoss: 11.824551\n",
      "Train Epoch: 1 [211200/237379 (89%)]\tLoss: 11.715459\n",
      "Train Epoch: 1 [214400/237379 (90%)]\tLoss: 11.706431\n",
      "Train Epoch: 1 [217600/237379 (92%)]\tLoss: 11.744543\n",
      "Train Epoch: 1 [220800/237379 (93%)]\tLoss: 11.647982\n",
      "Train Epoch: 1 [224000/237379 (94%)]\tLoss: 11.684549\n",
      "Train Epoch: 1 [227200/237379 (96%)]\tLoss: 11.885762\n",
      "Train Epoch: 1 [230400/237379 (97%)]\tLoss: 11.562406\n",
      "Train Epoch: 1 [233600/237379 (98%)]\tLoss: 11.716620\n",
      "Train Epoch: 1 [236800/237379 (100%)]\tLoss: 11.763771\n",
      "Epoch: 1 Average BCE: 11.761324358538658 Average KLD: 0.0009563937635409178 Total Loss: 11.762280756426684\n",
      "Model saved to vae_model_bicycle_graph.pth\n",
      "CUDA cache cleared.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# 定义训练函数\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    #train_loss = 0\n",
    "    total_BCE = 0\n",
    "    total_KLD = 0\n",
    "    total_loss = 0\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):  # 由于使用TensorDataset，数据被重复用作输入和标签\n",
    "        data = data.to(device)\n",
    "        # 检查输入数据的范围\n",
    "        if (data < 0).any() or (data > 1).any():\n",
    "            raise ValueError(\"Input data contains values out of range [0, 1]\")\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        if (recon_batch < 0).any() or (recon_batch > 1).any():\n",
    "            raise ValueError(\"Warning: recon_batch contains values out of range [0, 1]\")\n",
    "        BCE, KLD = loss_function(recon_batch, data, mu, logvar)\n",
    "        loss = BCE + KLD\n",
    "        loss.backward()\n",
    "        total_loss += loss.item()\n",
    "        total_BCE += BCE.item()\n",
    "        total_KLD += KLD.item()\n",
    "        optimizer.step()\n",
    "        # if epoch == 1:  # 只在第一个epoch检查\n",
    "        #     print(\"Sample recon_x:\", recon_batch[0].data)\n",
    "        #     print(\"Sample x:\", data[0].data)\n",
    "\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item() / len(data)))\n",
    "    #print('====> Epoch: {} Average loss: {:.4f}'.format(epoch, train_loss / len(train_loader.dataset)))\n",
    "    print(f'Epoch: {epoch} Average BCE: {total_BCE / len(train_loader.dataset)} Average KLD: {total_KLD / len(train_loader.dataset)} Total Loss: {total_loss / len(train_loader.dataset)}')\n",
    "\n",
    "# 训练模型\n",
    "num_epochs = 1  # 可根据需要调整\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    train(epoch)\n",
    "\n",
    "# 保存模型的状态字典\n",
    "torch.save(model.state_dict(), 'vae_model_bicycle_graph.pth')\n",
    "\n",
    "print(\"Model saved to vae_model_bicycle_graph.pth\")\n",
    "\n",
    "# 添加调试信息\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.synchronize()\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"CUDA cache cleared.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b6b9ddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118690\n",
      "118690\n",
      "688733\n",
      "Average loss on validation data: 11.761952425959699\n",
      "Average loss on test data: 11.758920027615297\n",
      "Average loss on test dirty data: 12.037338681076745\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, data_loader):\n",
    "    model.eval()  # 切换到评估模式\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():  # 关闭梯度计算\n",
    "        for inputs, _ in data_loader:  # 假设 data_loader 返回 inputs 和 targets，这里我们不需要 targets\n",
    "            inputs = inputs.to(device)  # 确保将 inputs 转移到正确的设备\n",
    "            recon, mu, logvar = model(inputs)\n",
    "            BCE, KLD = loss_function(recon, inputs, mu, logvar)\n",
    "            loss = BCE + KLD  # 将损失元组中的元素相加\n",
    "            total_loss += loss.item()  # 现在这是一个单一的数值\n",
    "    print(len(data_loader.dataset))\n",
    "    return total_loss / len(data_loader.dataset)\n",
    "\n",
    "\n",
    "model = VAE().to(device)\n",
    "model.load_state_dict(torch.load('vae_model_bicycle_graph.pth'))\n",
    "# 计算测试集上的平均损失\n",
    "# 计算测试集和验证集上的平均损失\n",
    "val_loss = evaluate_model(model, val_loader)\n",
    "test_loss = evaluate_model(model, test_loader)\n",
    "test_dirty_loss = evaluate_model(model, test_dirty_loader)\n",
    "print(f\"Average loss on validation data: {val_loss}\")\n",
    "print(f\"Average loss on test data: {test_loss}\")\n",
    "print(f\"Average loss on test dirty data: {test_dirty_loss}\")\n",
    "# 简单的基于阈值的数据质量问题判断\n",
    "# 这里我们需要设置一个阈值来决定什么样的重构误差被认为是“异常”的，此阈值可以基于训练集或验证集的性能来确定\n",
    "# 假设我们根据验证集确定阈值\n",
    "# threshold = np.quantile([loss_function(model(recon, data.to(device), mu, logvar).item() for data, _ in val_loader], 0.95)\n",
    "# print(f\"Loss threshold for detecting data quality issues: {threshold}\")\n",
    "\n",
    "# # 判断测试集\n",
    "# quality_issues = test_loss > threshold\n",
    "# print(f\"Data quality issues detected: {quality_issues}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8950bc94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss threshold for detecting data quality issues: 12.56061749458313\n",
      "Min validation error: 10.639154434204102\n",
      "Max validation error: 14.13665771484375\n",
      "Mean validation error: 11.761976583561024\n",
      "95th percentile of validation errors: 12.56061749458313\n",
      "Maximum validation error (100th percentile): 14.13665771484375\n"
     ]
    }
   ],
   "source": [
    "def collect_reconstruction_errors(model, data_loader):\n",
    "    model.eval()\n",
    "    reconstruction_errors = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, _ in data_loader:  # 假设 data_loader 返回的是 inputs 和 labels，这里我们忽略 labels\n",
    "            inputs = inputs.to(device)  # 将输入数据移动到正确的设备\n",
    "            recon, mu, logvar = model(inputs)\n",
    "            BCE, KLD = loss_function(recon, inputs, mu, logvar)\n",
    "            total_loss = BCE + KLD  # 计算总损失\n",
    "            average_loss = total_loss.item() / inputs.size(0)  # 计算平均损失\n",
    "            reconstruction_errors.append(average_loss)  # 添加单个损失值到列表中\n",
    "    return reconstruction_errors\n",
    "\n",
    "# 收集验证集的重构误差\n",
    "val_errors = collect_reconstruction_errors(model, val_loader)\n",
    "threshold = np.quantile(val_errors, 0.95)  # 计算95%分位数作为阈值\n",
    "threshold = threshold * 1\n",
    "print(f\"Loss threshold for detecting data quality issues: {threshold}\")\n",
    "\n",
    "min_val_error = min(val_errors)\n",
    "max_val_error = max(val_errors)\n",
    "mean_val_error = sum(val_errors) / len(val_errors)\n",
    "print(f\"Min validation error: {min_val_error}\")\n",
    "print(f\"Max validation error: {max_val_error}\")\n",
    "print(f\"Mean validation error: {mean_val_error}\")\n",
    "print(f\"95th percentile of validation errors: {np.quantile(val_errors, 0.95)}\")\n",
    "print(f\"Maximum validation error (100th percentile): {np.quantile(val_errors, 1)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b7b267a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0 is ok: 18 out of 2373 samples are faulty (0.76%).\n",
      "Batch 1 is ok: 27 out of 2373 samples are faulty (1.14%).\n",
      "Batch 2 is ok: 28 out of 2373 samples are faulty (1.18%).\n",
      "Batch 3 is ok: 20 out of 2373 samples are faulty (0.84%).\n",
      "Batch 4 is ok: 21 out of 2373 samples are faulty (0.88%).\n",
      "Batch 5 is ok: 18 out of 2373 samples are faulty (0.76%).\n",
      "Batch 6 is ok: 20 out of 2373 samples are faulty (0.84%).\n",
      "Batch 7 is ok: 19 out of 2373 samples are faulty (0.80%).\n",
      "Batch 8 is ok: 23 out of 2373 samples are faulty (0.97%).\n",
      "Batch 9 is ok: 18 out of 2373 samples are faulty (0.76%).\n",
      "Batch 10 is ok: 15 out of 2373 samples are faulty (0.63%).\n",
      "Batch 11 is ok: 32 out of 2373 samples are faulty (1.35%).\n",
      "Batch 12 is ok: 33 out of 2373 samples are faulty (1.39%).\n",
      "Batch 13 is ok: 25 out of 2373 samples are faulty (1.05%).\n",
      "Batch 14 is ok: 22 out of 2373 samples are faulty (0.93%).\n",
      "Batch 15 is ok: 27 out of 2373 samples are faulty (1.14%).\n",
      "Batch 16 is ok: 20 out of 2373 samples are faulty (0.84%).\n",
      "Batch 17 is ok: 22 out of 2373 samples are faulty (0.93%).\n",
      "Batch 18 is ok: 22 out of 2373 samples are faulty (0.93%).\n",
      "Batch 19 is ok: 26 out of 2373 samples are faulty (1.10%).\n",
      "Batch 20 is ok: 25 out of 2373 samples are faulty (1.05%).\n",
      "Batch 21 is ok: 31 out of 2373 samples are faulty (1.31%).\n",
      "Batch 22 is ok: 18 out of 2373 samples are faulty (0.76%).\n",
      "Batch 23 is ok: 31 out of 2373 samples are faulty (1.31%).\n",
      "Batch 24 is ok: 21 out of 2373 samples are faulty (0.88%).\n",
      "Batch 25 is ok: 33 out of 2373 samples are faulty (1.39%).\n",
      "Batch 26 is ok: 23 out of 2373 samples are faulty (0.97%).\n",
      "Batch 27 is ok: 18 out of 2373 samples are faulty (0.76%).\n",
      "Batch 28 is ok: 27 out of 2373 samples are faulty (1.14%).\n",
      "Batch 29 is ok: 24 out of 2373 samples are faulty (1.01%).\n",
      "Batch 30 is ok: 33 out of 2373 samples are faulty (1.39%).\n",
      "Batch 31 is ok: 18 out of 2373 samples are faulty (0.76%).\n",
      "Batch 32 is ok: 24 out of 2373 samples are faulty (1.01%).\n",
      "Batch 33 is ok: 22 out of 2373 samples are faulty (0.93%).\n",
      "Batch 34 is ok: 18 out of 2373 samples are faulty (0.76%).\n",
      "Batch 35 is ok: 28 out of 2373 samples are faulty (1.18%).\n",
      "Batch 36 is ok: 24 out of 2373 samples are faulty (1.01%).\n",
      "Batch 37 is ok: 31 out of 2373 samples are faulty (1.31%).\n",
      "Batch 38 is ok: 22 out of 2373 samples are faulty (0.93%).\n",
      "Batch 39 is ok: 25 out of 2373 samples are faulty (1.05%).\n",
      "Batch 40 is ok: 28 out of 2373 samples are faulty (1.18%).\n",
      "Batch 41 is ok: 24 out of 2373 samples are faulty (1.01%).\n",
      "Batch 42 is ok: 23 out of 2373 samples are faulty (0.97%).\n",
      "Batch 43 is ok: 17 out of 2373 samples are faulty (0.72%).\n",
      "Batch 44 is ok: 24 out of 2373 samples are faulty (1.01%).\n",
      "Batch 45 is ok: 23 out of 2373 samples are faulty (0.97%).\n",
      "Batch 46 is ok: 15 out of 2373 samples are faulty (0.63%).\n",
      "Batch 47 is ok: 32 out of 2373 samples are faulty (1.35%).\n",
      "Batch 48 is ok: 17 out of 2373 samples are faulty (0.72%).\n",
      "Batch 49 is ok: 38 out of 2373 samples are faulty (1.60%).\n",
      "Total batches with issues: 0 out of 50\n",
      "Total problematic samples: 1193 out of 118690 (1.01%)\n",
      "Percentage of data quality issues detected in the test set: 1.01%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "def detect_quality_issues(model, data_loader, threshold):\n",
    "    model.eval()\n",
    "    total_issue_count = 0\n",
    "    total_batches_with_issues = 0\n",
    "    total_samples = 0\n",
    "    current_batch_issues = 0\n",
    "    batch_count = 0\n",
    "    batch_size = len(data_loader.dataset) // 50  # 你希望的批次大小\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, _ in data_loader:\n",
    "            inputs = inputs.to(device)  # 将输入数据移动到正确的设备\n",
    "            recon, mu, logvar = model(inputs)\n",
    "            BCE, KLD = loss_function(recon, inputs, mu, logvar)\n",
    "            total_loss = BCE + KLD  # 计算当前样本的总损失\n",
    "            total_samples += 1\n",
    "\n",
    "            # 判断当前样本是否有问题\n",
    "            if total_loss.item() > threshold:\n",
    "                current_batch_issues += 1\n",
    "\n",
    "            # 当累积样本数达到你设定的批次大小时，评估这个批次\n",
    "            if total_samples % batch_size == 0:\n",
    "                if current_batch_issues >= batch_size * 0.02:  # 判断这个批次是否有超过5%的样本有问题\n",
    "                    print(f\"Batch {batch_count} is problematic: {current_batch_issues} out of {batch_size} samples are faulty ({(current_batch_issues/batch_size * 100):.2f}%).\")\n",
    "                    total_batches_with_issues += 1\n",
    "                    total_issue_count = total_issue_count + current_batch_issues\n",
    "                else:\n",
    "                    total_issue_count = total_issue_count + current_batch_issues\n",
    "                    print(f\"Batch {batch_count} is ok: {current_batch_issues} out of {batch_size} samples are faulty ({(current_batch_issues/batch_size * 100):.2f}%).\")\n",
    "                current_batch_issues = 0\n",
    "                batch_count += 1\n",
    "\n",
    "    total_issue_rate = total_issue_count / total_samples\n",
    "    print(f\"Total batches with issues: {total_batches_with_issues} out of {batch_count}\")\n",
    "    print(f\"Total problematic samples: {total_issue_count} out of {total_samples} ({(total_issue_rate * 100):.2f}%)\")\n",
    "    return total_issue_rate\n",
    "\n",
    "# Example usage\n",
    "# test_dataset = TensorDataset(test_tensor, test_tensor)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "issue_rate = detect_quality_issues(model, test_loader, threshold)\n",
    "print(f\"Percentage of data quality issues detected in the test set: {issue_rate * 100:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c139b017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0 is problematic: 1907 out of 13774 samples are faulty (13.84%).\n",
      "Batch 1 is problematic: 1920 out of 13774 samples are faulty (13.94%).\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 52\u001b[0m\n\u001b[1;32m     49\u001b[0m test_dirty_dataset \u001b[38;5;241m=\u001b[39m TensorDataset(test_dirty_tensor, test_dirty_tensor)\n\u001b[1;32m     50\u001b[0m test_dirty_loader \u001b[38;5;241m=\u001b[39m DataLoader(test_dirty_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \n\u001b[0;32m---> 52\u001b[0m issue_rate \u001b[38;5;241m=\u001b[39m \u001b[43mdetect_quality_issues\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dirty_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPercentage of data quality issues detected in the test set: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00missue_rate\u001b[38;5;250m \u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m100\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[24], line 16\u001b[0m, in \u001b[0;36mdetect_quality_issues\u001b[0;34m(model, data_loader, threshold)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inputs, _ \u001b[38;5;129;01min\u001b[39;00m data_loader:\n\u001b[1;32m     15\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device)  \u001b[38;5;66;03m# 将输入数据移动到正确的设备\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m     recon, mu, logvar \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m     BCE, KLD \u001b[38;5;241m=\u001b[39m loss_function(recon, inputs, mu, logvar)\n\u001b[1;32m     18\u001b[0m     total_loss \u001b[38;5;241m=\u001b[39m BCE \u001b[38;5;241m+\u001b[39m KLD  \u001b[38;5;66;03m# 计算当前样本的总损失\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/mainenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mainenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[20], line 38\u001b[0m, in \u001b[0;36mVAE.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     36\u001b[0m mu, logvar \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencode(x)\n\u001b[1;32m     37\u001b[0m z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreparameterize(mu, logvar)\n\u001b[0;32m---> 38\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m)\u001b[49m, mu, logvar\n",
      "Cell \u001b[0;32mIn[20], line 30\u001b[0m, in \u001b[0;36mVAE.decode\u001b[0;34m(self, z)\u001b[0m\n\u001b[1;32m     27\u001b[0m     eps \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn_like(std)\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mu \u001b[38;5;241m+\u001b[39m eps \u001b[38;5;241m*\u001b[39m std\n\u001b[0;32m---> 30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, z):\n\u001b[1;32m     31\u001b[0m     h3 \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc4(z))\n\u001b[1;32m     32\u001b[0m     h4 \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc5(h3))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "def detect_quality_issues(model, data_loader, threshold):\n",
    "    model.eval()\n",
    "    total_issue_count = 0\n",
    "    total_batches_with_issues = 0\n",
    "    total_samples = 0\n",
    "    current_batch_issues = 0\n",
    "    batch_count = 0\n",
    "    batch_size = len(data_loader.dataset) // 50  # 你希望的批次大小\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, _ in data_loader:\n",
    "            inputs = inputs.to(device)  # 将输入数据移动到正确的设备\n",
    "            recon, mu, logvar = model(inputs)\n",
    "            BCE, KLD = loss_function(recon, inputs, mu, logvar)\n",
    "            total_loss = BCE + KLD  # 计算当前样本的总损失\n",
    "            total_samples += 1\n",
    "\n",
    "            # 判断当前样本是否有问题\n",
    "            if total_loss.item() > threshold:\n",
    "                current_batch_issues += 1\n",
    "\n",
    "            # 当累积样本数达到你设定的批次大小时，评估这个批次\n",
    "            if total_samples % batch_size == 0:\n",
    "                if current_batch_issues >= batch_size * 0.02:  # 判断这个批次是否有超过5%的样本有问题\n",
    "                    print(f\"Batch {batch_count} is problematic: {current_batch_issues} out of {batch_size} samples are faulty ({(current_batch_issues/batch_size * 100):.2f}%).\")\n",
    "                    total_batches_with_issues += 1\n",
    "                    total_issue_count = total_issue_count + current_batch_issues\n",
    "                else:\n",
    "                    total_issue_count = total_issue_count + current_batch_issues\n",
    "                    print(f\"Batch {batch_count} is ok: {current_batch_issues} out of {batch_size} samples are faulty ({(current_batch_issues/batch_size * 100):.2f}%).\")\n",
    "                current_batch_issues = 0\n",
    "                batch_count += 1\n",
    "\n",
    "    total_issue_rate = total_issue_count / total_samples\n",
    "    print(f\"Total batches with issues: {total_batches_with_issues} out of {batch_count}\")\n",
    "    print(f\"Total problematic samples: {total_issue_count} out of {total_samples} ({(total_issue_rate * 100):.2f}%)\")\n",
    "    return total_issue_rate\n",
    "\n",
    "# Example usage\n",
    "\n",
    "\n",
    "\n",
    "# 假设 data_dirty 已经是一个经过预处理的 DataFrame\n",
    "data_dirty_array = data_dirty.values.astype(np.float32)  # 转换为浮点数类型的 NumPy 数组\n",
    "test_dirty_tensor = torch.tensor(data_dirty_array)  \n",
    "test_dirty_dataset = TensorDataset(test_dirty_tensor, test_dirty_tensor)\n",
    "test_dirty_loader = DataLoader(test_dirty_dataset, batch_size=1, shuffle=False) \n",
    "\n",
    "issue_rate = detect_quality_issues(model, test_dirty_loader, threshold)\n",
    "print(f\"Percentage of data quality issues detected in the test set: {issue_rate * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68bd6172",
   "metadata": {},
   "source": [
    "##loop test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "73c55af4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random sample 0 is problematic: 14130 out of 68874 samples are faulty (20.52%).\n",
      "Random sample 1 is problematic: 13941 out of 68874 samples are faulty (20.24%).\n",
      "Random sample 2 is problematic: 14016 out of 68874 samples are faulty (20.35%).\n",
      "Random sample 3 is problematic: 14183 out of 68874 samples are faulty (20.59%).\n",
      "Random sample 4 is problematic: 13949 out of 68874 samples are faulty (20.25%).\n",
      "Random sample 5 is problematic: 14062 out of 68874 samples are faulty (20.42%).\n",
      "Random sample 6 is problematic: 13942 out of 68874 samples are faulty (20.24%).\n",
      "Random sample 7 is problematic: 13903 out of 68874 samples are faulty (20.19%).\n",
      "Random sample 8 is problematic: 13982 out of 68874 samples are faulty (20.30%).\n",
      "Random sample 9 is problematic: 13990 out of 68874 samples are faulty (20.31%).\n",
      "Random sample 10 is problematic: 14107 out of 68874 samples are faulty (20.48%).\n",
      "Random sample 11 is problematic: 13970 out of 68874 samples are faulty (20.28%).\n",
      "Random sample 12 is problematic: 13980 out of 68874 samples are faulty (20.30%).\n",
      "Random sample 13 is problematic: 13879 out of 68874 samples are faulty (20.15%).\n",
      "Random sample 14 is problematic: 14077 out of 68874 samples are faulty (20.44%).\n",
      "Random sample 15 is problematic: 14070 out of 68874 samples are faulty (20.43%).\n",
      "Random sample 16 is problematic: 13933 out of 68874 samples are faulty (20.23%).\n",
      "Random sample 17 is problematic: 14079 out of 68874 samples are faulty (20.44%).\n",
      "Random sample 18 is problematic: 13925 out of 68874 samples are faulty (20.22%).\n",
      "Random sample 19 is problematic: 14044 out of 68874 samples are faulty (20.39%).\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 44\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal problematic batches across all tests: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mproblematic_batches\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# 假设 model, data_dirty, threshold, device 已经被正确定义和设置\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m \u001b[43mtest_random_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_dirty\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[11], line 38\u001b[0m, in \u001b[0;36mtest_random_samples\u001b[0;34m(model, data_dirty, threshold)\u001b[0m\n\u001b[1;32m     36\u001b[0m     sample_dataset \u001b[38;5;241m=\u001b[39m TensorDataset(sample_tensor, sample_tensor)\n\u001b[1;32m     37\u001b[0m     test_dirty_loader \u001b[38;5;241m=\u001b[39m DataLoader(sample_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \n\u001b[0;32m---> 38\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mdetect_quality_issues\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dirty_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     39\u001b[0m         problematic_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal problematic batches across all tests: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mproblematic_batches\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[11], line 14\u001b[0m, in \u001b[0;36mdetect_quality_issues\u001b[0;34m(model, data_loader, threshold, seed)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inputs, _ \u001b[38;5;129;01min\u001b[39;00m data_loader:\n\u001b[1;32m     13\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 14\u001b[0m     recon, mu, logvar \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m     BCE, KLD \u001b[38;5;241m=\u001b[39m loss_function(recon, inputs, mu, logvar)\n\u001b[1;32m     16\u001b[0m     total_loss \u001b[38;5;241m=\u001b[39m BCE \u001b[38;5;241m+\u001b[39m KLD\n",
      "File \u001b[0;32m~/miniconda3/envs/mainenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mainenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[6], line 37\u001b[0m, in \u001b[0;36mVAE.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     36\u001b[0m     mu, logvar \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencode(x)\n\u001b[0;32m---> 37\u001b[0m     z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreparameterize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogvar\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecode(z), mu, logvar\n",
      "Cell \u001b[0;32mIn[6], line 27\u001b[0m, in \u001b[0;36mVAE.reparameterize\u001b[0;34m(self, mu, logvar)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreparameterize\u001b[39m(\u001b[38;5;28mself\u001b[39m, mu, logvar):\n\u001b[1;32m     26\u001b[0m     std \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mexp(\u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m logvar) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1e-8\u001b[39m  \u001b[38;5;66;03m# Adding a small constant for numerical stability\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m     eps \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandn_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstd\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mu \u001b[38;5;241m+\u001b[39m eps \u001b[38;5;241m*\u001b[39m std\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def detect_quality_issues(model, data_loader, threshold, seed):\n",
    "    model.eval()\n",
    "    current_batch_issues = 0\n",
    "    batch_size = len(data_loader.dataset)  # 这里的批次大小是整个数据集的大小\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, _ in data_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            recon, mu, logvar = model(inputs)\n",
    "            BCE, KLD = loss_function(recon, inputs, mu, logvar)\n",
    "            total_loss = BCE + KLD\n",
    "            # 判断当前样本是否有问题\n",
    "            if total_loss.item() > threshold:\n",
    "                current_batch_issues += 1\n",
    "\n",
    "    # 评估是否有超过5%的样本有问题\n",
    "    if current_batch_issues > batch_size * 0.1:\n",
    "        print(f\"Random sample {seed} is problematic: {current_batch_issues} out of {batch_size} samples are faulty ({(current_batch_issues/batch_size * 100):.2f}%).\")\n",
    "        return True\n",
    "    else:\n",
    "        print(f\"Random sample {seed} is ok: {current_batch_issues} out of {batch_size} samples are faulty ({(current_batch_issues/batch_size * 100):.2f}%).\")\n",
    "        return False\n",
    "\n",
    "# 主函数：执行50次随机采样测试\n",
    "def test_random_samples(model, data_dirty, threshold):\n",
    "    problematic_batches = 0\n",
    "    for seed in range(50):\n",
    "        _, sample_data = train_test_split(data_dirty, test_size=0.1, random_state=seed)  # 随机采样20%\n",
    "        sample_data_array = sample_data.values.astype(np.float32)\n",
    "        sample_tensor = torch.tensor(sample_data_array)\n",
    "        sample_dataset = TensorDataset(sample_tensor, sample_tensor)\n",
    "        test_dirty_loader = DataLoader(sample_dataset, batch_size=1, shuffle=False) \n",
    "        if detect_quality_issues(model, test_dirty_loader, threshold, seed):\n",
    "            problematic_batches += 1\n",
    "\n",
    "    print(f\"Total problematic batches across all tests: {problematic_batches}\")\n",
    "\n",
    "# 假设 model, data_dirty, threshold, device 已经被正确定义和设置\n",
    "test_random_samples(model, data_dirty, threshold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74364557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random sample 0 is ok: 1217 out of 23738 samples are faulty (5.13%).\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 45\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal problematic batches across all tests: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mproblematic_batches\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# 假设 model, data_dirty, threshold, device 已经被正确定义和设置\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m \u001b[43mtest_random_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[27], line 38\u001b[0m, in \u001b[0;36mtest_random_samples\u001b[0;34m(model, data, threshold)\u001b[0m\n\u001b[1;32m     36\u001b[0m     sample_dataset \u001b[38;5;241m=\u001b[39m TensorDataset(sample_tensor, sample_tensor)\n\u001b[1;32m     37\u001b[0m     test_dirty_loader \u001b[38;5;241m=\u001b[39m DataLoader(sample_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \n\u001b[0;32m---> 38\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mdetect_quality_issues\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dirty_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     39\u001b[0m         problematic_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal problematic batches across all tests: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mproblematic_batches\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[27], line 14\u001b[0m, in \u001b[0;36mdetect_quality_issues\u001b[0;34m(model, data_loader, threshold, seed)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inputs, _ \u001b[38;5;129;01min\u001b[39;00m data_loader:\n\u001b[1;32m     13\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 14\u001b[0m     recon, mu, logvar \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m     BCE, KLD \u001b[38;5;241m=\u001b[39m loss_function(recon, inputs, mu, logvar)\n\u001b[1;32m     16\u001b[0m     total_loss \u001b[38;5;241m=\u001b[39m BCE \u001b[38;5;241m+\u001b[39m KLD\n",
      "File \u001b[0;32m~/miniconda3/envs/mainenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mainenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[20], line 38\u001b[0m, in \u001b[0;36mVAE.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     36\u001b[0m mu, logvar \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencode(x)\n\u001b[1;32m     37\u001b[0m z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreparameterize(mu, logvar)\n\u001b[0;32m---> 38\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m)\u001b[49m, mu, logvar\n",
      "Cell \u001b[0;32mIn[20], line 33\u001b[0m, in \u001b[0;36mVAE.decode\u001b[0;34m(self, z)\u001b[0m\n\u001b[1;32m     31\u001b[0m h3 \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc4(z))\n\u001b[1;32m     32\u001b[0m h4 \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc5(h3))\n\u001b[0;32m---> 33\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msigmoid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc6\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh4\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def detect_quality_issues(model, data_loader, threshold, seed):\n",
    "    model.eval()\n",
    "    current_batch_issues = 0\n",
    "    batch_size = len(data_loader.dataset)  # 这里的批次大小是整个数据集的大小\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, _ in data_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            recon, mu, logvar = model(inputs)\n",
    "            BCE, KLD = loss_function(recon, inputs, mu, logvar)\n",
    "            total_loss = BCE + KLD\n",
    "            # 判断当前样本是否有问题\n",
    "            if total_loss.item() > threshold:\n",
    "                current_batch_issues += 1\n",
    "\n",
    "    # 评估是否有超过5%的样本有问题\n",
    "    if current_batch_issues > batch_size * 0.1:\n",
    "        print(f\"Random sample {seed} is problematic: {current_batch_issues} out of {batch_size} samples are faulty ({(current_batch_issues/batch_size * 100):.2f}%).\")\n",
    "        return True\n",
    "    else:\n",
    "        print(f\"Random sample {seed} is ok: {current_batch_issues} out of {batch_size} samples are faulty ({(current_batch_issues/batch_size * 100):.2f}%).\")\n",
    "        return False\n",
    "\n",
    "# 主函数：执行50次随机采样测试\n",
    "def test_random_samples(model, data, threshold):\n",
    "    problematic_batches = 0\n",
    "    for seed in range(50):\n",
    "        _, sample_data = train_test_split(data, test_size=0.2, random_state=seed)  # 随机采样20%\n",
    "        ##sample_data_array = sample_data.values.astype(np.float32)\n",
    "        sample_tensor = torch.tensor(sample_data)\n",
    "        sample_dataset = TensorDataset(sample_tensor, sample_tensor)\n",
    "        test_dirty_loader = DataLoader(sample_dataset, batch_size=1, shuffle=False) \n",
    "        if detect_quality_issues(model, test_dirty_loader, threshold, seed):\n",
    "            problematic_batches += 1\n",
    "\n",
    "    print(f\"Total problematic batches across all tests: {problematic_batches}\")\n",
    "\n",
    "\n",
    "# 假设 model, data_dirty, threshold, device 已经被正确定义和设置\n",
    "test_random_samples(model, test_data, threshold)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8bc294",
   "metadata": {},
   "source": [
    "## 保存loss 到文件中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c690c9ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss data for random sample data_dirty_graph saved.\n"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "# import numpy as np\n",
    "# from torch.utils.data import DataLoader, TensorDataset\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# import pandas as pd\n",
    "\n",
    "# def record_losses(model, data_loader, seed):\n",
    "#     model.eval()\n",
    "#     losses = []\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         for inputs, _ in data_loader:\n",
    "#             inputs = inputs.to(device)\n",
    "#             recon, mu, logvar = model(inputs)\n",
    "#             BCE, KLD = loss_function(recon, inputs, mu, logvar)\n",
    "#             total_loss = BCE + KLD\n",
    "#             losses.append(total_loss.item())\n",
    "    \n",
    "#     # Save the losses to a CSV file\n",
    "#     losses_df = pd.DataFrame(losses, columns=['Loss_dirty_graph'])\n",
    "#     losses_df.to_csv(f'loss_data_dirty_graph.csv', index=False)\n",
    "#     print(f\"Loss data for random sample data_dirty_graph saved.\")\n",
    "\n",
    "# # 主函数：测试2000个随机样本\n",
    "# def test_random_sample(model, data_dirty):\n",
    "#     seed = 42  # Use a fixed seed for reproducibility\n",
    "#     _, sample_data = train_test_split(data_dirty, test_size=2000, train_size=None, random_state=seed)\n",
    "#     sample_data_array = sample_data.values.astype(np.float32)\n",
    "#     sample_tensor = torch.tensor(sample_data_array)\n",
    "#     sample_dataset = TensorDataset(sample_tensor, sample_tensor)\n",
    "#     test_loader = DataLoader(sample_dataset, batch_size=1, shuffle=False) \n",
    "#     record_losses(model, test_loader, seed)\n",
    "\n",
    "# # 假设 model, data_dirty, device, loss_function 已经被正确定义和设置\n",
    "# test_random_sample(model, data_dirty)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "652dc382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined loss data saved to /home/sdong/experiments/VAE_method/results/combined_loss_data_gragh.csv.\n"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "# import numpy as np\n",
    "# from torch.utils.data import DataLoader, TensorDataset\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# import pandas as pd\n",
    "# import os\n",
    "\n",
    "# def record_losses(model, data_loader, seed, device, loss_function):\n",
    "#     model.eval()\n",
    "#     losses = []\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         for inputs, _ in data_loader:\n",
    "#             inputs = inputs.to(device)\n",
    "#             recon, mu, logvar = model(inputs)\n",
    "#             BCE, KLD = loss_function(recon, inputs, mu, logvar)\n",
    "#             total_loss = BCE + KLD\n",
    "#             losses.append(total_loss.item())\n",
    "    \n",
    "#     return losses\n",
    "\n",
    "# # 主函数：测试两个数据集并将结果保存到同一个CSV文件\n",
    "# def test_and_save_combined_losses(model, data_clean, data_dirty, file_name='/home/sdong/experiments/VAE_method/results/combined_loss_data_gragh.csv'):\n",
    "#     seed = 42  # Use a fixed seed for reproducibility\n",
    "#     # Process clean data\n",
    "#     _, sample_data_clean = train_test_split(data_clean, test_size=2000, train_size=None, random_state=seed)\n",
    "#     #sample_clean_array = sample_data_clean.values.astype(np.float32)\n",
    "#     sample_clean_tensor = torch.tensor(sample_data_clean)\n",
    "#     sample_clean_dataset = TensorDataset(sample_clean_tensor, sample_clean_tensor)\n",
    "#     clean_loader = DataLoader(sample_clean_dataset, batch_size=1, shuffle=False)\n",
    "#     clean_losses = record_losses(model, clean_loader, seed, device, loss_function)\n",
    "    \n",
    "#     # Process dirty data\n",
    "#     _, sample_data_dirty = train_test_split(data_dirty, test_size=2000, train_size=None, random_state=seed)\n",
    "#     sample_dirty_array = sample_data_dirty.values.astype(np.float32)\n",
    "#     sample_dirty_tensor = torch.tensor(sample_dirty_array)\n",
    "#     sample_dirty_dataset = TensorDataset(sample_dirty_tensor, sample_dirty_tensor)\n",
    "#     dirty_loader = DataLoader(sample_dirty_dataset, batch_size=1, shuffle=False)\n",
    "#     dirty_losses = record_losses(model, dirty_loader, seed, device, loss_function)\n",
    "    \n",
    "#     # Combine and save to CSV\n",
    "#     combined_df = pd.DataFrame({\n",
    "#         'Loss_clean_graph': clean_losses,\n",
    "#         'Loss_dirty_graph': dirty_losses\n",
    "#     })\n",
    "#     combined_df.to_csv(file_name, index=False)\n",
    "#     print(f\"Combined loss data saved to {file_name}.\")\n",
    "\n",
    "# # 假设 model, data_clean, data_dirty, device, loss_function 已经被正确定义和设置\n",
    "# test_and_save_combined_losses(model, test_data, data_dirty)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95383854",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c828937",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "huggingface_learn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
