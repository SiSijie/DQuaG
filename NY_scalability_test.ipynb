{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b91df9ab-79c4-46e0-a477-ac62672be489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000000 entries, 0 to 9999999\n",
      "Data columns (total 5 columns):\n",
      " #   Column                 Dtype  \n",
      "---  ------                 -----  \n",
      " 0   VendorID               int64  \n",
      " 1   tpep_pickup_datetime   object \n",
      " 2   tpep_dropoff_datetime  object \n",
      " 3   passenger_count        int64  \n",
      " 4   trip_distance          float64\n",
      "dtypes: float64(1), int64(2), object(2)\n",
      "memory usage: 381.5+ MB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000000 entries, 0 to 9999999\n",
      "Data columns (total 5 columns):\n",
      " #   Column                 Dtype  \n",
      "---  ------                 -----  \n",
      " 0   VendorID               float64\n",
      " 1   tpep_pickup_datetime   object \n",
      " 2   tpep_dropoff_datetime  object \n",
      " 3   passenger_count        float64\n",
      " 4   trip_distance          float64\n",
      "dtypes: float64(3), object(2)\n",
      "memory usage: 381.5+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler\n",
    "import networkx as nx\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import from_networkx\n",
    "\n",
    "# 加载数据\n",
    "file_path_clean = '/home/sdong/data/taxi/yellow_tripdata_sample.csv'\n",
    "file_path_origi = '/home/sdong/data/taxi/yellow_tripdata_missing.csv'\n",
    "\n",
    "data = pd.read_csv(file_path_clean)\n",
    "data_dirty = pd.read_csv(file_path_origi)\n",
    "\n",
    "\n",
    "# 保留指定的列\n",
    "columns_to_keep = [\n",
    "    \"VendorID\", \"tpep_pickup_datetime\", \"tpep_dropoff_datetime\", \"passenger_count\",\n",
    "    \"trip_distance\"\n",
    "]\n",
    "# columns_to_keep = [\n",
    "#     \"VendorID\", \"tpep_pickup_datetime\", \"tpep_dropoff_datetime\", \"passenger_count\",\n",
    "#     \"trip_distance\", \"pickup_longitude\", \"pickup_latitude\", \"RateCodeID\",\n",
    "#     \"store_and_fwd_flag\", \"dropoff_longitude\"\n",
    "# ]\n",
    "data = data[columns_to_keep]\n",
    "data_dirty = data_dirty[columns_to_keep]\n",
    "# 设置显示所有列和部分行\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 10)  \n",
    "# 显示数据集的前几行和数据结构\n",
    "print(data.info())\n",
    "print(data_dirty.info())\n",
    "# 保存原始DataFrame的列名\n",
    "column_names = data_dirty.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8ca28482",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-numeric columns: Index(['tpep_pickup_datetime', 'tpep_dropoff_datetime'], dtype='object')\n",
      "Data types after encoding:\n",
      " VendorID                   int64\n",
      "tpep_pickup_datetime       int64\n",
      "tpep_dropoff_datetime      int64\n",
      "passenger_count            int64\n",
      "trip_distance            float64\n",
      "dtype: object\n",
      "Non-numeric columns: Index(['tpep_pickup_datetime', 'tpep_dropoff_datetime'], dtype='object')\n",
      "Data types after encoding:\n",
      " VendorID                 float64\n",
      "tpep_pickup_datetime       int64\n",
      "tpep_dropoff_datetime      int64\n",
      "passenger_count          float64\n",
      "trip_distance            float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data.fillna(0, inplace=True)\n",
    "# 检查非数值列\n",
    "non_numeric_cols = data.select_dtypes(include=['object']).columns\n",
    "print(\"Non-numeric columns:\", non_numeric_cols)\n",
    "\n",
    "# 将非数值列转换为数值类型（使用标签编码）\n",
    "label_encoders = {}\n",
    "for col in non_numeric_cols:\n",
    "    le = LabelEncoder()\n",
    "    data[col] = le.fit_transform(data[col].astype(str))\n",
    "    label_encoders[col] = le\n",
    "\n",
    "# 确保所有特征列都是数值类型\n",
    "print(\"Data types after encoding:\\n\", data.dtypes)\n",
    "\n",
    "\n",
    "data_dirty.fillna(0, inplace=True)\n",
    "# 检查非数值列\n",
    "non_numeric_cols = data_dirty.select_dtypes(include=['object']).columns\n",
    "print(\"Non-numeric columns:\", non_numeric_cols)\n",
    "\n",
    "# 将非数值列转换为数值类型（使用标签编码）\n",
    "label_encoders = {}\n",
    "for col in non_numeric_cols:\n",
    "    le = LabelEncoder()\n",
    "    data_dirty[col] = le.fit_transform(data_dirty[col].astype(str))\n",
    "    label_encoders[col] = le\n",
    "\n",
    "# 确保所有特征列都是数值类型\n",
    "print(\"Data types after encoding:\\n\", data_dirty.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1f495fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 标准化数值特征\n",
    "scaler = MinMaxScaler()\n",
    "data = scaler.fit_transform(data)\n",
    "\n",
    "# 标准化数值特征\n",
    "scaler = MinMaxScaler()\n",
    "data_dirty = scaler.fit_transform(data_dirty)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "73901a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(data, columns=column_names)\n",
    "data_dirty = pd.DataFrame(data_dirty, columns=column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "27dc078d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean data values are within [0, 1]: True\n",
      "Dirty data values are within [0, 1]: True\n"
     ]
    }
   ],
   "source": [
    "# 检查两个数据集中是否所有值都在0到1之间\n",
    "def check_values_in_range(df, lower=0, upper=1):\n",
    "    return ((df >= lower) & (df <= upper)).all().all()\n",
    "\n",
    "is_data_in_range = check_values_in_range(data)\n",
    "is_data_dirty_in_range = check_values_in_range(data_dirty)\n",
    "\n",
    "print(f\"Clean data values are within [0, 1]: {is_data_in_range}\")\n",
    "print(f\"Dirty data values are within [0, 1]: {is_data_dirty_in_range}\")\n",
    "\n",
    "if not is_data_in_range:\n",
    "    print(\"Clean data contains values out of range [0, 1].\")\n",
    "\n",
    "if not is_data_dirty_in_range:\n",
    "    print(\"Dirty data contains values out of range [0, 1].\")\n",
    "\n",
    "# 如果需要，打印出不在范围内的值和对应的索引\n",
    "def find_out_of_range_values(df, lower=0, upper=1):\n",
    "    out_of_range = df[(df < lower) | (df > upper)]\n",
    "    return out_of_range.dropna(how='all')\n",
    "\n",
    "if not is_data_in_range:\n",
    "    out_of_range_clean = find_out_of_range_values(data)\n",
    "    print(\"Out of range values in clean data:\")\n",
    "    print(out_of_range_clean)\n",
    "    out_of_range_clean_indices = out_of_range_clean.index\n",
    "    data = data.drop(out_of_range_clean_indices)\n",
    "\n",
    "if not is_data_dirty_in_range:\n",
    "    out_of_range_dirty = find_out_of_range_values(data_dirty)\n",
    "    print(\"Out of range values in dirty data:\")\n",
    "    print(out_of_range_dirty)\n",
    "    out_of_range_dirty_indices = out_of_range_dirty.index\n",
    "    data_dirty = data_dirty.drop(out_of_range_dirty_indices)\n",
    "    # 获取不在范围内的值的索引\n",
    "\n",
    "\n",
    "\n",
    "# 删除不在范围内的行\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5887b650",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 假设 data 已经是一个经过预处理的 DataFrame\n",
    "data_array = data.values.astype(np.float32)  # 转换为浮点数类型的 NumPy 数组\n",
    "\n",
    "# 分割数据为训练集和临时测试集（包括真正的测试集和验证集）\n",
    "train_data, val_test_data = train_test_split(data_array, test_size=0.5, random_state=42)\n",
    "\n",
    "# 将训练验证集进一步分割为训练集和验证集\n",
    "val_data, test_data = train_test_split(val_test_data, test_size=0.5, random_state=42)  # 0.25 x 0.8 = 0.2\n",
    "\n",
    "\n",
    "# 转换为PyTorch张量\n",
    "\n",
    "\n",
    "# 创建数据加载器\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "batch_size = 1280  # 或者任何适合你GPU的大小\n",
    "\n",
    "train_tensor = torch.tensor(train_data) #0.6\n",
    "train_dataset = TensorDataset(train_tensor, train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "val_tensor = torch.tensor(val_data) #0.2\n",
    "val_dataset = TensorDataset(val_tensor, val_tensor)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "test_tensor = torch.tensor(test_data)  #20%\n",
    "test_dataset = TensorDataset(test_tensor, test_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)  # 50个批次\n",
    "#len(test_dataset)// 50\n",
    "\n",
    "data_dirty_array = data_dirty.values.astype(np.float32)  # 转换为浮点数类型的 NumPy 数组\n",
    "test_dirty_tensor = torch.tensor(data_dirty_array)  #20%\n",
    "test_dirty_dataset = TensorDataset(test_dirty_tensor, test_dirty_tensor)\n",
    "test_dirty_loader = DataLoader(test_dirty_dataset, batch_size=batch_size, shuffle=False)  # 50个批次\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "63a8f2af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAE(\n",
      "  (fc1): Linear(in_features=5, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc31): Linear(in_features=64, out_features=20, bias=True)\n",
      "  (fc32): Linear(in_features=64, out_features=20, bias=True)\n",
      "  (fc4): Linear(in_features=20, out_features=64, bias=True)\n",
      "  (fc5): Linear(in_features=64, out_features=128, bias=True)\n",
      "  (fc6): Linear(in_features=128, out_features=5, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "        # Encoder\n",
    "        self.fc1 = nn.Linear(5, 128)  # Input layer\n",
    "        self.fc2 = nn.Linear(128, 64)  # Hidden layer\n",
    "        self.fc31 = nn.Linear(64, 20)  # Output layer for mu\n",
    "        self.fc32 = nn.Linear(64, 20)  # Output layer for logvar\n",
    "\n",
    "        # Decoder\n",
    "        self.fc4 = nn.Linear(20, 64)   # Input layer\n",
    "        self.fc5 = nn.Linear(64, 128)  # Hidden layer\n",
    "        self.fc6 = nn.Linear(128, 5)  # Output layer\n",
    "\n",
    "    def encode(self, x):\n",
    "        h1 = F.relu(self.fc1(x))\n",
    "        h2 = F.relu(self.fc2(h1))\n",
    "        return self.fc31(h2), self.fc32(h2)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar) + 1e-8  # Adding a small constant for numerical stability\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z):\n",
    "        h3 = F.relu(self.fc4(z))\n",
    "        h4 = F.relu(self.fc5(h3))\n",
    "        return torch.sigmoid(self.fc6(h4))  # Use sigmoid to ensure output is between 0 and 1\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n",
    "\n",
    "# Instantiate the model\n",
    "model = VAE()\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c1d03c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#device = torch.device(\"cpu\")\n",
    "model = VAE().to(device)\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "# 设置优化器\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    # 确保目标张量也是浮点类型且维度匹配\n",
    "    recon_x = torch.clamp(recon_x, 0, 1)  # 确保输出值在[0, 1]范围内\n",
    "    BCE = F.binary_cross_entropy(recon_x, x, reduction='none')  # 保留每个样本的损失\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp(), dim=1, keepdim=True)  # 保持维度\n",
    "    return BCE + KLD\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c4a9e966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Total Loss: 2.5664610731933593\n",
      "Epoch: 2 Total Loss: 2.559936175830078\n",
      "Epoch: 3 Total Loss: 2.559724727282715\n",
      "Epoch: 4 Total Loss: 2.5596493733764647\n",
      "Epoch: 5 Total Loss: 2.559626724243164\n",
      "Epoch: 6 Total Loss: 2.5596165051757813\n",
      "Epoch: 7 Total Loss: 2.5596081761230467\n",
      "Epoch: 8 Total Loss: 2.5596041731445314\n",
      "Epoch: 9 Total Loss: 2.5595967587646484\n",
      "Epoch: 10 Total Loss: 2.5595948743408203\n",
      "Model saved to vae_model_ny_graph_scal.pth\n",
      "CUDA cache cleared.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    total_BCE = 0\n",
    "    total_KLD = 0\n",
    "    total_loss = 0\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):  # 使用TensorDataset，数据被重复用作输入和标签\n",
    "        data = data.to(device)\n",
    "        # 检查输入数据的范围\n",
    "        if (data < 0).any() or (data > 1).any():\n",
    "            raise ValueError(\"Input data contains values out of range [0, 1]\")\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        if (recon_batch < 0).any() or (recon_batch > 1).any():\n",
    "            raise ValueError(\"Warning: recon_batch contains values out of range [0, 1]\")\n",
    "        loss = loss_function(recon_batch, data, mu, logvar)  # 这里loss是每个样本的损失\n",
    "        loss.mean().backward()  # 使用.mean()在所有样本上平均损失然后进行反向传播\n",
    "        total_loss += loss.sum().item()  # 更新总损失\n",
    "        optimizer.step()\n",
    "        \n",
    "        # if batch_idx % 100 == 0:\n",
    "        #     print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "        #         epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "        #         100. * batch_idx / len(train_loader), loss.mean().item()))\n",
    "\n",
    "    # 打印每个epoch的平均损失\n",
    "    print(f'Epoch: {epoch} Total Loss: {total_loss / len(train_loader.dataset)}')\n",
    "\n",
    "# 训练模型\n",
    "num_epochs = 10  # 可根据需要调整\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    train(epoch)\n",
    "\n",
    "# 保存模型的状态字典\n",
    "torch.save(model.state_dict(), 'vae_model_ny_graph_scal.pth')\n",
    "print(\"Model saved to vae_model_ny_graph_scal.pth\")\n",
    "\n",
    "# 添加调试信息\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.synchronize()\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"CUDA cache cleared.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b6b9ddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss on validation data: 5.411014973925782\n",
      "Average loss on test data: 5.4115777953125\n",
      "Average loss on test dirty data: 9.254327900585938\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, data_loader):\n",
    "    model.eval()  # 切换到评估模式\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():  # 关闭梯度计算\n",
    "        for inputs, _ in data_loader:  # 假设 data_loader 返回 inputs 和 targets，这里我们不需要 targets\n",
    "            inputs = inputs.to(device)  # 确保将 inputs 转移到正确的设备\n",
    "            recon, mu, logvar = model(inputs)\n",
    "            loss = loss_function(recon, inputs, mu, logvar)  # 每个样本的损失列表\n",
    "            total_loss += loss.sum().item()  # 累计所有样本的损失\n",
    "\n",
    "    average_loss = total_loss / len(data_loader.dataset)\n",
    "    return average_loss\n",
    "\n",
    "# 加载模型\n",
    "model = VAE().to(device)\n",
    "model.load_state_dict(torch.load('vae_model_ny_graph_scal.pth'))\n",
    "\n",
    "# 计算测试集和验证集上的平均损失\n",
    "val_loss = evaluate_model(model, val_loader)\n",
    "test_loss = evaluate_model(model, test_loader)\n",
    "test_dirty_loss = evaluate_model(model, test_dirty_loader)\n",
    "\n",
    "print(f\"Average loss on validation data: {val_loss}\")\n",
    "print(f\"Average loss on test data: {test_loss}\")\n",
    "print(f\"Average loss on test dirty data: {test_dirty_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8950bc94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss threshold for detecting data quality issues: 3.086783230304718\n",
      "Min validation error: 2.1996965408325195\n",
      "Max validation error: 12.267314910888672\n",
      "Mean validation error: 2.559661284045696\n",
      "95th percentile of validation errors: 3.086783230304718\n",
      "Maximum validation error (100th percentile): 12.267314910888672\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def collect_reconstruction_errors(model, data_loader):\n",
    "    model.eval()\n",
    "    reconstruction_errors = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, _ in data_loader:  # 假设 data_loader 返回的是 inputs 和 labels，这里我们忽略 labels\n",
    "            inputs = inputs.to(device)  # 将输入数据移动到正确的设备\n",
    "            recon, mu, logvar = model(inputs)\n",
    "            loss = loss_function(recon, inputs, mu, logvar)  # 每个样本的损失列表\n",
    "            loss_per_sample = loss.sum(dim=1)\n",
    "            # print(\"Type of loss:\", type(loss_per_sample))\n",
    "            # print(\"Shape of loss:\", loss_per_sample.shape)\n",
    "            #print(\"First few loss values:\", loss[:10])  # 打印前10个损失值\n",
    "            reconstruction_errors.extend(loss_per_sample.tolist())  \n",
    "\n",
    "    return reconstruction_errors\n",
    "\n",
    "# 收集验证集的重构误差\n",
    "val_errors = collect_reconstruction_errors(model, val_loader)\n",
    "\n",
    "threshold = np.quantile(val_errors, 0.95)  # 计算95%分位数作为阈值\n",
    "print(f\"Loss threshold for detecting data quality issues: {threshold}\")\n",
    "\n",
    "min_val_error = min(val_errors)\n",
    "max_val_error = max(val_errors)\n",
    "mean_val_error = sum(val_errors) / len(val_errors)\n",
    "print(f\"Min validation error: {min_val_error}\")\n",
    "print(f\"Max validation error: {max_val_error}\")\n",
    "print(f\"Mean validation error: {mean_val_error}\")\n",
    "print(f\"95th percentile of validation errors: {np.quantile(val_errors, 0.95)}\")\n",
    "print(f\"Maximum validation error (100th percentile): {np.quantile(val_errors, 1)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dab3d7e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "样本大小: 1000, 运行时间: 0.016 秒\n",
      "Random sample size 1000 is ok: 58 out of 1000 samples are faulty (5.80%).\n",
      "样本大小: 5000, 运行时间: 0.070 秒\n",
      "Random sample size 5000 is ok: 266 out of 5000 samples are faulty (5.32%).\n",
      "样本大小: 10000, 运行时间: 0.140 秒\n",
      "Random sample size 10000 is ok: 579 out of 10000 samples are faulty (5.79%).\n",
      "样本大小: 20000, 运行时间: 0.409 秒\n",
      "Random sample size 20000 is ok: 1158 out of 20000 samples are faulty (5.79%).\n",
      "样本大小: 50000, 运行时间: 0.979 秒\n",
      "Random sample size 50000 is ok: 2979 out of 50000 samples are faulty (5.96%).\n",
      "样本大小: 100000, 运行时间: 1.998 秒\n",
      "Random sample size 100000 is ok: 6028 out of 100000 samples are faulty (6.03%).\n",
      "样本大小: 200000, 运行时间: 3.812 秒\n",
      "Random sample size 200000 is ok: 12092 out of 200000 samples are faulty (6.05%).\n",
      "样本大小: 500000, 运行时间: 9.392 秒\n",
      "Random sample size 500000 is ok: 31151 out of 500000 samples are faulty (6.23%).\n",
      "样本大小: 1000000, 运行时间: 18.073 秒\n",
      "Random sample size 1000000 is ok: 61718 out of 1000000 samples are faulty (6.17%).\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "\n",
    "def detect_quality_issues(model, data_loader, threshold):\n",
    "    model.eval()\n",
    "    current_issues_count = 0\n",
    "    total_samples = 0  # 累积处理的样本总数\n",
    "\n",
    "    start_time = time.time()  # 开始计时\n",
    "    with torch.no_grad():\n",
    "        for inputs, _ in data_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            recon, mu, logvar = model(inputs)\n",
    "            losses = loss_function(recon, inputs, mu, logvar)  # 获取每个样本的损失\n",
    "            loss_per_sample = losses.sum(dim=1)\n",
    "            # 检查每个样本是否有问题\n",
    "            for loss in loss_per_sample:\n",
    "                if loss.item() > threshold:\n",
    "                    current_issues_count += 1\n",
    "            total_samples += inputs.size(0)  # 更新处理的样本总数\n",
    "    end_time = time.time()  # 结束计时\n",
    "\n",
    "    run_time = end_time - start_time  # 计算运行时间\n",
    "    return current_issues_count, total_samples, run_time\n",
    "\n",
    "def test_random_samples(model, data, threshold, data_sizes):\n",
    "    for size in data_sizes:\n",
    "        sample_data = data[:size]  # 从数据中获取指定大小的样本\n",
    "        sample_data_array = sample_data.values.astype(np.float32)  # 转换为浮点数类型的 NumPy 数组\n",
    "        sample_tensor = torch.tensor(sample_data_array)  # 转换为tensor\n",
    "        sample_dataset = TensorDataset(sample_tensor, sample_tensor)\n",
    "        test_dirty_loader = DataLoader(sample_dataset, batch_size=len(sample_tensor), shuffle=False)  # 使用整个样本作为一个批次\n",
    "        current_issues_count, total_samples, run_time = detect_quality_issues(model, test_dirty_loader, threshold)\n",
    "        print(f\"样本大小: {size}, 运行时间: {run_time:.3f} 秒\")\n",
    "        if current_issues_count > total_samples * 0.1:\n",
    "            print(f\"Random sample size {size} is problematic: {current_issues_count} out of {total_samples} samples are faulty ({(current_issues_count / total_samples * 100):.2f}%).\")\n",
    "        else:\n",
    "            print(f\"Random sample size {size} is ok: {current_issues_count} out of {total_samples} samples are faulty ({(current_issues_count / total_samples * 100):.2f}%).\")\n",
    "        # 保存结果\n",
    "        results.append((size, run_time))\n",
    "# 假设 model, data_dirty, threshold, device 已经被正确定义和设置\n",
    "\n",
    "results = []\n",
    "data_sizes = [1000, 5000, 10000, 20000, 50000, 100000, 200000, 500000, 1000000]\n",
    "test_random_samples(model, data_dirty, threshold, data_sizes)\n",
    "\n",
    "# 将结果保存到文本文件\n",
    "with open(\"/home/sdong/experiments/results/results.txt\", \"a\") as file:\n",
    "    file.write(f\"NY taxi My approach scalability:\\n\")\n",
    "    for size, time in results:\n",
    "        file.write(f\"Data Size: {size}, Time Taken: {time} seconds\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5158c12f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e70407d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c585cc0c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "huggingface_learn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
