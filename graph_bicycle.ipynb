{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trip_id</th>\n",
       "      <th>usertype</th>\n",
       "      <th>gender</th>\n",
       "      <th>starttime</th>\n",
       "      <th>stoptime</th>\n",
       "      <th>tripduration</th>\n",
       "      <th>temperature</th>\n",
       "      <th>events</th>\n",
       "      <th>from_station_id</th>\n",
       "      <th>from_station_name</th>\n",
       "      <th>latitude_start</th>\n",
       "      <th>longitude_start</th>\n",
       "      <th>dpcapacity_start</th>\n",
       "      <th>to_station_id</th>\n",
       "      <th>to_station_name</th>\n",
       "      <th>latitude_end</th>\n",
       "      <th>longitude_end</th>\n",
       "      <th>dpcapacity_end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2355134</td>\n",
       "      <td>Subscriber</td>\n",
       "      <td>Male</td>\n",
       "      <td>2014-06-30 23:57:00</td>\n",
       "      <td>2014-07-01 00:07:00</td>\n",
       "      <td>10.066667</td>\n",
       "      <td>68.0</td>\n",
       "      <td>tstorms</td>\n",
       "      <td>131</td>\n",
       "      <td>Lincoln Ave &amp; Belmont Ave</td>\n",
       "      <td>41.939365</td>\n",
       "      <td>-87.668385</td>\n",
       "      <td>15.0</td>\n",
       "      <td>303</td>\n",
       "      <td>Broadway &amp; Cornelia Ave</td>\n",
       "      <td>41.945512</td>\n",
       "      <td>-87.645980</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2355133</td>\n",
       "      <td>Subscriber</td>\n",
       "      <td>Male</td>\n",
       "      <td>2014-06-30 23:56:00</td>\n",
       "      <td>2014-07-01 00:00:00</td>\n",
       "      <td>4.383333</td>\n",
       "      <td>68.0</td>\n",
       "      <td>tstorms</td>\n",
       "      <td>282</td>\n",
       "      <td>Halsted St &amp; Maxwell St</td>\n",
       "      <td>41.864580</td>\n",
       "      <td>-87.646930</td>\n",
       "      <td>15.0</td>\n",
       "      <td>22</td>\n",
       "      <td>May St &amp; Taylor St</td>\n",
       "      <td>41.869482</td>\n",
       "      <td>-87.655486</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2355130</td>\n",
       "      <td>Subscriber</td>\n",
       "      <td>Male</td>\n",
       "      <td>2014-06-30 23:33:00</td>\n",
       "      <td>2014-06-30 23:35:00</td>\n",
       "      <td>2.100000</td>\n",
       "      <td>68.0</td>\n",
       "      <td>tstorms</td>\n",
       "      <td>327</td>\n",
       "      <td>Sheffield Ave &amp; Webster Ave</td>\n",
       "      <td>41.921687</td>\n",
       "      <td>-87.653714</td>\n",
       "      <td>19.0</td>\n",
       "      <td>225</td>\n",
       "      <td>Halsted St &amp; Dickens Ave</td>\n",
       "      <td>41.919936</td>\n",
       "      <td>-87.648830</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2355129</td>\n",
       "      <td>Subscriber</td>\n",
       "      <td>Female</td>\n",
       "      <td>2014-06-30 23:26:00</td>\n",
       "      <td>2014-07-01 00:24:00</td>\n",
       "      <td>58.016667</td>\n",
       "      <td>68.0</td>\n",
       "      <td>tstorms</td>\n",
       "      <td>134</td>\n",
       "      <td>Peoria St &amp; Jackson Blvd</td>\n",
       "      <td>41.877749</td>\n",
       "      <td>-87.649633</td>\n",
       "      <td>19.0</td>\n",
       "      <td>194</td>\n",
       "      <td>State St &amp; Wacker Dr</td>\n",
       "      <td>41.887155</td>\n",
       "      <td>-87.627750</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2355128</td>\n",
       "      <td>Subscriber</td>\n",
       "      <td>Female</td>\n",
       "      <td>2014-06-30 23:16:00</td>\n",
       "      <td>2014-06-30 23:26:00</td>\n",
       "      <td>10.633333</td>\n",
       "      <td>68.0</td>\n",
       "      <td>tstorms</td>\n",
       "      <td>320</td>\n",
       "      <td>Loomis St &amp; Lexington St</td>\n",
       "      <td>41.872187</td>\n",
       "      <td>-87.661501</td>\n",
       "      <td>15.0</td>\n",
       "      <td>134</td>\n",
       "      <td>Peoria St &amp; Jackson Blvd</td>\n",
       "      <td>41.877749</td>\n",
       "      <td>-87.649633</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   trip_id    usertype  gender            starttime             stoptime  \\\n",
       "0  2355134  Subscriber    Male  2014-06-30 23:57:00  2014-07-01 00:07:00   \n",
       "1  2355133  Subscriber    Male  2014-06-30 23:56:00  2014-07-01 00:00:00   \n",
       "2  2355130  Subscriber    Male  2014-06-30 23:33:00  2014-06-30 23:35:00   \n",
       "3  2355129  Subscriber  Female  2014-06-30 23:26:00  2014-07-01 00:24:00   \n",
       "4  2355128  Subscriber  Female  2014-06-30 23:16:00  2014-06-30 23:26:00   \n",
       "\n",
       "   tripduration  temperature   events  from_station_id  \\\n",
       "0     10.066667         68.0  tstorms              131   \n",
       "1      4.383333         68.0  tstorms              282   \n",
       "2      2.100000         68.0  tstorms              327   \n",
       "3     58.016667         68.0  tstorms              134   \n",
       "4     10.633333         68.0  tstorms              320   \n",
       "\n",
       "             from_station_name  latitude_start  longitude_start  \\\n",
       "0    Lincoln Ave & Belmont Ave       41.939365       -87.668385   \n",
       "1      Halsted St & Maxwell St       41.864580       -87.646930   \n",
       "2  Sheffield Ave & Webster Ave       41.921687       -87.653714   \n",
       "3     Peoria St & Jackson Blvd       41.877749       -87.649633   \n",
       "4     Loomis St & Lexington St       41.872187       -87.661501   \n",
       "\n",
       "   dpcapacity_start  to_station_id           to_station_name  latitude_end  \\\n",
       "0              15.0            303   Broadway & Cornelia Ave     41.945512   \n",
       "1              15.0             22        May St & Taylor St     41.869482   \n",
       "2              19.0            225  Halsted St & Dickens Ave     41.919936   \n",
       "3              19.0            194      State St & Wacker Dr     41.887155   \n",
       "4              15.0            134  Peoria St & Jackson Blvd     41.877749   \n",
       "\n",
       "   longitude_end  dpcapacity_end  \n",
       "0     -87.645980            15.0  \n",
       "1     -87.655486            15.0  \n",
       "2     -87.648830            15.0  \n",
       "3     -87.627750            11.0  \n",
       "4     -87.649633            19.0  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# 加载数据\n",
    "file_path_clean = '/home/sdong/data/chicago_bicycle/data_pr_cleaned.csv'\n",
    "file_path_dirty = '/home/sdong/data/chicago_bicycle/data_pr_raw.csv'\n",
    "data_clean = pd.read_csv(file_path_clean)\n",
    "data_dirty = pd.read_csv(file_path_dirty)\n",
    "\n",
    "# 填充缺失值\n",
    "data_clean.fillna(0, inplace=True)\n",
    "data_dirty.fillna(0, inplace=True)\n",
    "\n",
    "\n",
    "data_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data types after encoding:\n",
      " trip_id                int64\n",
      "usertype               int64\n",
      "gender                 int64\n",
      "starttime              int64\n",
      "stoptime               int64\n",
      "tripduration         float64\n",
      "temperature          float64\n",
      "events                 int64\n",
      "from_station_id        int64\n",
      "from_station_name      int64\n",
      "latitude_start       float64\n",
      "longitude_start      float64\n",
      "dpcapacity_start     float64\n",
      "to_station_id          int64\n",
      "to_station_name        int64\n",
      "latitude_end         float64\n",
      "longitude_end        float64\n",
      "dpcapacity_end       float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# 识别非数值列\n",
    "non_numeric_cols = data_clean.select_dtypes(include=['object']).columns\n",
    "\n",
    "# 对非数值列进行标签编码\n",
    "label_encoders = {}\n",
    "for col in non_numeric_cols:\n",
    "    le = LabelEncoder()\n",
    "    # 合并干净和脏的数据\n",
    "    combined_data = pd.concat([data_clean[col], data_dirty[col]], axis=0)\n",
    "    le.fit(combined_data.astype(str))\n",
    "    # 对干净和脏的数据分别进行转换\n",
    "    data_clean[col] = le.transform(data_clean[col].astype(str))\n",
    "    data_dirty[col] = le.transform(data_dirty[col].astype(str))\n",
    "    label_encoders[col] = le\n",
    "\n",
    "# 确保所有特征都是数值类型\n",
    "print(\"Data types after encoding:\\n\", data_clean.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean = data_clean.astype(np.float64)\n",
    "data_dirty = data_dirty.astype(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "# 分离数值型特征和类别型特征\n",
    "numeric_cols = data_clean.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_cols = list(set(data_clean.columns) - set(numeric_cols))\n",
    "\n",
    "# 初始化标准化器\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# 在干净和脏数据的组合上拟合标准化器\n",
    "combined_numeric_data = pd.concat([data_clean[numeric_cols], data_dirty[numeric_cols]], axis=0)\n",
    "scaler.fit(combined_numeric_data)\n",
    "\n",
    "# 对干净和脏数据进行标准化\n",
    "data_clean[numeric_cols] = scaler.transform(data_clean[numeric_cols])\n",
    "data_dirty[numeric_cols] = scaler.transform(data_dirty[numeric_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data types of data_clean:\n",
      "trip_id              float64\n",
      "usertype             float64\n",
      "gender               float64\n",
      "starttime            float64\n",
      "stoptime             float64\n",
      "tripduration         float64\n",
      "temperature          float64\n",
      "events               float64\n",
      "from_station_id      float64\n",
      "from_station_name    float64\n",
      "latitude_start       float64\n",
      "longitude_start      float64\n",
      "dpcapacity_start     float64\n",
      "to_station_id        float64\n",
      "to_station_name      float64\n",
      "latitude_end         float64\n",
      "longitude_end        float64\n",
      "dpcapacity_end       float64\n",
      "dtype: object\n",
      "Data types of data_dirty:\n",
      "trip_id              float64\n",
      "usertype             float64\n",
      "gender               float64\n",
      "starttime            float64\n",
      "stoptime             float64\n",
      "tripduration         float64\n",
      "temperature          float64\n",
      "events               float64\n",
      "from_station_id      float64\n",
      "from_station_name    float64\n",
      "latitude_start       float64\n",
      "longitude_start      float64\n",
      "dpcapacity_start     float64\n",
      "to_station_id        float64\n",
      "to_station_name      float64\n",
      "latitude_end         float64\n",
      "longitude_end        float64\n",
      "dpcapacity_end       float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(\"Data types of data_clean:\")\n",
    "print(data_clean.dtypes)\n",
    "\n",
    "print(\"Data types of data_dirty:\")\n",
    "print(data_dirty.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trip_id</th>\n",
       "      <th>usertype</th>\n",
       "      <th>gender</th>\n",
       "      <th>starttime</th>\n",
       "      <th>stoptime</th>\n",
       "      <th>tripduration</th>\n",
       "      <th>temperature</th>\n",
       "      <th>events</th>\n",
       "      <th>from_station_id</th>\n",
       "      <th>from_station_name</th>\n",
       "      <th>latitude_start</th>\n",
       "      <th>longitude_start</th>\n",
       "      <th>dpcapacity_start</th>\n",
       "      <th>to_station_id</th>\n",
       "      <th>to_station_name</th>\n",
       "      <th>latitude_end</th>\n",
       "      <th>longitude_end</th>\n",
       "      <th>dpcapacity_end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.134103</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.057209</td>\n",
       "      <td>0.058470</td>\n",
       "      <td>0.000093</td>\n",
       "      <td>0.997216</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.206731</td>\n",
       "      <td>0.549849</td>\n",
       "      <td>0.997030</td>\n",
       "      <td>0.001532</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.482372</td>\n",
       "      <td>0.078550</td>\n",
       "      <td>0.997176</td>\n",
       "      <td>0.001787</td>\n",
       "      <td>0.272727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.134103</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.057208</td>\n",
       "      <td>0.058470</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.997216</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.448718</td>\n",
       "      <td>0.416918</td>\n",
       "      <td>0.995252</td>\n",
       "      <td>0.001776</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.032051</td>\n",
       "      <td>0.605740</td>\n",
       "      <td>0.995368</td>\n",
       "      <td>0.001679</td>\n",
       "      <td>0.272727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.134103</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.057208</td>\n",
       "      <td>0.058470</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.997216</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.520833</td>\n",
       "      <td>0.790030</td>\n",
       "      <td>0.996609</td>\n",
       "      <td>0.001699</td>\n",
       "      <td>0.345455</td>\n",
       "      <td>0.357372</td>\n",
       "      <td>0.410876</td>\n",
       "      <td>0.996568</td>\n",
       "      <td>0.001754</td>\n",
       "      <td>0.272727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.134103</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.057208</td>\n",
       "      <td>0.058470</td>\n",
       "      <td>0.000648</td>\n",
       "      <td>0.997216</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.211538</td>\n",
       "      <td>0.691843</td>\n",
       "      <td>0.995565</td>\n",
       "      <td>0.001745</td>\n",
       "      <td>0.345455</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.861027</td>\n",
       "      <td>0.995788</td>\n",
       "      <td>0.001994</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.134103</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.057208</td>\n",
       "      <td>0.058469</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.997216</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.509615</td>\n",
       "      <td>0.569486</td>\n",
       "      <td>0.995433</td>\n",
       "      <td>0.001610</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.211538</td>\n",
       "      <td>0.691843</td>\n",
       "      <td>0.995565</td>\n",
       "      <td>0.001745</td>\n",
       "      <td>0.345455</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    trip_id  usertype  gender  starttime  stoptime  tripduration  temperature  \\\n",
       "0  0.134103       1.0     1.0   0.057209  0.058470      0.000093     0.997216   \n",
       "1  0.134103       1.0     1.0   0.057208  0.058470      0.000028     0.997216   \n",
       "2  0.134103       1.0     1.0   0.057208  0.058470      0.000001     0.997216   \n",
       "3  0.134103       1.0     0.5   0.057208  0.058470      0.000648     0.997216   \n",
       "4  0.134103       1.0     0.5   0.057208  0.058469      0.000100     0.997216   \n",
       "\n",
       "     events  from_station_id  from_station_name  latitude_start  \\\n",
       "0  0.916667         0.206731           0.549849        0.997030   \n",
       "1  0.916667         0.448718           0.416918        0.995252   \n",
       "2  0.916667         0.520833           0.790030        0.996609   \n",
       "3  0.916667         0.211538           0.691843        0.995565   \n",
       "4  0.916667         0.509615           0.569486        0.995433   \n",
       "\n",
       "   longitude_start  dpcapacity_start  to_station_id  to_station_name  \\\n",
       "0         0.001532          0.272727       0.482372         0.078550   \n",
       "1         0.001776          0.272727       0.032051         0.605740   \n",
       "2         0.001699          0.345455       0.357372         0.410876   \n",
       "3         0.001745          0.345455       0.307692         0.861027   \n",
       "4         0.001610          0.272727       0.211538         0.691843   \n",
       "\n",
       "   latitude_end  longitude_end  dpcapacity_end  \n",
       "0      0.997176       0.001787        0.272727  \n",
       "1      0.995368       0.001679        0.272727  \n",
       "2      0.996568       0.001754        0.272727  \n",
       "3      0.995788       0.001994        0.200000  \n",
       "4      0.995565       0.001745        0.345455  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 特征图构建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sdong/miniconda3/envs/mainenv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "from torch_geometric.utils import from_networkx\n",
    "from torch_geometric.data import Data\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 示例表格数据特征\n",
    "columns = ['trip_id', 'usertype', 'gender',\n",
    "    'starttime', 'stoptime', 'tripduration', 'temperature', 'events',\n",
    "    'from_station_id', 'from_station_name', 'latitude_start', 'longitude_start',\n",
    "    'dpcapacity_start', 'to_station_id', 'to_station_name', 'latitude_end',\n",
    "    'longitude_end', 'dpcapacity_end'\n",
    "    \n",
    "]\n",
    "\n",
    "# 定义特征之间的关系\n",
    "relations = [\n",
    "    ('trip_id', 'usertype'),\n",
    "    ('trip_id', 'gender'),\n",
    "    ('starttime', 'stoptime'),\n",
    "    ('starttime', 'latitude_start'),\n",
    "    ('starttime', 'longitude_start'),\n",
    "    ('stoptime', 'latitude_end'),\n",
    "    ('stoptime', 'longitude_end'),\n",
    "    ('latitude_start', 'longitude_start'),\n",
    "    ('latitude_end', 'longitude_end'),\n",
    "    ('from_station_id', 'from_station_name'),\n",
    "    ('to_station_id', 'to_station_name'),\n",
    "    ('from_station_id', 'latitude_start'),\n",
    "    ('from_station_id', 'longitude_start'),\n",
    "    ('to_station_id', 'latitude_end'),\n",
    "    ('to_station_id', 'longitude_end'),\n",
    "    ('tripduration', 'temperature'),\n",
    "    ('tripduration', 'events'),\n",
    "    ('dpcapacity_start', 'from_station_id'),\n",
    "    ('dpcapacity_end', 'to_station_id')\n",
    "    # 可以添加更多关系\n",
    "]\n",
    "\n",
    "# 创建特征名称到索引的映射\n",
    "feature_to_index = {col: idx for idx, col in enumerate(columns)}\n",
    "index_to_feature = {idx: col for idx, col in enumerate(columns)}\n",
    "\n",
    "# 创建空的无向图\n",
    "G = nx.Graph()\n",
    "\n",
    "# 添加节点（使用索引作为节点）\n",
    "for idx in range(len(columns)):\n",
    "    G.add_node(idx)\n",
    "\n",
    "# 添加边（将特征名称映射到索引）\n",
    "for src, dst in relations:\n",
    "    src_idx = feature_to_index[src]\n",
    "    dst_idx = feature_to_index[dst]\n",
    "    G.add_edge(src_idx, dst_idx)\n",
    "\n",
    "# 将 NetworkX 图转换为 PyTorch Geometric 图\n",
    "data = Data()\n",
    "data.edge_index = torch.tensor(list(G.edges())).t().contiguous()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 新的GNN模型 编码器设计"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GATConv, GINConv\n",
    "\n",
    "class GAT_GIN_Encoder(nn.Module):\n",
    "    def __init__(self, num_features, hidden_channels):\n",
    "        super(GAT_GIN_Encoder, self).__init__()\n",
    "        # 第一层：GATConv\n",
    "        self.gat_conv1 = GATConv(num_features, hidden_channels, heads=8, concat=False)\n",
    "        # 第二层：GINConv\n",
    "        nn1 = nn.Sequential(\n",
    "            nn.Linear(hidden_channels, hidden_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_channels, hidden_channels)\n",
    "        )\n",
    "        self.gin_conv1 = GINConv(nn1)\n",
    "        # 第三层：GATConv\n",
    "        self.gat_conv2 = GATConv(hidden_channels, hidden_channels, heads=8, concat=False)\n",
    "        # 第四层：GINConv\n",
    "        nn2 = nn.Sequential(\n",
    "            nn.Linear(hidden_channels, hidden_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_channels, hidden_channels)\n",
    "        )\n",
    "        self.gin_conv2 = GINConv(nn2)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.relu(self.gat_conv1(x, edge_index))\n",
    "        x = F.relu(self.gin_conv1(x, edge_index))\n",
    "        x = F.relu(self.gat_conv2(x, edge_index))\n",
    "        x = F.relu(self.gin_conv2(x, edge_index))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 解码器设计"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTaskDecoder(nn.Module):\n",
    "    def __init__(self, hidden_channels, num_features):\n",
    "        super(MultiTaskDecoder, self).__init__()\n",
    "        # 数据质量验证解码器\n",
    "        self.decoder_validation = nn.Sequential(\n",
    "            nn.Linear(hidden_channels, hidden_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_channels, num_features)\n",
    "        )\n",
    "        # 数据修复解码器\n",
    "        self.decoder_repair = nn.Sequential(\n",
    "            nn.Linear(hidden_channels, hidden_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_channels, num_features)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 数据质量验证输出\n",
    "        out_validation = self.decoder_validation(x)\n",
    "        # 数据修复输出\n",
    "        out_repair = self.decoder_repair(x)\n",
    "        return out_validation, out_repair\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 整合模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTaskGNN(nn.Module):\n",
    "    def __init__(self, num_features, hidden_channels):\n",
    "        super(MultiTaskGNN, self).__init__()\n",
    "        self.encoder = GAT_GIN_Encoder(num_features, hidden_channels)\n",
    "        self.decoder = MultiTaskDecoder(hidden_channels, num_features)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x = data.x  # x 的形状应为 [num_nodes, num_node_features]\n",
    "        edge_index = data.edge_index\n",
    "        x = self.encoder(x, edge_index)\n",
    "        out_validation, out_repair = self.decoder(x)\n",
    "        return out_validation, out_repair\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 准备数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 准备图数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建节点特征矩阵\n",
    "num_nodes = len(columns)\n",
    "num_features = num_nodes  # 每个节点的特征维度\n",
    "\n",
    "def create_node_features(instance):\n",
    "    # 确保实例中的特征按照 columns 列表的顺序排列\n",
    "    values = instance[columns].values.astype(np.float32)\n",
    "    # 将数据转换为张量，形状为 [num_nodes, num_node_features]\n",
    "    node_features = torch.tensor(values, dtype=torch.float).view(-1, 1)\n",
    "    return node_features\n",
    "\n",
    "\n",
    "# 创建 PyTorch Geometric 数据对象\n",
    "def create_data_object(instance):\n",
    "    node_features = create_node_features(instance)\n",
    "    data_instance = Data()\n",
    "    data_instance.x = node_features\n",
    "    data_instance.edge_index = data.edge_index\n",
    "    data_instance.num_nodes = data.num_nodes\n",
    "    return data_instance\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据集划分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 假设 data_clean 和 data_dirty 已经经过预处理，并且所有列都是数值类型\n",
    "\n",
    "# 将 data_clean 随机打乱，并划分为训练集和临时集（50% / 50%）\n",
    "train_data, temp_data = train_test_split(data_clean, test_size=0.001, random_state=42)\n",
    "\n",
    "# 将临时集再划分为验证集和测试集1（各占25%）\n",
    "val_data, test_data_1 = train_test_split(temp_data, test_size=0.5, random_state=42)\n",
    "\n",
    "# data_dirty 作为测试集2\n",
    "test_data_2 = data_dirty.head(10000)  # 已经预处理好的脏数据\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 18 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   trip_id            10000 non-null  float64\n",
      " 1   usertype           10000 non-null  float64\n",
      " 2   gender             10000 non-null  float64\n",
      " 3   starttime          10000 non-null  float64\n",
      " 4   stoptime           10000 non-null  float64\n",
      " 5   tripduration       10000 non-null  float64\n",
      " 6   temperature        10000 non-null  float64\n",
      " 7   events             10000 non-null  float64\n",
      " 8   from_station_id    10000 non-null  float64\n",
      " 9   from_station_name  10000 non-null  float64\n",
      " 10  latitude_start     10000 non-null  float64\n",
      " 11  longitude_start    10000 non-null  float64\n",
      " 12  dpcapacity_start   10000 non-null  float64\n",
      " 13  to_station_id      10000 non-null  float64\n",
      " 14  to_station_name    10000 non-null  float64\n",
      " 15  latitude_end       10000 non-null  float64\n",
      " 16  longitude_end      10000 non-null  float64\n",
      " 17  dpcapacity_end     10000 non-null  float64\n",
      "dtypes: float64(18)\n",
      "memory usage: 1.4 MB\n"
     ]
    }
   ],
   "source": [
    "test_data_2.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 创建数据加载器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "\n",
    "class GraphDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.dataframe = dataframe\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        instance = self.dataframe.iloc[idx]\n",
    "        data_instance = create_data_object(instance)\n",
    "        return data_instance\n",
    "\n",
    "# 创建数据集对象\n",
    "train_dataset = GraphDataset(train_data)\n",
    "val_dataset = GraphDataset(val_data)\n",
    "test_dataset_1 = GraphDataset(test_data_1)\n",
    "test_dataset_2 = GraphDataset(test_data_2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(out_validation, out_repair, target, lambda_validation=1.0, lambda_repair=1.0):\n",
    "    # 数据质量验证损失\n",
    "    loss_validation = F.mse_loss(out_validation, target)\n",
    "    # 数据修复损失\n",
    "    loss_repair = F.mse_loss(out_repair, target)\n",
    "    # 总损失\n",
    "    loss = lambda_validation * loss_validation + lambda_repair * loss_repair\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 训练循环"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "import warnings\n",
    "\n",
    "# 抑制所有警告\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# 创建数据加载器\n",
    "batch_size = 128\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader_1 = DataLoader(test_dataset_1, batch_size=batch_size, shuffle=False)\n",
    "test_loader_2 = DataLoader(test_dataset_2, batch_size=batch_size, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiTaskGNN(\n",
      "  (encoder): GAT_GIN_Encoder(\n",
      "    (gat_conv1): GATConv(1, 64, heads=8)\n",
      "    (gin_conv1): GINConv(nn=Sequential(\n",
      "      (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "    ))\n",
      "    (gat_conv2): GATConv(64, 64, heads=8)\n",
      "    (gin_conv2): GINConv(nn=Sequential(\n",
      "      (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "    ))\n",
      "  )\n",
      "  (decoder): MultiTaskDecoder(\n",
      "    (decoder_validation): Sequential(\n",
      "      (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=64, out_features=1, bias=True)\n",
      "    )\n",
      "    (decoder_repair): Sequential(\n",
      "      (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=64, out_features=1, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 初始化模型、优化器\n",
    "num_features = 1  # 因为每个节点只有一个特征\n",
    "hidden_channels = 64\n",
    "model = MultiTaskGNN(num_features, hidden_channels)\n",
    "# 打印模型结构\n",
    "print(model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "恢复训练，从 epoch 51 开始\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# 定义保存模型的路径\n",
    "save_path = '/home/sdong/experiments/GVAE/model/multitask_gnn_model.pth'\n",
    "# 定义checkpoint文件的路径\n",
    "checkpoint_path = '/home/sdong/experiments/GVAE/model/checkpoint_bicycle.pth'\n",
    "\n",
    "# 加载已保存的checkpoint\n",
    "if os.path.isfile(checkpoint_path):\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    start_epoch = checkpoint['epoch'] + 1  # 从中断的下一轮开始\n",
    "    loss = checkpoint['loss']\n",
    "\n",
    "    print(f\"恢复训练，从 epoch {start_epoch} 开始\")\n",
    "else:\n",
    "    start_epoch = 1\n",
    "    print(\"没有找到已保存的模型，从头开始训练\")\n",
    "\n",
    "# 训练模型\n",
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        out_validation, out_repair = model(data)\n",
    "        target = data.x\n",
    "        loss = loss_function(out_validation, out_repair, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "# 训练循环\n",
    "num_epochs = 50\n",
    "for epoch in range(start_epoch, num_epochs + 1):\n",
    "    loss = train()\n",
    "    if epoch % 5 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {loss:.4f}')\n",
    "        \n",
    "        with open('train_log.txt', 'a') as f:\n",
    "            if epoch % 5 == 0:\n",
    "                f.write(f'Epoch {epoch}, Loss: {loss:.4f}\\n')\n",
    "                \n",
    "        # 保存新的模型状态\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': loss,\n",
    "        }, checkpoint_path)\n",
    "        print(f'Model saved at epoch {epoch}')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 测试模型\n",
    "### 定义评估函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, data_loader):\n",
    "    model.eval()\n",
    "    reconstruction_errors = []\n",
    "    with torch.no_grad():\n",
    "        for data in data_loader:\n",
    "            out_validation, _ = model(data)\n",
    "            target = data.x\n",
    "            # 计算重构误差\n",
    "            loss = F.mse_loss(out_validation, target, reduction='none')\n",
    "            # 对每个样本计算平均误差\n",
    "            loss_per_sample = loss.mean(dim=1)\n",
    "            reconstruction_errors.extend(loss_per_sample.tolist())\n",
    "    # 返回所有样本的重构误差列表\n",
    "    return reconstruction_errors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 计算阈值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss threshold for detecting data quality issues: 6.1952723626745865e-06\n",
      "Min validation error: 2.7200464103316335e-15\n",
      "Max validation error: 0.0030698971822857857\n",
      "Mean validation error: 2.8008653246326952e-06\n",
      "95th percentile of validation errors: 6.1952723626745865e-06\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 收集验证集的重构误差\n",
    "val_errors = evaluate_model(model, val_loader)\n",
    "\n",
    "# 计算95%分位数作为阈值\n",
    "threshold = np.quantile(val_errors, 0.95)\n",
    "print(f\"Loss threshold for detecting data quality issues: {threshold}\")\n",
    "\n",
    "# 打印一些统计信息\n",
    "print(f\"Min validation error: {min(val_errors)}\")\n",
    "print(f\"Max validation error: {max(val_errors)}\")\n",
    "print(f\"Mean validation error: {np.mean(val_errors)}\")\n",
    "print(f\"95th percentile of validation errors: {threshold}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 检测数据质量问题\n",
    "### 定义检测函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_quality_issues(model, data_loader, threshold):\n",
    "    model.eval()\n",
    "    total_issues = 0\n",
    "    total_samples = 0\n",
    "    with torch.no_grad():\n",
    "        for data in data_loader:\n",
    "            out_validation, _ = model(data)\n",
    "            target = data.x\n",
    "            # 计算重构误差\n",
    "            loss = F.mse_loss(out_validation, target, reduction='none')\n",
    "            loss_per_sample = loss.mean(dim=1)\n",
    "            # 检测超过阈值的样本\n",
    "            issues = (loss_per_sample > threshold).sum().item()\n",
    "            total_issues += issues\n",
    "            total_samples += loss_per_sample.size(0)\n",
    "    # 计算有问题的样本比例\n",
    "    issue_ratio = total_issues / total_samples\n",
    "    return total_issues, total_samples, issue_ratio\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 在测试集1和测试集2上检测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set 1 (Clean Data): 467/8550 samples are faulty (5.46%)\n",
      "Test Set 2 (Dirty Data): 489164/1800000 samples are faulty (27.18%)\n"
     ]
    }
   ],
   "source": [
    "# 检测测试集1（干净数据）的质量问题\n",
    "issues_test1, samples_test1, ratio_test1 = detect_quality_issues(model, test_loader_1, threshold)\n",
    "print(f\"Test Set 1 (Clean Data): {issues_test1}/{samples_test1} samples are faulty ({ratio_test1 * 100:.2f}%)\")\n",
    "\n",
    "# 检测测试集2（脏数据）的质量问题\n",
    "issues_test2, samples_test2, ratio_test2 = detect_quality_issues(model, test_loader_2, threshold)\n",
    "print(f\"Test Set 2 (Dirty Data): {issues_test2}/{samples_test2} samples are faulty ({ratio_test2 * 100:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 额外的测试：随机采样测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_random_samples(model, data, threshold, num_tests=50):\n",
    "    from sklearn.utils import shuffle\n",
    "    problematic_batches = 0\n",
    "    total_tests = num_tests\n",
    "    for seed in range(num_tests):\n",
    "        # 随机采样20%的数据\n",
    "        sample_data = data.sample(frac=0.2, random_state=seed).reset_index(drop=True)\n",
    "        sample_dataset = GraphDataset(sample_data)\n",
    "        sample_loader = DataLoader(sample_dataset, batch_size=batch_size, shuffle=False)\n",
    "        issues, samples, ratio = detect_quality_issues(model, sample_loader, threshold)\n",
    "        if ratio > 0.06:  # 超过6%的样本有问题\n",
    "            print(f\"Random sample {seed} is problematic: {issues} out of {samples} samples are faulty ({ratio * 100:.2f}%).\")\n",
    "            problematic_batches += 1\n",
    "        else:\n",
    "            print(f\"Random sample {seed} is ok: {issues} out of {samples} samples are faulty ({ratio * 100:.2f}%).\")\n",
    "    print(f\"Total problematic batches across all tests: {problematic_batches}/{total_tests}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing on Test Set 1 (Clean Data):\n",
      "Random sample 0 is ok: 91 out of 1710 samples are faulty (5.32%).\n",
      "Random sample 1 is ok: 83 out of 1710 samples are faulty (4.85%).\n",
      "Random sample 2 is ok: 89 out of 1710 samples are faulty (5.20%).\n",
      "Random sample 3 is ok: 89 out of 1710 samples are faulty (5.20%).\n",
      "Random sample 4 is ok: 94 out of 1710 samples are faulty (5.50%).\n",
      "Random sample 5 is ok: 86 out of 1710 samples are faulty (5.03%).\n",
      "Random sample 6 is ok: 100 out of 1710 samples are faulty (5.85%).\n",
      "Random sample 7 is ok: 91 out of 1710 samples are faulty (5.32%).\n",
      "Random sample 8 is problematic: 103 out of 1710 samples are faulty (6.02%).\n",
      "Random sample 9 is ok: 94 out of 1710 samples are faulty (5.50%).\n",
      "Random sample 10 is problematic: 110 out of 1710 samples are faulty (6.43%).\n",
      "Random sample 11 is ok: 80 out of 1710 samples are faulty (4.68%).\n",
      "Random sample 12 is ok: 89 out of 1710 samples are faulty (5.20%).\n",
      "Random sample 13 is ok: 102 out of 1710 samples are faulty (5.96%).\n",
      "Random sample 14 is ok: 91 out of 1710 samples are faulty (5.32%).\n",
      "Random sample 15 is ok: 102 out of 1710 samples are faulty (5.96%).\n",
      "Random sample 16 is ok: 92 out of 1710 samples are faulty (5.38%).\n",
      "Random sample 17 is ok: 81 out of 1710 samples are faulty (4.74%).\n",
      "Random sample 18 is ok: 96 out of 1710 samples are faulty (5.61%).\n",
      "Random sample 19 is ok: 91 out of 1710 samples are faulty (5.32%).\n",
      "Random sample 20 is ok: 100 out of 1710 samples are faulty (5.85%).\n",
      "Random sample 21 is problematic: 105 out of 1710 samples are faulty (6.14%).\n",
      "Random sample 22 is ok: 98 out of 1710 samples are faulty (5.73%).\n",
      "Random sample 23 is ok: 91 out of 1710 samples are faulty (5.32%).\n",
      "Random sample 24 is ok: 94 out of 1710 samples are faulty (5.50%).\n",
      "Random sample 25 is ok: 81 out of 1710 samples are faulty (4.74%).\n",
      "Random sample 26 is ok: 87 out of 1710 samples are faulty (5.09%).\n",
      "Random sample 27 is ok: 99 out of 1710 samples are faulty (5.79%).\n",
      "Random sample 28 is ok: 88 out of 1710 samples are faulty (5.15%).\n",
      "Random sample 29 is ok: 88 out of 1710 samples are faulty (5.15%).\n",
      "Random sample 30 is ok: 90 out of 1710 samples are faulty (5.26%).\n",
      "Random sample 31 is ok: 85 out of 1710 samples are faulty (4.97%).\n",
      "Random sample 32 is ok: 90 out of 1710 samples are faulty (5.26%).\n",
      "Random sample 33 is ok: 93 out of 1710 samples are faulty (5.44%).\n",
      "Random sample 34 is problematic: 104 out of 1710 samples are faulty (6.08%).\n",
      "Random sample 35 is ok: 96 out of 1710 samples are faulty (5.61%).\n",
      "Random sample 36 is ok: 88 out of 1710 samples are faulty (5.15%).\n",
      "Random sample 37 is ok: 97 out of 1710 samples are faulty (5.67%).\n",
      "Random sample 38 is ok: 94 out of 1710 samples are faulty (5.50%).\n",
      "Random sample 39 is ok: 88 out of 1710 samples are faulty (5.15%).\n",
      "Random sample 40 is ok: 88 out of 1710 samples are faulty (5.15%).\n",
      "Random sample 41 is ok: 100 out of 1710 samples are faulty (5.85%).\n",
      "Random sample 42 is ok: 97 out of 1710 samples are faulty (5.67%).\n",
      "Random sample 43 is ok: 95 out of 1710 samples are faulty (5.56%).\n",
      "Random sample 44 is ok: 94 out of 1710 samples are faulty (5.50%).\n",
      "Random sample 45 is problematic: 108 out of 1710 samples are faulty (6.32%).\n",
      "Random sample 46 is ok: 84 out of 1710 samples are faulty (4.91%).\n",
      "Random sample 47 is ok: 97 out of 1710 samples are faulty (5.67%).\n",
      "Random sample 48 is problematic: 107 out of 1710 samples are faulty (6.26%).\n",
      "Random sample 49 is ok: 96 out of 1710 samples are faulty (5.61%).\n",
      "Total problematic batches across all tests: 6/50\n",
      "Testing on Test Set 2 (Dirty Data):\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# 对测试集2（脏数据）执行随机采样测试\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTesting on Test Set 2 (Dirty Data):\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m \u001b[43mtest_random_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_data_2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[30], line 10\u001b[0m, in \u001b[0;36mtest_random_samples\u001b[0;34m(model, data, threshold, num_tests)\u001b[0m\n\u001b[1;32m      8\u001b[0m sample_dataset \u001b[38;5;241m=\u001b[39m GraphDataset(sample_data)\n\u001b[1;32m      9\u001b[0m sample_loader \u001b[38;5;241m=\u001b[39m DataLoader(sample_dataset, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m---> 10\u001b[0m issues, samples, ratio \u001b[38;5;241m=\u001b[39m \u001b[43mdetect_quality_issues\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ratio \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.06\u001b[39m:  \u001b[38;5;66;03m# 超过6%的样本有问题\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRandom sample \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mseed\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is problematic: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00missues\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m out of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msamples\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m samples are faulty (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mratio\u001b[38;5;250m \u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m100\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%).\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[28], line 7\u001b[0m, in \u001b[0;36mdetect_quality_issues\u001b[0;34m(model, data_loader, threshold)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m data_loader:\n\u001b[0;32m----> 7\u001b[0m         out_validation, _ \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m         target \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mx\n\u001b[1;32m      9\u001b[0m         \u001b[38;5;66;03m# 计算重构误差\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/mainenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mainenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[12], line 11\u001b[0m, in \u001b[0;36mMultiTaskGNN.forward\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m      9\u001b[0m edge_index \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39medge_index\n\u001b[1;32m     10\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(x, edge_index)\n\u001b[0;32m---> 11\u001b[0m out_validation, out_repair \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out_validation, out_repair\n",
      "File \u001b[0;32m~/miniconda3/envs/mainenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mainenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[11], line 19\u001b[0m, in \u001b[0;36mMultiTaskDecoder.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;66;03m# 数据质量验证输出\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m     out_validation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder_validation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;66;03m# 数据修复输出\u001b[39;00m\n\u001b[1;32m     21\u001b[0m     out_repair \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder_repair(x)\n",
      "File \u001b[0;32m~/miniconda3/envs/mainenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mainenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/mainenv/lib/python3.11/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/mainenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mainenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/mainenv/lib/python3.11/site-packages/torch/nn/modules/activation.py:103\u001b[0m, in \u001b[0;36mReLU.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 103\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mainenv/lib/python3.11/site-packages/torch/nn/functional.py:1500\u001b[0m, in \u001b[0;36mrelu\u001b[0;34m(input, inplace)\u001b[0m\n\u001b[1;32m   1498\u001b[0m     result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu_(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m   1499\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1500\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1501\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 对测试集1（干净数据）执行随机采样测试\n",
    "print(\"Testing on Test Set 1 (Clean Data):\")\n",
    "test_random_samples(model, test_data_1, threshold)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing on Test Set 2 (Dirty Data):\n",
      "Random sample 0 is problematic: 7263 out of 36000 samples are faulty (20.18%).\n",
      "Random sample 1 is problematic: 7102 out of 36000 samples are faulty (19.73%).\n",
      "Random sample 2 is problematic: 7299 out of 36000 samples are faulty (20.28%).\n",
      "Random sample 3 is problematic: 7191 out of 36000 samples are faulty (19.98%).\n",
      "Random sample 4 is problematic: 7173 out of 36000 samples are faulty (19.93%).\n",
      "Random sample 5 is problematic: 7260 out of 36000 samples are faulty (20.17%).\n",
      "Random sample 6 is problematic: 7109 out of 36000 samples are faulty (19.75%).\n",
      "Random sample 7 is problematic: 7252 out of 36000 samples are faulty (20.14%).\n",
      "Random sample 8 is problematic: 7191 out of 36000 samples are faulty (19.98%).\n",
      "Random sample 9 is problematic: 7174 out of 36000 samples are faulty (19.93%).\n",
      "Random sample 10 is problematic: 7238 out of 36000 samples are faulty (20.11%).\n",
      "Random sample 11 is problematic: 7134 out of 36000 samples are faulty (19.82%).\n",
      "Random sample 12 is problematic: 7295 out of 36000 samples are faulty (20.26%).\n",
      "Random sample 13 is problematic: 7096 out of 36000 samples are faulty (19.71%).\n",
      "Random sample 14 is problematic: 7216 out of 36000 samples are faulty (20.04%).\n",
      "Random sample 15 is problematic: 7160 out of 36000 samples are faulty (19.89%).\n",
      "Random sample 16 is problematic: 7134 out of 36000 samples are faulty (19.82%).\n",
      "Random sample 17 is problematic: 7141 out of 36000 samples are faulty (19.84%).\n",
      "Random sample 18 is problematic: 7174 out of 36000 samples are faulty (19.93%).\n",
      "Random sample 19 is problematic: 7138 out of 36000 samples are faulty (19.83%).\n",
      "Random sample 20 is problematic: 7226 out of 36000 samples are faulty (20.07%).\n",
      "Random sample 21 is problematic: 7227 out of 36000 samples are faulty (20.08%).\n",
      "Random sample 22 is problematic: 7143 out of 36000 samples are faulty (19.84%).\n",
      "Random sample 23 is problematic: 7134 out of 36000 samples are faulty (19.82%).\n",
      "Random sample 24 is problematic: 7121 out of 36000 samples are faulty (19.78%).\n",
      "Random sample 25 is problematic: 7131 out of 36000 samples are faulty (19.81%).\n",
      "Random sample 26 is problematic: 7261 out of 36000 samples are faulty (20.17%).\n",
      "Random sample 27 is problematic: 7158 out of 36000 samples are faulty (19.88%).\n",
      "Random sample 28 is problematic: 7119 out of 36000 samples are faulty (19.78%).\n",
      "Random sample 29 is problematic: 7160 out of 36000 samples are faulty (19.89%).\n",
      "Random sample 30 is problematic: 7101 out of 36000 samples are faulty (19.73%).\n",
      "Random sample 31 is problematic: 7237 out of 36000 samples are faulty (20.10%).\n",
      "Random sample 32 is problematic: 7183 out of 36000 samples are faulty (19.95%).\n",
      "Random sample 33 is problematic: 7202 out of 36000 samples are faulty (20.01%).\n",
      "Random sample 34 is problematic: 7256 out of 36000 samples are faulty (20.16%).\n",
      "Random sample 35 is problematic: 7188 out of 36000 samples are faulty (19.97%).\n",
      "Random sample 36 is problematic: 7195 out of 36000 samples are faulty (19.99%).\n",
      "Random sample 37 is problematic: 7225 out of 36000 samples are faulty (20.07%).\n",
      "Random sample 38 is problematic: 7137 out of 36000 samples are faulty (19.82%).\n",
      "Random sample 39 is problematic: 7152 out of 36000 samples are faulty (19.87%).\n",
      "Random sample 40 is problematic: 7236 out of 36000 samples are faulty (20.10%).\n",
      "Random sample 41 is problematic: 7136 out of 36000 samples are faulty (19.82%).\n",
      "Random sample 42 is problematic: 7216 out of 36000 samples are faulty (20.04%).\n",
      "Random sample 43 is problematic: 7196 out of 36000 samples are faulty (19.99%).\n",
      "Random sample 44 is problematic: 7189 out of 36000 samples are faulty (19.97%).\n",
      "Random sample 45 is problematic: 7179 out of 36000 samples are faulty (19.94%).\n",
      "Random sample 46 is problematic: 7311 out of 36000 samples are faulty (20.31%).\n",
      "Random sample 47 is problematic: 7268 out of 36000 samples are faulty (20.19%).\n",
      "Random sample 48 is problematic: 7198 out of 36000 samples are faulty (19.99%).\n",
      "Random sample 49 is problematic: 7150 out of 36000 samples are faulty (19.86%).\n",
      "Total problematic batches across all tests: 50/50\n"
     ]
    }
   ],
   "source": [
    "# 对测试集2（脏数据）执行随机采样测试\n",
    "print(\"Testing on Test Set 2 (Dirty Data):\")\n",
    "test_random_samples(model, test_data_2, threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary of Overall Correct Ratios for Different Sample Sizes:\n",
      "Sample Size 10: Correct Ratio = 86.00%\n",
      "Sample Size 20: Correct Ratio = 92.00%\n",
      "Sample Size 50: Correct Ratio = 89.00%\n",
      "Sample Size 100: Correct Ratio = 97.00%\n",
      "Sample Size 500: Correct Ratio = 100.00%\n",
      "Sample Size 1000: Correct Ratio = 100.00%\n"
     ]
    }
   ],
   "source": [
    "def test_random_samples(model, data, threshold, num_tests=50, sample_size=0.2):\n",
    "    from sklearn.utils import shuffle\n",
    "    problematic_batches = 0\n",
    "    total_tests = num_tests\n",
    "\n",
    "    for seed in range(num_tests):\n",
    "        # 随机采样指定数量的数据\n",
    "        sample_data = data.sample(n=sample_size, random_state=seed).reset_index(drop=True)\n",
    "        sample_dataset = GraphDataset(sample_data)\n",
    "        sample_loader = DataLoader(sample_dataset, batch_size=batch_size, shuffle=False)\n",
    "        issues, samples, ratio = detect_quality_issues(model, sample_loader, threshold)\n",
    "\n",
    "        if ratio > 0.06:  # 超过6%的样本有问题\n",
    "            #print(f\"Random sample {seed} is problematic: {issues} out of {samples} samples are faulty ({ratio * 100:.2f}%).\")\n",
    "            problematic_batches += 1\n",
    "        #else:\n",
    "            #print(f\"Random sample {seed} is ok: {issues} out of {samples} samples are faulty ({ratio * 100:.2f}%).\")\n",
    "\n",
    "    correct_batches = total_tests - problematic_batches\n",
    "    correct_ratio = correct_batches / total_tests\n",
    "\n",
    "    # print(f\"Total problematic batches across all tests: {problematic_batches}/{total_tests}\")\n",
    "    # print(f\"Correct ratio: {correct_batches}/{total_tests} ({correct_ratio * 100:.2f}%)\")\n",
    "\n",
    "    return correct_batches, total_tests\n",
    "\n",
    "# 定义不同的采样大小\n",
    "sample_sizes = [10 ,20,50, 100, 500, 1000]\n",
    "\n",
    "# 用于保存每个采样大小下的总体正确率\n",
    "overall_correct_ratios = []\n",
    "\n",
    "for sample_size in sample_sizes:\n",
    "    #print(f\"\\nTesting with sample size: {sample_size}\")\n",
    "\n",
    "    # 对测试集1（干净数据）执行随机采样测试\n",
    "    #print(f\"Testing on Test Set 1 (Clean Data) with sample size {sample_size}:\")\n",
    "    correct_batches_1, total_tests_1 = test_random_samples(model, test_data_1, threshold, sample_size=sample_size)\n",
    "\n",
    "    # 对测试集2（脏数据）执行随机采样测试\n",
    "    #print(f\"Testing on Test Set 2 (Dirty Data) with sample size {sample_size}:\")\n",
    "    correct_batches_2, total_tests_2 = test_random_samples(model, test_data_2, threshold, sample_size=sample_size)\n",
    "\n",
    "    # 计算总的预测正确的百分比\n",
    "    total_correct_batches = correct_batches_1 + (50-correct_batches_2)\n",
    "    total_tests = total_tests_1 + total_tests_2\n",
    "    overall_correct_ratio = total_correct_batches / total_tests\n",
    "\n",
    "    overall_correct_ratios.append((sample_size, overall_correct_ratio))\n",
    "\n",
    "    #print(f\"\\nOverall correct ratio for sample size {sample_size}: {total_correct_batches}/{total_tests} ({overall_correct_ratio * 100:.2f}%)\")\n",
    "\n",
    "# 显示不同采样大小下的总体正确率\n",
    "print(\"\\nSummary of Overall Correct Ratios for Different Sample Sizes:\")\n",
    "for sample_size, ratio in overall_correct_ratios:\n",
    "    print(f\"Sample Size {sample_size}: Correct Ratio = {ratio * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 数据质量验证和数据修复\n",
    "### 6.1 检测数据质量问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 修复数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repair_data(model, data_loader):\n",
    "    model.eval()\n",
    "    repaired_data = []\n",
    "    with torch.no_grad():\n",
    "        for data in data_loader:\n",
    "            _, out_repair = model(data)\n",
    "            repaired_instance = out_repair.squeeze().numpy()\n",
    "            repaired_data.append(repaired_instance)\n",
    "    repaired_data = np.array(repaired_data)\n",
    "    return repaired_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. 应用于脏数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m threshold \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.1\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# 检测数据质量问题\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m issues_detected, total_samples \u001b[38;5;241m=\u001b[39m \u001b[43mdetect_quality_issues\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# 修复数据\u001b[39;00m\n\u001b[1;32m     11\u001b[0m repaired_data \u001b[38;5;241m=\u001b[39m repair_data(model, test_loader)\n",
      "Cell \u001b[0;32mIn[13], line 7\u001b[0m, in \u001b[0;36mdetect_quality_issues\u001b[0;34m(model, data_loader, threshold)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m data_loader:\n\u001b[0;32m----> 7\u001b[0m         out_validation, _ \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m         target \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mx\n\u001b[1;32m      9\u001b[0m         loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmse_loss(out_validation, target, reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/mainenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mainenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[8], line 10\u001b[0m, in \u001b[0;36mMultiTaskGNN.forward\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m      8\u001b[0m x \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mx  \u001b[38;5;66;03m# x 的形状应为 [num_nodes, num_node_features]\u001b[39;00m\n\u001b[1;32m      9\u001b[0m edge_index \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39medge_index\n\u001b[0;32m---> 10\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m out_validation, out_repair \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(x)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out_validation, out_repair\n",
      "File \u001b[0;32m~/miniconda3/envs/mainenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mainenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[6], line 31\u001b[0m, in \u001b[0;36mGAT_GIN_Encoder.forward\u001b[0;34m(self, x, edge_index)\u001b[0m\n\u001b[1;32m     29\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgat_conv1(x, edge_index))\n\u001b[1;32m     30\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgin_conv1(x, edge_index))\n\u001b[0;32m---> 31\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgat_conv2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     32\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgin_conv2(x, edge_index))\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/miniconda3/envs/mainenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mainenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/mainenv/lib/python3.11/site-packages/torch_geometric/nn/conv/gat_conv.py:341\u001b[0m, in \u001b[0;36mGATConv.forward\u001b[0;34m(self, x, edge_index, edge_attr, size, return_attention_weights)\u001b[0m\n\u001b[1;32m    337\u001b[0m alpha \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39medge_updater(edge_index, alpha\u001b[38;5;241m=\u001b[39malpha, edge_attr\u001b[38;5;241m=\u001b[39medge_attr,\n\u001b[1;32m    338\u001b[0m                           size\u001b[38;5;241m=\u001b[39msize)\n\u001b[1;32m    340\u001b[0m \u001b[38;5;66;03m# propagate_type: (x: OptPairTensor, alpha: Tensor)\u001b[39;00m\n\u001b[0;32m--> 341\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpropagate\u001b[49m\u001b[43m(\u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconcat:\n\u001b[1;32m    344\u001b[0m     out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheads \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_channels)\n",
      "File \u001b[0;32m/tmp/torch_geometric.nn.conv.gat_conv_GATConv_propagate_i3l3pxlu.py:197\u001b[0m, in \u001b[0;36mpropagate\u001b[0;34m(self, edge_index, x, alpha, size)\u001b[0m\n\u001b[1;32m    188\u001b[0m             kwargs \u001b[38;5;241m=\u001b[39m CollectArgs(\n\u001b[1;32m    189\u001b[0m                 x_j\u001b[38;5;241m=\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mx_j,\n\u001b[1;32m    190\u001b[0m                 alpha\u001b[38;5;241m=\u001b[39mkwargs\u001b[38;5;241m.\u001b[39malpha,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    193\u001b[0m                 dim_size\u001b[38;5;241m=\u001b[39mhook_kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdim_size\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    194\u001b[0m             )\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m# End Aggregate Forward Pre Hook #######################################\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maggregate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m    \u001b[49m\u001b[43mptr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mptr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdim_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdim_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;66;03m# Begin Aggregate Forward Hook #########################################\u001b[39;00m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_scripting() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_compiling():\n",
      "File \u001b[0;32m~/miniconda3/envs/mainenv/lib/python3.11/site-packages/torch_geometric/nn/conv/message_passing.py:625\u001b[0m, in \u001b[0;36mMessagePassing.aggregate\u001b[0;34m(self, inputs, index, ptr, dim_size)\u001b[0m\n\u001b[1;32m    608\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21maggregate\u001b[39m(\n\u001b[1;32m    609\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    610\u001b[0m     inputs: Tensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    613\u001b[0m     dim_size: Optional[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    614\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m    615\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Aggregates messages from neighbors as\u001b[39;00m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;124;03m    :math:`\\bigoplus_{j \\in \\mathcal{N}(i)}`.\u001b[39;00m\n\u001b[1;32m    617\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    623\u001b[0m \u001b[38;5;124;03m    as specified in :meth:`__init__` by the :obj:`aggr` argument.\u001b[39;00m\n\u001b[1;32m    624\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 625\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maggr_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mptr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mptr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdim_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    626\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnode_dim\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mainenv/lib/python3.11/site-packages/torch_geometric/experimental.py:117\u001b[0m, in \u001b[0;36mdisable_dynamic_shapes.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_experimental_mode_enabled(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdisable_dynamic_shapes\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m--> 117\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m required_arg \u001b[38;5;129;01min\u001b[39;00m required_args:\n\u001b[1;32m    120\u001b[0m         index \u001b[38;5;241m=\u001b[39m required_args_pos[required_arg]\n",
      "File \u001b[0;32m~/miniconda3/envs/mainenv/lib/python3.11/site-packages/torch_geometric/nn/aggr/base.py:128\u001b[0m, in \u001b[0;36mAggregation.__call__\u001b[0;34m(self, x, index, ptr, dim_size, dim, **kwargs)\u001b[0m\n\u001b[1;32m    125\u001b[0m     dim_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(index\u001b[38;5;241m.\u001b[39mmax()) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m index\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 128\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mptr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mptr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdim_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mIndexError\u001b[39;00m, \u001b[38;5;167;01mRuntimeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/mainenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mainenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/mainenv/lib/python3.11/site-packages/torch_geometric/nn/aggr/basic.py:22\u001b[0m, in \u001b[0;36mSumAggregation.forward\u001b[0;34m(self, x, index, ptr, dim_size, dim)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor, index: Optional[Tensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     20\u001b[0m             ptr: Optional[Tensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, dim_size: Optional[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     21\u001b[0m             dim: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m---> 22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mptr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduce\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msum\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mainenv/lib/python3.11/site-packages/torch_geometric/nn/aggr/base.py:182\u001b[0m, in \u001b[0;36mAggregation.reduce\u001b[0;34m(self, x, index, ptr, dim_size, dim, reduce)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    181\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAggregation requires \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to be specified\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 182\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mscatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduce\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mainenv/lib/python3.11/site-packages/torch_geometric/utils/_scatter.py:75\u001b[0m, in \u001b[0;36mscatter\u001b[0;34m(src, index, dim, dim_size, reduce)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reduce \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124madd\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     74\u001b[0m     index \u001b[38;5;241m=\u001b[39m broadcast(index, src, dim)\n\u001b[0;32m---> 75\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msrc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnew_zeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscatter_add_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reduce \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     78\u001b[0m     count \u001b[38;5;241m=\u001b[39m src\u001b[38;5;241m.\u001b[39mnew_zeros(dim_size)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 创建脏数据的加载器\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "# 设置阈值，可以根据验证集的损失分布确定，这里暂时设置为经验值\n",
    "threshold = 0.1\n",
    "\n",
    "# 检测数据质量问题\n",
    "issues_detected, total_samples = detect_quality_issues(model, test_loader, threshold)\n",
    "\n",
    "# 修复数据\n",
    "repaired_data = repair_data(model, test_loader)\n",
    "\n",
    "# 将修复后的数据保存\n",
    "repaired_df = pd.DataFrame(repaired_data, columns=columns)\n",
    "repaired_df.to_csv('repaired_data.csv', index=False)\n",
    "print(\"Repaired data saved to repaired_data.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mainenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
