{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>host_id</th>\n",
       "      <th>host_identity_verified</th>\n",
       "      <th>host_name</th>\n",
       "      <th>neighbourhood_group</th>\n",
       "      <th>neighbourhood</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>instant_bookable</th>\n",
       "      <th>...</th>\n",
       "      <th>price</th>\n",
       "      <th>service_fee</th>\n",
       "      <th>minimum_nights</th>\n",
       "      <th>number_of_reviews</th>\n",
       "      <th>last_review</th>\n",
       "      <th>reviews_per_month</th>\n",
       "      <th>review_rate_number</th>\n",
       "      <th>calculated_host_listings_count</th>\n",
       "      <th>availability_365</th>\n",
       "      <th>house_rules</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1001254</td>\n",
       "      <td>Clean &amp; quiet apt home by the park</td>\n",
       "      <td>80014485718</td>\n",
       "      <td>unconfirmed</td>\n",
       "      <td>Madaline</td>\n",
       "      <td>Brooklyn</td>\n",
       "      <td>Kensington</td>\n",
       "      <td>40.64749</td>\n",
       "      <td>-73.97237</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>966.0</td>\n",
       "      <td>193.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2021-10-19</td>\n",
       "      <td>0.21</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>286.0</td>\n",
       "      <td>Clean up and treat the home the way you'd like...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1002102</td>\n",
       "      <td>Skylit Midtown Castle</td>\n",
       "      <td>52335172823</td>\n",
       "      <td>verified</td>\n",
       "      <td>Jenna</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>Midtown</td>\n",
       "      <td>40.75362</td>\n",
       "      <td>-73.98377</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>142.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>2022-05-21</td>\n",
       "      <td>0.38</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>228.0</td>\n",
       "      <td>Pet friendly but please confirm with me if the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1002403</td>\n",
       "      <td>THE VILLAGE OF HARLEM....NEW YORK !</td>\n",
       "      <td>78829239556</td>\n",
       "      <td>unconfirmed</td>\n",
       "      <td>Elise</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>Harlem</td>\n",
       "      <td>40.80902</td>\n",
       "      <td>-73.94190</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>620.0</td>\n",
       "      <td>124.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2019-06-14</td>\n",
       "      <td>0.79</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>352.0</td>\n",
       "      <td>I encourage you to use my kitchen, cooking and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1002755</td>\n",
       "      <td>blank</td>\n",
       "      <td>85098326012</td>\n",
       "      <td>unconfirmed</td>\n",
       "      <td>Garry</td>\n",
       "      <td>Brooklyn</td>\n",
       "      <td>Clinton Hill</td>\n",
       "      <td>40.68514</td>\n",
       "      <td>-73.95976</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>368.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>270.0</td>\n",
       "      <td>2019-07-05</td>\n",
       "      <td>4.64</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>322.0</td>\n",
       "      <td>blank</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1003689</td>\n",
       "      <td>Entire Apt: Spacious Studio/Loft by central park</td>\n",
       "      <td>92037596077</td>\n",
       "      <td>verified</td>\n",
       "      <td>Lyndon</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>East Harlem</td>\n",
       "      <td>40.79851</td>\n",
       "      <td>-73.94399</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>204.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2018-11-19</td>\n",
       "      <td>0.10</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>289.0</td>\n",
       "      <td>Please no smoking in the house, porch or on th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                              name      host_id  \\\n",
       "0  1001254                Clean & quiet apt home by the park  80014485718   \n",
       "1  1002102                             Skylit Midtown Castle  52335172823   \n",
       "2  1002403               THE VILLAGE OF HARLEM....NEW YORK !  78829239556   \n",
       "3  1002755                                             blank  85098326012   \n",
       "4  1003689  Entire Apt: Spacious Studio/Loft by central park  92037596077   \n",
       "\n",
       "  host_identity_verified host_name neighbourhood_group neighbourhood  \\\n",
       "0            unconfirmed  Madaline            Brooklyn    Kensington   \n",
       "1               verified     Jenna           Manhattan       Midtown   \n",
       "2            unconfirmed     Elise           Manhattan        Harlem   \n",
       "3            unconfirmed     Garry            Brooklyn  Clinton Hill   \n",
       "4               verified    Lyndon           Manhattan   East Harlem   \n",
       "\n",
       "        lat      long  instant_bookable  ...  price service_fee  \\\n",
       "0  40.64749 -73.97237             False  ...  966.0       193.0   \n",
       "1  40.75362 -73.98377             False  ...  142.0        28.0   \n",
       "2  40.80902 -73.94190              True  ...  620.0       124.0   \n",
       "3  40.68514 -73.95976              True  ...  368.0        74.0   \n",
       "4  40.79851 -73.94399             False  ...  204.0        41.0   \n",
       "\n",
       "   minimum_nights  number_of_reviews  last_review  reviews_per_month  \\\n",
       "0            10.0                9.0   2021-10-19               0.21   \n",
       "1            13.0               45.0   2022-05-21               0.38   \n",
       "2             3.0                0.0   2019-06-14               0.79   \n",
       "3            13.0              270.0   2019-07-05               4.64   \n",
       "4            10.0                9.0   2018-11-19               0.10   \n",
       "\n",
       "   review_rate_number calculated_host_listings_count  availability_365  \\\n",
       "0                 4.0                            6.0             286.0   \n",
       "1                 4.0                            2.0             228.0   \n",
       "2                 5.0                            1.0             352.0   \n",
       "3                 4.0                            1.0             322.0   \n",
       "4                 3.0                            1.0             289.0   \n",
       "\n",
       "                                         house_rules  \n",
       "0  Clean up and treat the home the way you'd like...  \n",
       "1  Pet friendly but please confirm with me if the...  \n",
       "2  I encourage you to use my kitchen, cooking and...  \n",
       "3                                              blank  \n",
       "4  Please no smoking in the house, porch or on th...  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# 加载数据\n",
    "file_path_clean = '/home/sdong/data/airbnb/airbnb_nyc_clean.csv'\n",
    "file_path_dirty = '/home/sdong/data/airbnb/Airbnb_Open_Data_Alignement.csv'\n",
    "data_clean = pd.read_csv(file_path_clean)\n",
    "data_dirty = pd.read_csv(file_path_dirty)\n",
    "\n",
    "# 填充缺失值\n",
    "data_clean.fillna(0, inplace=True)\n",
    "data_dirty.fillna(0, inplace=True)\n",
    "\n",
    "\n",
    "data_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data types after encoding:\n",
      " id                                  int64\n",
      "name                                int64\n",
      "host_id                             int64\n",
      "host_identity_verified              int64\n",
      "host_name                           int64\n",
      "neighbourhood_group                 int64\n",
      "neighbourhood                       int64\n",
      "lat                               float64\n",
      "long                              float64\n",
      "instant_bookable                     bool\n",
      "cancellation_policy                 int64\n",
      "room_type                           int64\n",
      "construction_year                 float64\n",
      "price                             float64\n",
      "service_fee                       float64\n",
      "minimum_nights                    float64\n",
      "number_of_reviews                 float64\n",
      "last_review                         int64\n",
      "reviews_per_month                 float64\n",
      "review_rate_number                float64\n",
      "calculated_host_listings_count    float64\n",
      "availability_365                  float64\n",
      "house_rules                         int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# 识别非数值列\n",
    "non_numeric_cols = data_clean.select_dtypes(include=['object']).columns\n",
    "\n",
    "# 对非数值列进行标签编码\n",
    "label_encoders = {}\n",
    "for col in non_numeric_cols:\n",
    "    le = LabelEncoder()\n",
    "    # 合并干净和脏的数据\n",
    "    combined_data = pd.concat([data_clean[col], data_dirty[col]], axis=0)\n",
    "    le.fit(combined_data.astype(str))\n",
    "    # 对干净和脏的数据分别进行转换\n",
    "    data_clean[col] = le.transform(data_clean[col].astype(str))\n",
    "    data_dirty[col] = le.transform(data_dirty[col].astype(str))\n",
    "    label_encoders[col] = le\n",
    "\n",
    "# 确保所有特征都是数值类型\n",
    "print(\"Data types after encoding:\\n\", data_clean.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean = data_clean.astype(np.float64)\n",
    "data_dirty = data_dirty.astype(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "# 分离数值型特征和类别型特征\n",
    "numeric_cols = data_clean.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_cols = list(set(data_clean.columns) - set(numeric_cols))\n",
    "\n",
    "# 初始化标准化器\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# 在干净和脏数据的组合上拟合标准化器\n",
    "combined_numeric_data = pd.concat([data_clean[numeric_cols], data_dirty[numeric_cols]], axis=0)\n",
    "scaler.fit(combined_numeric_data)\n",
    "\n",
    "# 对干净和脏数据进行标准化\n",
    "data_clean[numeric_cols] = scaler.transform(data_clean[numeric_cols])\n",
    "data_dirty[numeric_cols] = scaler.transform(data_dirty[numeric_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data types of data_clean:\n",
      "id                                float64\n",
      "name                              float64\n",
      "host_id                           float64\n",
      "host_identity_verified            float64\n",
      "host_name                         float64\n",
      "neighbourhood_group               float64\n",
      "neighbourhood                     float64\n",
      "lat                               float64\n",
      "long                              float64\n",
      "instant_bookable                  float64\n",
      "cancellation_policy               float64\n",
      "room_type                         float64\n",
      "construction_year                 float64\n",
      "price                             float64\n",
      "service_fee                       float64\n",
      "minimum_nights                    float64\n",
      "number_of_reviews                 float64\n",
      "last_review                       float64\n",
      "reviews_per_month                 float64\n",
      "review_rate_number                float64\n",
      "calculated_host_listings_count    float64\n",
      "availability_365                  float64\n",
      "house_rules                       float64\n",
      "dtype: object\n",
      "Data types of data_dirty:\n",
      "id                                float64\n",
      "name                              float64\n",
      "host_id                           float64\n",
      "host_identity_verified            float64\n",
      "host_name                         float64\n",
      "neighbourhood_group               float64\n",
      "neighbourhood                     float64\n",
      "lat                               float64\n",
      "long                              float64\n",
      "instant_bookable                  float64\n",
      "cancellation_policy               float64\n",
      "room_type                         float64\n",
      "construction_year                 float64\n",
      "price                             float64\n",
      "service_fee                       float64\n",
      "minimum_nights                    float64\n",
      "number_of_reviews                 float64\n",
      "last_review                       float64\n",
      "reviews_per_month                 float64\n",
      "review_rate_number                float64\n",
      "calculated_host_listings_count    float64\n",
      "availability_365                  float64\n",
      "house_rules                       float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(\"Data types of data_clean:\")\n",
    "print(data_clean.dtypes)\n",
    "\n",
    "print(\"Data types of data_dirty:\")\n",
    "print(data_dirty.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>host_id</th>\n",
       "      <th>host_identity_verified</th>\n",
       "      <th>host_name</th>\n",
       "      <th>neighbourhood_group</th>\n",
       "      <th>neighbourhood</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>instant_bookable</th>\n",
       "      <th>...</th>\n",
       "      <th>price</th>\n",
       "      <th>service_fee</th>\n",
       "      <th>minimum_nights</th>\n",
       "      <th>number_of_reviews</th>\n",
       "      <th>last_review</th>\n",
       "      <th>reviews_per_month</th>\n",
       "      <th>review_rate_number</th>\n",
       "      <th>calculated_host_listings_count</th>\n",
       "      <th>availability_365</th>\n",
       "      <th>house_rules</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.257449</td>\n",
       "      <td>0.809928</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.562884</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.518828</td>\n",
       "      <td>0.993414</td>\n",
       "      <td>0.003737</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.805000</td>\n",
       "      <td>0.804167</td>\n",
       "      <td>0.179528</td>\n",
       "      <td>0.008789</td>\n",
       "      <td>0.669832</td>\n",
       "      <td>0.002333</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.018072</td>\n",
       "      <td>0.080282</td>\n",
       "      <td>0.225089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.781975</td>\n",
       "      <td>0.529317</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.416344</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.606695</td>\n",
       "      <td>0.996008</td>\n",
       "      <td>0.003583</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.118333</td>\n",
       "      <td>0.116667</td>\n",
       "      <td>0.179965</td>\n",
       "      <td>0.043945</td>\n",
       "      <td>0.698121</td>\n",
       "      <td>0.004222</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.006024</td>\n",
       "      <td>0.064551</td>\n",
       "      <td>0.616591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.903267</td>\n",
       "      <td>0.797912</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.253051</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.456067</td>\n",
       "      <td>0.997362</td>\n",
       "      <td>0.004147</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.516667</td>\n",
       "      <td>0.516667</td>\n",
       "      <td>0.178509</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.546777</td>\n",
       "      <td>0.008778</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.003012</td>\n",
       "      <td>0.098183</td>\n",
       "      <td>0.295397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.976943</td>\n",
       "      <td>0.861467</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.308013</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.200837</td>\n",
       "      <td>0.994334</td>\n",
       "      <td>0.003907</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.306667</td>\n",
       "      <td>0.308333</td>\n",
       "      <td>0.179965</td>\n",
       "      <td>0.263672</td>\n",
       "      <td>0.551020</td>\n",
       "      <td>0.051556</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.003012</td>\n",
       "      <td>0.090046</td>\n",
       "      <td>0.975721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.387128</td>\n",
       "      <td>0.931817</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.558108</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.297071</td>\n",
       "      <td>0.997105</td>\n",
       "      <td>0.004119</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.170000</td>\n",
       "      <td>0.170833</td>\n",
       "      <td>0.179528</td>\n",
       "      <td>0.008789</td>\n",
       "      <td>0.504950</td>\n",
       "      <td>0.001111</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.003012</td>\n",
       "      <td>0.081096</td>\n",
       "      <td>0.692463</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id      name   host_id  host_identity_verified  host_name  \\\n",
       "0  0.000000  0.257449  0.809928                     0.5   0.562884   \n",
       "1  0.000015  0.781975  0.529317                     1.0   0.416344   \n",
       "2  0.000020  0.903267  0.797912                     0.5   0.253051   \n",
       "3  0.000027  0.976943  0.861467                     0.5   0.308013   \n",
       "4  0.000043  0.387128  0.931817                     1.0   0.558108   \n",
       "\n",
       "   neighbourhood_group  neighbourhood       lat      long  instant_bookable  \\\n",
       "0             0.285714       0.518828  0.993414  0.003737               0.0   \n",
       "1             0.428571       0.606695  0.996008  0.003583               0.0   \n",
       "2             0.428571       0.456067  0.997362  0.004147               1.0   \n",
       "3             0.285714       0.200837  0.994334  0.003907               1.0   \n",
       "4             0.428571       0.297071  0.997105  0.004119               0.0   \n",
       "\n",
       "   ...     price  service_fee  minimum_nights  number_of_reviews  last_review  \\\n",
       "0  ...  0.805000     0.804167        0.179528           0.008789     0.669832   \n",
       "1  ...  0.118333     0.116667        0.179965           0.043945     0.698121   \n",
       "2  ...  0.516667     0.516667        0.178509           0.000000     0.546777   \n",
       "3  ...  0.306667     0.308333        0.179965           0.263672     0.551020   \n",
       "4  ...  0.170000     0.170833        0.179528           0.008789     0.504950   \n",
       "\n",
       "   reviews_per_month  review_rate_number  calculated_host_listings_count  \\\n",
       "0           0.002333                 0.8                        0.018072   \n",
       "1           0.004222                 0.8                        0.006024   \n",
       "2           0.008778                 1.0                        0.003012   \n",
       "3           0.051556                 0.8                        0.003012   \n",
       "4           0.001111                 0.6                        0.003012   \n",
       "\n",
       "   availability_365  house_rules  \n",
       "0          0.080282     0.225089  \n",
       "1          0.064551     0.616591  \n",
       "2          0.098183     0.295397  \n",
       "3          0.090046     0.975721  \n",
       "4          0.081096     0.692463  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 特征图构建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from torch_geometric.utils import from_networkx\n",
    "from torch_geometric.data import Data\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 定义特征列和关系\n",
    "columns = [\n",
    "    'id', 'name', 'host_id', 'host_identity_verified', 'host_name',\n",
    "    'neighbourhood_group', 'neighbourhood', 'lat', 'long',\n",
    "    'instant_bookable', 'cancellation_policy', 'room_type',\n",
    "    'construction_year', 'price', 'service_fee', 'minimum_nights',\n",
    "    'number_of_reviews', 'last_review', 'reviews_per_month',\n",
    "    'review_rate_number', 'calculated_host_listings_count', 'availability_365',\n",
    "    'house_rules'\n",
    "]\n",
    "relations = [\n",
    "    ('id', 'host_id'),\n",
    "    ('host_id', 'host_identity_verified'),\n",
    "    ('host_id', 'host_name'),\n",
    "    ('neighbourhood_group', 'neighbourhood'),\n",
    "    ('lat', 'long'),\n",
    "    ('instant_bookable', 'cancellation_policy'),\n",
    "    ('room_type', 'price'),\n",
    "    ('price', 'service_fee'),\n",
    "    ('minimum_nights', 'number_of_reviews'),\n",
    "    ('number_of_reviews', 'reviews_per_month'),\n",
    "    ('reviews_per_month', 'review_rate_number'),\n",
    "    ('review_rate_number', 'calculated_host_listings_count'),\n",
    "    ('calculated_host_listings_count', 'availability_365'),\n",
    "    ('availability_365', 'house_rules')\n",
    "]\n",
    "\n",
    "# 创建特征名称到索引的映射\n",
    "feature_to_index = {col: idx for idx, col in enumerate(columns)}\n",
    "index_to_feature = {idx: col for idx, col in enumerate(columns)}\n",
    "\n",
    "# 创建空的无向图\n",
    "G = nx.Graph()\n",
    "\n",
    "# 添加节点（使用索引作为节点）\n",
    "for idx in range(len(columns)):\n",
    "    G.add_node(idx)\n",
    "\n",
    "# 添加边（将特征名称映射到索引）\n",
    "for src, dst in relations:\n",
    "    src_idx = feature_to_index[src]\n",
    "    dst_idx = feature_to_index[dst]\n",
    "    G.add_edge(src_idx, dst_idx)\n",
    "\n",
    "# 将 NetworkX 图转换为 PyTorch Geometric 图\n",
    "data = Data()\n",
    "data.edge_index = torch.tensor(list(G.edges())).t().contiguous()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 新的GNN模型 编码器设计"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GATConv, GINConv\n",
    "\n",
    "class GAT_GIN_Encoder(nn.Module):\n",
    "    def __init__(self, num_features, hidden_channels):\n",
    "        super(GAT_GIN_Encoder, self).__init__()\n",
    "        # 第一层：GATConv\n",
    "        self.gat_conv1 = GATConv(num_features, hidden_channels, heads=8, concat=False)\n",
    "        # 第二层：GINConv\n",
    "        nn1 = nn.Sequential(\n",
    "            nn.Linear(hidden_channels, hidden_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_channels, hidden_channels)\n",
    "        )\n",
    "        self.gin_conv1 = GINConv(nn1)\n",
    "        # 第三层：GATConv\n",
    "        self.gat_conv2 = GATConv(hidden_channels, hidden_channels, heads=8, concat=False)\n",
    "        # 第四层：GINConv\n",
    "        nn2 = nn.Sequential(\n",
    "            nn.Linear(hidden_channels, hidden_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_channels, hidden_channels)\n",
    "        )\n",
    "        self.gin_conv2 = GINConv(nn2)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.relu(self.gat_conv1(x, edge_index))\n",
    "        x = F.relu(self.gin_conv1(x, edge_index))\n",
    "        x = F.relu(self.gat_conv2(x, edge_index))\n",
    "        x = F.relu(self.gin_conv2(x, edge_index))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 解码器设计"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTaskDecoder(nn.Module):\n",
    "    def __init__(self, hidden_channels, num_features):\n",
    "        super(MultiTaskDecoder, self).__init__()\n",
    "        # 数据质量验证解码器\n",
    "        self.decoder_validation = nn.Sequential(\n",
    "            nn.Linear(hidden_channels, hidden_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_channels, num_features)\n",
    "        )\n",
    "        # 数据修复解码器\n",
    "        self.decoder_repair = nn.Sequential(\n",
    "            nn.Linear(hidden_channels, hidden_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_channels, num_features)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 数据质量验证输出\n",
    "        out_validation = self.decoder_validation(x)\n",
    "        # 数据修复输出\n",
    "        out_repair = self.decoder_repair(x)\n",
    "        return out_validation, out_repair\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 整合模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTaskGNN(nn.Module):\n",
    "    def __init__(self, num_features, hidden_channels):\n",
    "        super(MultiTaskGNN, self).__init__()\n",
    "        self.encoder = GAT_GIN_Encoder(num_features, hidden_channels)\n",
    "        self.decoder = MultiTaskDecoder(hidden_channels, num_features)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x = data.x  # x 的形状应为 [num_nodes, num_node_features]\n",
    "        edge_index = data.edge_index\n",
    "        x = self.encoder(x, edge_index)\n",
    "        out_validation, out_repair = self.decoder(x)\n",
    "        return out_validation, out_repair\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 准备数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 准备图数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建节点特征矩阵\n",
    "num_nodes = len(columns)\n",
    "num_features = num_nodes  # 每个节点的特征维度\n",
    "\n",
    "def create_node_features(instance):\n",
    "    # 确保实例中的特征按照 columns 列表的顺序排列\n",
    "    values = instance[columns].values.astype(np.float32)\n",
    "    # 将数据转换为张量，形状为 [num_nodes, num_node_features]\n",
    "    node_features = torch.tensor(values, dtype=torch.float).view(-1, 1)\n",
    "    return node_features\n",
    "\n",
    "\n",
    "# 创建 PyTorch Geometric 数据对象\n",
    "def create_data_object(instance):\n",
    "    node_features = create_node_features(instance)\n",
    "    data_instance = Data()\n",
    "    data_instance.x = node_features\n",
    "    data_instance.edge_index = data.edge_index\n",
    "    data_instance.num_nodes = data.num_nodes\n",
    "    return data_instance\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据集划分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 假设 data_clean 和 data_dirty 已经经过预处理，并且所有列都是数值类型\n",
    "\n",
    "# 将 data_clean 随机打乱，并划分为训练集和临时集（50% / 50%）\n",
    "train_data, temp_data = train_test_split(data_clean, test_size=0.5, random_state=42)\n",
    "\n",
    "# 将临时集再划分为验证集和测试集1（各占25%）\n",
    "val_data, test_data_1 = train_test_split(temp_data, test_size=0.5, random_state=42)\n",
    "\n",
    "# data_dirty 作为测试集2\n",
    "test_data_2 = data_dirty  # 已经预处理好的脏数据\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 创建数据加载器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "\n",
    "class GraphDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.dataframe = dataframe\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        instance = self.dataframe.iloc[idx]\n",
    "        data_instance = create_data_object(instance)\n",
    "        return data_instance\n",
    "\n",
    "# 创建数据集对象\n",
    "train_dataset = GraphDataset(train_data)\n",
    "val_dataset = GraphDataset(val_data)\n",
    "test_dataset_1 = GraphDataset(test_data_1)\n",
    "test_dataset_2 = GraphDataset(test_data_2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def loss_function(out_validation, out_repair, target, lambda_validation=1.0, lambda_repair=1.0):\n",
    "#     # 数据质量验证损失\n",
    "#     loss_validation = F.mse_loss(out_validation, target)\n",
    "#     # 数据修复损失\n",
    "#     loss_repair = F.mse_loss(out_repair, target)\n",
    "#     # 总损失\n",
    "#     loss = lambda_validation * loss_validation + lambda_repair * loss_repair\n",
    "#     return loss\n",
    "# 定义损失函数，包含验证损失和修复损失\n",
    "def loss_function(out_validation, out_repair, target, weights, lambda_validation=1.0, lambda_repair=1.0):\n",
    "    # 数据质量验证损失，使用加权 MSE Loss\n",
    "    loss_validation = (weights * F.mse_loss(out_validation, target, reduction='none').sum(dim=1)).mean()\n",
    "    \n",
    "    # 数据修复损失，普通的 MSE Loss\n",
    "    loss_repair = F.mse_loss(out_repair, target)\n",
    "    \n",
    "    # 总损失\n",
    "    loss = lambda_validation * loss_validation + lambda_repair * loss_repair\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 训练循环"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "恢复训练，从 epoch 11 开始\n",
      "Epoch 15, Loss: 0.0015\n",
      "Model saved at epoch 15\n",
      "Epoch 20, Loss: 0.0014\n",
      "Model saved at epoch 20\n",
      "Epoch 25, Loss: 0.0009\n",
      "Model saved at epoch 25\n",
      "Epoch 30, Loss: 0.0015\n",
      "Model saved at epoch 30\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "import warnings\n",
    "\n",
    "# 抑制所有警告\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# 创建数据加载器\n",
    "batch_size = 128\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader_1 = DataLoader(test_dataset_1, batch_size=batch_size, shuffle=False)\n",
    "test_loader_2 = DataLoader(test_dataset_2, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# 初始化模型、优化器\n",
    "num_features = 1  # 因为每个节点只有一个特征\n",
    "hidden_channels = 64\n",
    "model = MultiTaskGNN(num_features, hidden_channels)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# 定义保存模型的路径\n",
    "save_path = '/home/sdong/experiments/GVAE/model/multitask_gnn_model.pth'\n",
    "# 定义checkpoint文件的路径\n",
    "checkpoint_path = '/home/sdong/experiments/GVAE/model/checkpoint.pth'\n",
    "\n",
    "# 加载已保存的checkpoint\n",
    "if os.path.isfile(checkpoint_path):\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    start_epoch = checkpoint['epoch'] + 1  # 从中断的下一轮开始\n",
    "    loss = checkpoint['loss']\n",
    "\n",
    "    print(f\"恢复训练，从 epoch {start_epoch} 开始\")\n",
    "else:\n",
    "    start_epoch = 1\n",
    "    print(\"没有找到已保存的模型，从头开始训练\")\n",
    "\n",
    "# 训练模型\n",
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    alpha = 0.5  # 控制权重衰减的超参数\n",
    "    for data in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        out_validation, out_repair = model(data)\n",
    "        target = data.x\n",
    "        \n",
    "        # 计算每个样本的重构误差\n",
    "        reconstruction_error = F.mse_loss(out_validation, target, reduction='none').sum(dim=1)\n",
    "        \n",
    "        # 计算权重：误差越小权重越大，误差越大权重越小\n",
    "        weights = torch.exp(-alpha * reconstruction_error)\n",
    "        \n",
    "        # 计算损失\n",
    "        loss = loss_function(out_validation, out_repair, target, weights)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "# 训练循环\n",
    "num_epochs = 30\n",
    "for epoch in range(start_epoch, num_epochs + 1):\n",
    "    loss = train()\n",
    "    if epoch % 5 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {loss:.4f}')\n",
    "        \n",
    "        with open('train_log.txt', 'a') as f:\n",
    "            if epoch % 5 == 0:\n",
    "                f.write(f'Epoch {epoch}, Loss: {loss:.4f}\\n')\n",
    "                \n",
    "        # 保存新的模型状态\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': loss,\n",
    "        }, checkpoint_path)\n",
    "        print(f'Model saved at epoch {epoch}')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 测试模型\n",
    "### 定义评估函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, data_loader):\n",
    "    model.eval()\n",
    "    reconstruction_errors = []\n",
    "    with torch.no_grad():\n",
    "        for data in data_loader:\n",
    "            out_validation, _ = model(data)\n",
    "            target = data.x\n",
    "            # 计算重构误差\n",
    "            loss = F.mse_loss(out_validation, target, reduction='none')\n",
    "            # 对每个样本计算平均误差\n",
    "            loss_per_sample = loss.mean(dim=1)\n",
    "            reconstruction_errors.extend(loss_per_sample.tolist())\n",
    "    # 返回所有样本的重构误差列表\n",
    "    return reconstruction_errors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 计算阈值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss threshold for detecting data quality issues: 0.0024004371254704856\n",
      "Min validation error: 5.640019701269594e-16\n",
      "Max validation error: 0.9992021918296814\n",
      "Mean validation error: 0.0005992085086316146\n",
      "95th percentile of validation errors: 0.0024004371254704856\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 收集验证集的重构误差\n",
    "val_errors = evaluate_model(model, val_loader)\n",
    "\n",
    "# 计算95%分位数作为阈值\n",
    "threshold = np.quantile(val_errors, 0.95)\n",
    "print(f\"Loss threshold for detecting data quality issues: {threshold}\")\n",
    "\n",
    "# 打印一些统计信息\n",
    "print(f\"Min validation error: {min(val_errors)}\")\n",
    "print(f\"Max validation error: {max(val_errors)}\")\n",
    "print(f\"Mean validation error: {np.mean(val_errors)}\")\n",
    "print(f\"95th percentile of validation errors: {threshold}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 检测数据质量问题\n",
    "### 定义检测函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_quality_issues(model, data_loader, threshold):\n",
    "    model.eval()\n",
    "    total_issues = 0\n",
    "    total_samples = 0\n",
    "    with torch.no_grad():\n",
    "        for data in data_loader:\n",
    "            out_validation, _ = model(data)\n",
    "            target = data.x\n",
    "            # 计算重构误差\n",
    "            loss = F.mse_loss(out_validation, target, reduction='none')\n",
    "            loss_per_sample = loss.mean(dim=1)\n",
    "            # 检测超过阈值的样本\n",
    "            issues = (loss_per_sample > threshold).sum().item()\n",
    "            total_issues += issues\n",
    "            total_samples += loss_per_sample.size(0)\n",
    "    # 计算有问题的样本比例\n",
    "    issue_ratio = total_issues / total_samples\n",
    "    return total_issues, total_samples, issue_ratio\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 在测试集1和测试集2上检测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set 1 (Clean Data): 19671/398521 samples are faulty (4.94%)\n",
      "Test Set 2 (Dirty Data): 214910/2359777 samples are faulty (9.11%)\n"
     ]
    }
   ],
   "source": [
    "# 检测测试集1（干净数据）的质量问题\n",
    "issues_test1, samples_test1, ratio_test1 = detect_quality_issues(model, test_loader_1, threshold)\n",
    "print(f\"Test Set 1 (Clean Data): {issues_test1}/{samples_test1} samples are faulty ({ratio_test1 * 100:.2f}%)\")\n",
    "\n",
    "# 检测测试集2（脏数据）的质量问题\n",
    "issues_test2, samples_test2, ratio_test2 = detect_quality_issues(model, test_loader_2, threshold)\n",
    "print(f\"Test Set 2 (Dirty Data): {issues_test2}/{samples_test2} samples are faulty ({ratio_test2 * 100:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 额外的测试：随机采样测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_random_samples(model, data, threshold, num_tests=50):\n",
    "    from sklearn.utils import shuffle\n",
    "    problematic_batches = 0\n",
    "    total_tests = num_tests\n",
    "    for seed in range(num_tests):\n",
    "        # 随机采样20%的数据\n",
    "        sample_data = data.sample(frac=0.2, random_state=seed).reset_index(drop=True)\n",
    "        sample_dataset = GraphDataset(sample_data)\n",
    "        sample_loader = DataLoader(sample_dataset, batch_size=batch_size, shuffle=False)\n",
    "        issues, samples, ratio = detect_quality_issues(model, sample_loader, threshold)\n",
    "        if ratio > 0.06:  # 超过6%的样本有问题\n",
    "            print(f\"Random sample {seed} is problematic: {issues} out of {samples} samples are faulty ({ratio * 100:.2f}%).\")\n",
    "            problematic_batches += 1\n",
    "        else:\n",
    "            print(f\"Random sample {seed} is ok: {issues} out of {samples} samples are faulty ({ratio * 100:.2f}%).\")\n",
    "    print(f\"Total problematic batches across all tests: {problematic_batches}/{total_tests}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing on Test Set 1 (Clean Data):\n",
      "Random sample 0 is ok: 3836 out of 79695 samples are faulty (4.81%).\n",
      "Random sample 1 is ok: 3930 out of 79695 samples are faulty (4.93%).\n",
      "Random sample 2 is ok: 3925 out of 79695 samples are faulty (4.93%).\n",
      "Random sample 3 is ok: 3902 out of 79695 samples are faulty (4.90%).\n",
      "Random sample 4 is ok: 3988 out of 79695 samples are faulty (5.00%).\n",
      "Random sample 5 is ok: 3929 out of 79695 samples are faulty (4.93%).\n",
      "Random sample 6 is ok: 3963 out of 79695 samples are faulty (4.97%).\n",
      "Random sample 7 is ok: 3899 out of 79695 samples are faulty (4.89%).\n",
      "Random sample 8 is ok: 3915 out of 79695 samples are faulty (4.91%).\n",
      "Random sample 9 is ok: 3925 out of 79695 samples are faulty (4.93%).\n",
      "Random sample 10 is ok: 3837 out of 79695 samples are faulty (4.81%).\n",
      "Random sample 11 is ok: 3887 out of 79695 samples are faulty (4.88%).\n",
      "Random sample 12 is ok: 3910 out of 79695 samples are faulty (4.91%).\n",
      "Random sample 13 is ok: 3980 out of 79695 samples are faulty (4.99%).\n",
      "Random sample 14 is ok: 4008 out of 79695 samples are faulty (5.03%).\n",
      "Random sample 15 is ok: 3914 out of 79695 samples are faulty (4.91%).\n",
      "Random sample 16 is ok: 3919 out of 79695 samples are faulty (4.92%).\n",
      "Random sample 17 is ok: 3972 out of 79695 samples are faulty (4.98%).\n",
      "Random sample 18 is ok: 3929 out of 79695 samples are faulty (4.93%).\n",
      "Random sample 19 is ok: 3893 out of 79695 samples are faulty (4.88%).\n",
      "Random sample 20 is ok: 3961 out of 79695 samples are faulty (4.97%).\n",
      "Random sample 21 is ok: 3939 out of 79695 samples are faulty (4.94%).\n",
      "Random sample 22 is ok: 3909 out of 79695 samples are faulty (4.90%).\n",
      "Random sample 23 is ok: 3952 out of 79695 samples are faulty (4.96%).\n",
      "Random sample 24 is ok: 3928 out of 79695 samples are faulty (4.93%).\n",
      "Random sample 25 is ok: 3979 out of 79695 samples are faulty (4.99%).\n",
      "Random sample 26 is ok: 3948 out of 79695 samples are faulty (4.95%).\n",
      "Random sample 27 is ok: 3914 out of 79695 samples are faulty (4.91%).\n",
      "Random sample 28 is ok: 3910 out of 79695 samples are faulty (4.91%).\n",
      "Random sample 29 is ok: 4007 out of 79695 samples are faulty (5.03%).\n",
      "Random sample 30 is ok: 3997 out of 79695 samples are faulty (5.02%).\n",
      "Random sample 31 is ok: 3950 out of 79695 samples are faulty (4.96%).\n",
      "Random sample 32 is ok: 3871 out of 79695 samples are faulty (4.86%).\n",
      "Random sample 33 is ok: 3920 out of 79695 samples are faulty (4.92%).\n",
      "Random sample 34 is ok: 3945 out of 79695 samples are faulty (4.95%).\n",
      "Random sample 35 is ok: 3928 out of 79695 samples are faulty (4.93%).\n",
      "Random sample 36 is ok: 3966 out of 79695 samples are faulty (4.98%).\n",
      "Random sample 37 is ok: 3977 out of 79695 samples are faulty (4.99%).\n",
      "Random sample 38 is ok: 3895 out of 79695 samples are faulty (4.89%).\n",
      "Random sample 39 is ok: 3949 out of 79695 samples are faulty (4.96%).\n",
      "Random sample 40 is ok: 3874 out of 79695 samples are faulty (4.86%).\n",
      "Random sample 41 is ok: 3982 out of 79695 samples are faulty (5.00%).\n",
      "Random sample 42 is ok: 3901 out of 79695 samples are faulty (4.89%).\n",
      "Random sample 43 is ok: 4014 out of 79695 samples are faulty (5.04%).\n",
      "Random sample 44 is ok: 3918 out of 79695 samples are faulty (4.92%).\n",
      "Random sample 45 is ok: 3993 out of 79695 samples are faulty (5.01%).\n",
      "Random sample 46 is ok: 3936 out of 79695 samples are faulty (4.94%).\n",
      "Random sample 47 is ok: 3901 out of 79695 samples are faulty (4.89%).\n",
      "Random sample 48 is ok: 3951 out of 79695 samples are faulty (4.96%).\n",
      "Random sample 49 is ok: 3884 out of 79695 samples are faulty (4.87%).\n",
      "Total problematic batches across all tests: 0/50\n",
      "Testing on Test Set 2 (Dirty Data):\n",
      "Random sample 0 is problematic: 43085 out of 471960 samples are faulty (9.13%).\n",
      "Random sample 1 is problematic: 43010 out of 471960 samples are faulty (9.11%).\n",
      "Random sample 2 is problematic: 42988 out of 471960 samples are faulty (9.11%).\n",
      "Random sample 3 is problematic: 43098 out of 471960 samples are faulty (9.13%).\n",
      "Random sample 4 is problematic: 42698 out of 471960 samples are faulty (9.05%).\n",
      "Random sample 5 is problematic: 42747 out of 471960 samples are faulty (9.06%).\n",
      "Random sample 6 is problematic: 43188 out of 471960 samples are faulty (9.15%).\n",
      "Random sample 7 is problematic: 42851 out of 471960 samples are faulty (9.08%).\n",
      "Random sample 8 is problematic: 43019 out of 471960 samples are faulty (9.11%).\n",
      "Random sample 9 is problematic: 43142 out of 471960 samples are faulty (9.14%).\n",
      "Random sample 10 is problematic: 43091 out of 471960 samples are faulty (9.13%).\n",
      "Random sample 11 is problematic: 42734 out of 471960 samples are faulty (9.05%).\n",
      "Random sample 12 is problematic: 42910 out of 471960 samples are faulty (9.09%).\n",
      "Random sample 13 is problematic: 43052 out of 471960 samples are faulty (9.12%).\n",
      "Random sample 14 is problematic: 42787 out of 471960 samples are faulty (9.07%).\n",
      "Random sample 15 is problematic: 43140 out of 471960 samples are faulty (9.14%).\n",
      "Random sample 16 is problematic: 42882 out of 471960 samples are faulty (9.09%).\n",
      "Random sample 17 is problematic: 42945 out of 471960 samples are faulty (9.10%).\n",
      "Random sample 18 is problematic: 42755 out of 471960 samples are faulty (9.06%).\n",
      "Random sample 19 is problematic: 42822 out of 471960 samples are faulty (9.07%).\n",
      "Random sample 20 is problematic: 43266 out of 471960 samples are faulty (9.17%).\n",
      "Random sample 21 is problematic: 43134 out of 471960 samples are faulty (9.14%).\n",
      "Random sample 22 is problematic: 43134 out of 471960 samples are faulty (9.14%).\n",
      "Random sample 23 is problematic: 43050 out of 471960 samples are faulty (9.12%).\n",
      "Random sample 24 is problematic: 42849 out of 471960 samples are faulty (9.08%).\n",
      "Random sample 25 is problematic: 43195 out of 471960 samples are faulty (9.15%).\n",
      "Random sample 26 is problematic: 42944 out of 471960 samples are faulty (9.10%).\n",
      "Random sample 27 is problematic: 43101 out of 471960 samples are faulty (9.13%).\n",
      "Random sample 28 is problematic: 43096 out of 471960 samples are faulty (9.13%).\n",
      "Random sample 29 is problematic: 43157 out of 471960 samples are faulty (9.14%).\n",
      "Random sample 30 is problematic: 43128 out of 471960 samples are faulty (9.14%).\n",
      "Random sample 31 is problematic: 42764 out of 471960 samples are faulty (9.06%).\n",
      "Random sample 32 is problematic: 42858 out of 471960 samples are faulty (9.08%).\n",
      "Random sample 33 is problematic: 42834 out of 471960 samples are faulty (9.08%).\n",
      "Random sample 34 is problematic: 43049 out of 471960 samples are faulty (9.12%).\n",
      "Random sample 35 is problematic: 42743 out of 471960 samples are faulty (9.06%).\n",
      "Random sample 36 is problematic: 42887 out of 471960 samples are faulty (9.09%).\n",
      "Random sample 37 is problematic: 42846 out of 471960 samples are faulty (9.08%).\n",
      "Random sample 38 is problematic: 43153 out of 471960 samples are faulty (9.14%).\n",
      "Random sample 39 is problematic: 43175 out of 471960 samples are faulty (9.15%).\n",
      "Random sample 40 is problematic: 42845 out of 471960 samples are faulty (9.08%).\n",
      "Random sample 41 is problematic: 42992 out of 471960 samples are faulty (9.11%).\n",
      "Random sample 42 is problematic: 42878 out of 471960 samples are faulty (9.09%).\n",
      "Random sample 43 is problematic: 42834 out of 471960 samples are faulty (9.08%).\n",
      "Random sample 44 is problematic: 42964 out of 471960 samples are faulty (9.10%).\n",
      "Random sample 45 is problematic: 42822 out of 471960 samples are faulty (9.07%).\n",
      "Random sample 46 is problematic: 42957 out of 471960 samples are faulty (9.10%).\n",
      "Random sample 47 is problematic: 42771 out of 471960 samples are faulty (9.06%).\n",
      "Random sample 48 is problematic: 42953 out of 471960 samples are faulty (9.10%).\n",
      "Random sample 49 is problematic: 42928 out of 471960 samples are faulty (9.10%).\n",
      "Total problematic batches across all tests: 50/50\n"
     ]
    }
   ],
   "source": [
    "# 对测试集1（干净数据）执行随机采样测试\n",
    "print(\"Testing on Test Set 1 (Clean Data):\")\n",
    "test_random_samples(model, test_data_1, threshold)\n",
    "\n",
    "# 对测试集2（脏数据）执行随机采样测试\n",
    "print(\"Testing on Test Set 2 (Dirty Data):\")\n",
    "test_random_samples(model, test_data_2, threshold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing on Test Set 1 (Clean Data):\n",
      "Random sample 0 is ok: 3836 out of 79695 samples are faulty (4.81%).\n",
      "Random sample 1 is ok: 3930 out of 79695 samples are faulty (4.93%).\n",
      "Random sample 2 is ok: 3925 out of 79695 samples are faulty (4.93%).\n",
      "Random sample 3 is ok: 3902 out of 79695 samples are faulty (4.90%).\n",
      "Random sample 4 is ok: 3988 out of 79695 samples are faulty (5.00%).\n",
      "Random sample 5 is ok: 3929 out of 79695 samples are faulty (4.93%).\n",
      "Random sample 6 is ok: 3963 out of 79695 samples are faulty (4.97%).\n",
      "Random sample 7 is ok: 3899 out of 79695 samples are faulty (4.89%).\n",
      "Random sample 8 is ok: 3915 out of 79695 samples are faulty (4.91%).\n",
      "Random sample 9 is ok: 3925 out of 79695 samples are faulty (4.93%).\n",
      "Random sample 10 is ok: 3837 out of 79695 samples are faulty (4.81%).\n",
      "Random sample 11 is ok: 3887 out of 79695 samples are faulty (4.88%).\n",
      "Random sample 12 is ok: 3910 out of 79695 samples are faulty (4.91%).\n",
      "Random sample 13 is ok: 3980 out of 79695 samples are faulty (4.99%).\n",
      "Random sample 14 is ok: 4008 out of 79695 samples are faulty (5.03%).\n",
      "Random sample 15 is ok: 3914 out of 79695 samples are faulty (4.91%).\n",
      "Random sample 16 is ok: 3919 out of 79695 samples are faulty (4.92%).\n",
      "Random sample 17 is ok: 3972 out of 79695 samples are faulty (4.98%).\n",
      "Random sample 18 is ok: 3929 out of 79695 samples are faulty (4.93%).\n",
      "Random sample 19 is ok: 3893 out of 79695 samples are faulty (4.88%).\n",
      "Random sample 20 is ok: 3961 out of 79695 samples are faulty (4.97%).\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 29\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# 对测试集1（干净数据）执行随机采样测试\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTesting on Test Set 1 (Clean Data):\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 29\u001b[0m correct_batches_1, total_tests_1 \u001b[38;5;241m=\u001b[39m \u001b[43mtest_random_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_data_1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# 对测试集2（脏数据）执行随机采样测试\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTesting on Test Set 2 (Dirty Data):\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[23], line 11\u001b[0m, in \u001b[0;36mtest_random_samples\u001b[0;34m(model, data, threshold, num_tests)\u001b[0m\n\u001b[1;32m      9\u001b[0m sample_dataset \u001b[38;5;241m=\u001b[39m GraphDataset(sample_data)\n\u001b[1;32m     10\u001b[0m sample_loader \u001b[38;5;241m=\u001b[39m DataLoader(sample_dataset, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m---> 11\u001b[0m issues, samples, ratio \u001b[38;5;241m=\u001b[39m \u001b[43mdetect_quality_issues\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ratio \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.06\u001b[39m:  \u001b[38;5;66;03m# 超过6%的样本有问题\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRandom sample \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mseed\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is problematic: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00missues\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m out of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msamples\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m samples are faulty (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mratio\u001b[38;5;250m \u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m100\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%).\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[19], line 6\u001b[0m, in \u001b[0;36mdetect_quality_issues\u001b[0;34m(model, data_loader, threshold)\u001b[0m\n\u001b[1;32m      4\u001b[0m total_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m----> 6\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43mout_validation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mainenv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/envs/mainenv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/miniconda3/envs/mainenv/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/miniconda3/envs/mainenv/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[14], line 13\u001b[0m, in \u001b[0;36mGraphDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[1;32m     12\u001b[0m     instance \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataframe\u001b[38;5;241m.\u001b[39miloc[idx]\n\u001b[0;32m---> 13\u001b[0m     data_instance \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_data_object\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstance\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m data_instance\n",
      "Cell \u001b[0;32mIn[12], line 15\u001b[0m, in \u001b[0;36mcreate_data_object\u001b[0;34m(instance)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_data_object\u001b[39m(instance):\n\u001b[0;32m---> 15\u001b[0m     node_features \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_node_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstance\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m     data_instance \u001b[38;5;241m=\u001b[39m Data()\n\u001b[1;32m     17\u001b[0m     data_instance\u001b[38;5;241m.\u001b[39mx \u001b[38;5;241m=\u001b[39m node_features\n",
      "Cell \u001b[0;32mIn[12], line 7\u001b[0m, in \u001b[0;36mcreate_node_features\u001b[0;34m(instance)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_node_features\u001b[39m(instance):\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# 确保实例中的特征按照 columns 列表的顺序排列\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m     values \u001b[38;5;241m=\u001b[39m \u001b[43minstance\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# 将数据转换为张量，形状为 [num_nodes, num_node_features]\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     node_features \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(values, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat)\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/mainenv/lib/python3.11/site-packages/pandas/core/series.py:1153\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     key \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(key, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mbool\u001b[39m)\n\u001b[1;32m   1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_rows_with_mask(key)\n\u001b[0;32m-> 1153\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_with\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mainenv/lib/python3.11/site-packages/pandas/core/series.py:1194\u001b[0m, in \u001b[0;36mSeries._get_with\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1191\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miloc[key]\n\u001b[1;32m   1193\u001b[0m \u001b[38;5;66;03m# handle the dup indexing case GH#4246\u001b[39;00m\n\u001b[0;32m-> 1194\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mainenv/lib/python3.11/site-packages/pandas/core/indexing.py:1191\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1189\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39mapply_if_callable(key, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj)\n\u001b[1;32m   1190\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_deprecated_callable_usage(key, maybe_callable)\n\u001b[0;32m-> 1191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaybe_callable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mainenv/lib/python3.11/site-packages/pandas/core/indexing.py:1420\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1417\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(key, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mndim\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   1418\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot index with multidimensional key\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_iterable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1422\u001b[0m \u001b[38;5;66;03m# nested tuple slicing\u001b[39;00m\n\u001b[1;32m   1423\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_nested_tuple(key, labels):\n",
      "File \u001b[0;32m~/miniconda3/envs/mainenv/lib/python3.11/site-packages/pandas/core/indexing.py:1360\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_iterable\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1357\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_key(key, axis)\n\u001b[1;32m   1359\u001b[0m \u001b[38;5;66;03m# A collection of keys\u001b[39;00m\n\u001b[0;32m-> 1360\u001b[0m keyarr, indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_listlike_indexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1361\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_reindex_with_indexers(\n\u001b[1;32m   1362\u001b[0m     {axis: [keyarr, indexer]}, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, allow_dups\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1363\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/mainenv/lib/python3.11/site-packages/pandas/core/indexing.py:1558\u001b[0m, in \u001b[0;36m_LocIndexer._get_listlike_indexer\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1555\u001b[0m ax \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_get_axis(axis)\n\u001b[1;32m   1556\u001b[0m axis_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_get_axis_name(axis)\n\u001b[0;32m-> 1558\u001b[0m keyarr, indexer \u001b[38;5;241m=\u001b[39m \u001b[43max\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1560\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m keyarr, indexer\n",
      "File \u001b[0;32m~/miniconda3/envs/mainenv/lib/python3.11/site-packages/pandas/core/indexes/base.py:6195\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6192\u001b[0m     keyarr \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39masarray_tuplesafe(keyarr)\n\u001b[1;32m   6194\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_as_unique:\n\u001b[0;32m-> 6195\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_indexer_for\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6196\u001b[0m     keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreindex(keyarr)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   6197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/mainenv/lib/python3.11/site-packages/pandas/core/indexes/base.py:6182\u001b[0m, in \u001b[0;36mIndex.get_indexer_for\u001b[0;34m(self, target)\u001b[0m\n\u001b[1;32m   6164\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   6165\u001b[0m \u001b[38;5;124;03mGuaranteed return of an indexer even when non-unique.\u001b[39;00m\n\u001b[1;32m   6166\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   6179\u001b[0m \u001b[38;5;124;03marray([0, 2])\u001b[39;00m\n\u001b[1;32m   6180\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   6181\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_as_unique:\n\u001b[0;32m-> 6182\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_indexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6183\u001b[0m indexer, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_indexer_non_unique(target)\n\u001b[1;32m   6184\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m indexer\n",
      "File \u001b[0;32m~/miniconda3/envs/mainenv/lib/python3.11/site-packages/pandas/core/indexes/base.py:3932\u001b[0m, in \u001b[0;36mIndex.get_indexer\u001b[0;34m(self, target, method, limit, tolerance)\u001b[0m\n\u001b[1;32m   3928\u001b[0m         indexer[mask] \u001b[38;5;241m=\u001b[39m loc\n\u001b[1;32m   3930\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ensure_platform_int(indexer)\n\u001b[0;32m-> 3932\u001b[0m pself, ptarget \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_downcast_for_indexing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3933\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pself \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m ptarget \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m target:\n\u001b[1;32m   3934\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pself\u001b[38;5;241m.\u001b[39mget_indexer(\n\u001b[1;32m   3935\u001b[0m         ptarget, method\u001b[38;5;241m=\u001b[39mmethod, limit\u001b[38;5;241m=\u001b[39mlimit, tolerance\u001b[38;5;241m=\u001b[39mtolerance\n\u001b[1;32m   3936\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/mainenv/lib/python3.11/site-packages/pandas/core/indexes/base.py:6364\u001b[0m, in \u001b[0;36mIndex._maybe_downcast_for_indexing\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m   6360\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[1;32m   6361\u001b[0m         \u001b[38;5;66;03m# let's instead try with a straight Index\u001b[39;00m\n\u001b[1;32m   6362\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m Index(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values)\n\u001b[0;32m-> 6364\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mis_object_dtype\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mand\u001b[39;00m is_object_dtype(other\u001b[38;5;241m.\u001b[39mdtype):\n\u001b[1;32m   6365\u001b[0m     \u001b[38;5;66;03m# Reverse op so we dont need to re-implement on the subclasses\u001b[39;00m\n\u001b[1;32m   6366\u001b[0m     other, \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m other\u001b[38;5;241m.\u001b[39m_maybe_downcast_for_indexing(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   6368\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m, other\n",
      "File \u001b[0;32m~/miniconda3/envs/mainenv/lib/python3.11/site-packages/pandas/core/dtypes/common.py:137\u001b[0m, in \u001b[0;36mis_object_dtype\u001b[0;34m(arr_or_dtype)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;124;03m    Evaluate if the tipo is a subclass of the klasses\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;124;03m    and not a datetimelike.\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mlambda\u001b[39;00m tipo: (\n\u001b[1;32m    132\u001b[0m         \u001b[38;5;28missubclass\u001b[39m(tipo, klasses)\n\u001b[1;32m    133\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(tipo, (np\u001b[38;5;241m.\u001b[39mdatetime64, np\u001b[38;5;241m.\u001b[39mtimedelta64))\n\u001b[1;32m    134\u001b[0m     )\n\u001b[0;32m--> 137\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mis_object_dtype\u001b[39m(arr_or_dtype) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[1;32m    138\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;124;03m    Check whether an array-like or dtype is of the object dtype.\u001b[39;00m\n\u001b[1;32m    140\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;124;03m    False\u001b[39;00m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _is_dtype_type(arr_or_dtype, classes(np\u001b[38;5;241m.\u001b[39mobject_))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def test_random_samples(model, data, threshold, num_tests=50):\n",
    "    from sklearn.utils import shuffle\n",
    "    problematic_batches = 0\n",
    "    total_tests = num_tests\n",
    "\n",
    "    for seed in range(num_tests):\n",
    "        # 随机采样20%的数据\n",
    "        sample_data = data.sample(frac=0.2, random_state=seed).reset_index(drop=True)\n",
    "        sample_dataset = GraphDataset(sample_data)\n",
    "        sample_loader = DataLoader(sample_dataset, batch_size=batch_size, shuffle=False)\n",
    "        issues, samples, ratio = detect_quality_issues(model, sample_loader, threshold)\n",
    "\n",
    "        if ratio > 0.06:  # 超过6%的样本有问题\n",
    "            print(f\"Random sample {seed} is problematic: {issues} out of {samples} samples are faulty ({ratio * 100:.2f}%).\")\n",
    "            problematic_batches += 1\n",
    "        else:\n",
    "            print(f\"Random sample {seed} is ok: {issues} out of {samples} samples are faulty ({ratio * 100:.2f}%).\")\n",
    "\n",
    "    correct_batches = total_tests - problematic_batches\n",
    "    correct_ratio = correct_batches / total_tests\n",
    "\n",
    "    print(f\"Total problematic batches across all tests: {problematic_batches}/{total_tests}\")\n",
    "    print(f\"Correct ratio: {correct_batches}/{total_tests} ({correct_ratio * 100:.2f}%)\")\n",
    "\n",
    "    return correct_batches, total_tests\n",
    "\n",
    "# 对测试集1（干净数据）执行随机采样测试\n",
    "print(\"Testing on Test Set 1 (Clean Data):\")\n",
    "correct_batches_1, total_tests_1 = test_random_samples(model, test_data_1, threshold)\n",
    "\n",
    "# 对测试集2（脏数据）执行随机采样测试\n",
    "print(\"Testing on Test Set 2 (Dirty Data):\")\n",
    "correct_batches_2, total_tests_2 = test_random_samples(model, test_data_2, threshold)\n",
    "\n",
    "# 计算总的预测正确的百分比\n",
    "total_correct_batches = correct_batches_1 + correct_batches_2\n",
    "total_tests = total_tests_1 + total_tests_2\n",
    "overall_correct_ratio = total_correct_batches / total_tests\n",
    "\n",
    "print(f\"\\nOverall correct ratio across all tests: {total_correct_batches}/{total_tests} ({overall_correct_ratio * 100:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary of Overall Correct Ratios for Different Sample Sizes:\n",
      "Sample Size 10: Correct Ratio = 85.00%\n",
      "Sample Size 20: Correct Ratio = 93.00%\n",
      "Sample Size 50: Correct Ratio = 99.00%\n",
      "Sample Size 100: Correct Ratio = 99.00%\n",
      "Sample Size 500: Correct Ratio = 100.00%\n",
      "Sample Size 1000: Correct Ratio = 100.00%\n"
     ]
    }
   ],
   "source": [
    "def test_random_samples(model, data, threshold, num_tests=50, sample_size=0.2):\n",
    "    from sklearn.utils import shuffle\n",
    "    problematic_batches = 0\n",
    "    total_tests = num_tests\n",
    "\n",
    "    for seed in range(num_tests):\n",
    "        # 随机采样指定数量的数据\n",
    "        sample_data = data.sample(n=sample_size, random_state=seed).reset_index(drop=True)\n",
    "        sample_dataset = GraphDataset(sample_data)\n",
    "        sample_loader = DataLoader(sample_dataset, batch_size=batch_size, shuffle=False)\n",
    "        issues, samples, ratio = detect_quality_issues(model, sample_loader, threshold)\n",
    "\n",
    "        if ratio > 0.06:  # 超过6%的样本有问题\n",
    "            #print(f\"Random sample {seed} is problematic: {issues} out of {samples} samples are faulty ({ratio * 100:.2f}%).\")\n",
    "            problematic_batches += 1\n",
    "        #else:\n",
    "            #print(f\"Random sample {seed} is ok: {issues} out of {samples} samples are faulty ({ratio * 100:.2f}%).\")\n",
    "\n",
    "    correct_batches = total_tests - problematic_batches\n",
    "    correct_ratio = correct_batches / total_tests\n",
    "\n",
    "    # print(f\"Total problematic batches across all tests: {problematic_batches}/{total_tests}\")\n",
    "    # print(f\"Correct ratio: {correct_batches}/{total_tests} ({correct_ratio * 100:.2f}%)\")\n",
    "\n",
    "    return correct_batches, total_tests\n",
    "\n",
    "# 定义不同的采样大小\n",
    "sample_sizes = [10 ,20,50, 100, 500, 1000]\n",
    "\n",
    "# 用于保存每个采样大小下的总体正确率\n",
    "overall_correct_ratios = []\n",
    "\n",
    "for sample_size in sample_sizes:\n",
    "    #print(f\"\\nTesting with sample size: {sample_size}\")\n",
    "\n",
    "    # 对测试集1（干净数据）执行随机采样测试\n",
    "    #print(f\"Testing on Test Set 1 (Clean Data) with sample size {sample_size}:\")\n",
    "    correct_batches_1, total_tests_1 = test_random_samples(model, test_data_1, threshold, sample_size=sample_size)\n",
    "\n",
    "    # 对测试集2（脏数据）执行随机采样测试\n",
    "    #print(f\"Testing on Test Set 2 (Dirty Data) with sample size {sample_size}:\")\n",
    "    correct_batches_2, total_tests_2 = test_random_samples(model, test_data_2, threshold, sample_size=sample_size)\n",
    "\n",
    "    # 计算总的预测正确的百分比\n",
    "    total_correct_batches = correct_batches_1 + (50-correct_batches_2)\n",
    "    total_tests = total_tests_1 + total_tests_2\n",
    "    overall_correct_ratio = total_correct_batches / total_tests\n",
    "\n",
    "    overall_correct_ratios.append((sample_size, overall_correct_ratio))\n",
    "\n",
    "    #print(f\"\\nOverall correct ratio for sample size {sample_size}: {total_correct_batches}/{total_tests} ({overall_correct_ratio * 100:.2f}%)\")\n",
    "\n",
    "# 显示不同采样大小下的总体正确率\n",
    "print(\"\\nSummary of Overall Correct Ratios for Different Sample Sizes:\")\n",
    "for sample_size, ratio in overall_correct_ratios:\n",
    "    print(f\"Sample Size {sample_size}: Correct Ratio = {ratio * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 数据质量验证和数据修复\n",
    "### 6.1 检测数据质量问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 修复数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repair_data(model, data_loader):\n",
    "    model.eval()\n",
    "    repaired_data = []\n",
    "    with torch.no_grad():\n",
    "        for data in data_loader:\n",
    "            _, out_repair = model(data)\n",
    "            repaired_instance = out_repair.squeeze().numpy()\n",
    "            repaired_data.append(repaired_instance)\n",
    "    repaired_data = np.array(repaired_data)\n",
    "    return repaired_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. 应用于脏数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mainenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
