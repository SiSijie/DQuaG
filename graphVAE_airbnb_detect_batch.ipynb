{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b91df9ab-79c4-46e0-a477-ac62672be489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 102599 entries, 0 to 102598\n",
      "Data columns (total 23 columns):\n",
      " #   Column  Non-Null Count   Dtype  \n",
      "---  ------  --------------   -----  \n",
      " 0   0       102599 non-null  float64\n",
      " 1   1       102599 non-null  float64\n",
      " 2   2       102599 non-null  float64\n",
      " 3   3       102599 non-null  float64\n",
      " 4   4       102599 non-null  float64\n",
      " 5   5       102599 non-null  float64\n",
      " 6   6       102599 non-null  float64\n",
      " 7   7       102599 non-null  float64\n",
      " 8   8       102599 non-null  float64\n",
      " 9   9       102599 non-null  float64\n",
      " 10  10      102599 non-null  float64\n",
      " 11  11      102599 non-null  float64\n",
      " 12  12      102599 non-null  float64\n",
      " 13  13      102599 non-null  float64\n",
      " 14  14      102599 non-null  float64\n",
      " 15  15      102599 non-null  float64\n",
      " 16  16      102599 non-null  float64\n",
      " 17  17      102599 non-null  float64\n",
      " 18  18      102599 non-null  float64\n",
      " 19  19      102599 non-null  float64\n",
      " 20  20      102599 non-null  float64\n",
      " 21  21      102599 non-null  float64\n",
      " 22  22      102599 non-null  float64\n",
      "dtypes: float64(23)\n",
      "memory usage: 18.0 MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# 加载数据\n",
    "file_path_clean = '/home/sdong/data/airbnb/airbnb_nyc_clean_embeddings.csv'\n",
    "file_path_origi = '/home/sdong/data/airbnb/Airbnb_Open_Data_Alignement_embeddings.csv'\n",
    "data = pd.read_csv(file_path_clean)\n",
    "data_dirty = pd.read_csv(file_path_origi)\n",
    "# 设置显示所有列和部分行\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 10)  \n",
    "# 显示数据集的前几行和数据结构\n",
    "\n",
    "print(data_dirty.info())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "27dc078d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean data values are within [0, 1]: False\n",
      "Dirty data values are within [0, 1]: False\n",
      "Clean data contains values out of range [0, 1].\n",
      "Dirty data contains values out of range [0, 1].\n",
      "Out of range values in clean data:\n",
      "        0   1   2   3   4   5   6   7   8   9  10   11  12  13  14  15  16  \\\n",
      "2173  NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  1.0 NaN NaN NaN NaN NaN   \n",
      "49653 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  NaN NaN NaN NaN NaN NaN   \n",
      "\n",
      "       17  18  19  20  21   22  \n",
      "2173  NaN NaN NaN NaN NaN  NaN  \n",
      "49653 NaN NaN NaN NaN NaN  1.0  \n",
      "Out of range values in dirty data:\n",
      "        0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  \\\n",
      "63948 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN   \n",
      "\n",
      "        17  18  19  20  21  22  \n",
      "63948  1.0 NaN NaN NaN NaN NaN  \n"
     ]
    }
   ],
   "source": [
    "# 检查两个数据集中是否所有值都在0到1之间\n",
    "def check_values_in_range(df, lower=0, upper=1):\n",
    "    return ((df >= lower) & (df <= upper)).all().all()\n",
    "\n",
    "is_data_in_range = check_values_in_range(data)\n",
    "is_data_dirty_in_range = check_values_in_range(data_dirty)\n",
    "\n",
    "print(f\"Clean data values are within [0, 1]: {is_data_in_range}\")\n",
    "print(f\"Dirty data values are within [0, 1]: {is_data_dirty_in_range}\")\n",
    "\n",
    "if not is_data_in_range:\n",
    "    print(\"Clean data contains values out of range [0, 1].\")\n",
    "\n",
    "if not is_data_dirty_in_range:\n",
    "    print(\"Dirty data contains values out of range [0, 1].\")\n",
    "\n",
    "# 如果需要，打印出不在范围内的值和对应的索引\n",
    "def find_out_of_range_values(df, lower=0, upper=1):\n",
    "    out_of_range = df[(df < lower) | (df > upper)]\n",
    "    return out_of_range.dropna(how='all')\n",
    "\n",
    "if not is_data_in_range:\n",
    "    out_of_range_clean = find_out_of_range_values(data)\n",
    "    print(\"Out of range values in clean data:\")\n",
    "    print(out_of_range_clean)\n",
    "\n",
    "if not is_data_dirty_in_range:\n",
    "    out_of_range_dirty = find_out_of_range_values(data_dirty)\n",
    "    print(\"Out of range values in dirty data:\")\n",
    "    print(out_of_range_dirty)\n",
    "    \n",
    "    # 获取不在范围内的值的索引\n",
    "out_of_range_clean_indices = out_of_range_clean.index\n",
    "out_of_range_dirty_indices = out_of_range_dirty.index\n",
    "\n",
    "# 删除不在范围内的行\n",
    "data = data.drop(out_of_range_clean_indices)\n",
    "data_dirty = data_dirty.drop(out_of_range_dirty_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "14966a57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 102598 entries, 0 to 102598\n",
      "Data columns (total 23 columns):\n",
      " #   Column  Non-Null Count   Dtype  \n",
      "---  ------  --------------   -----  \n",
      " 0   0       102598 non-null  float64\n",
      " 1   1       102598 non-null  float64\n",
      " 2   2       102598 non-null  float64\n",
      " 3   3       102598 non-null  float64\n",
      " 4   4       102598 non-null  float64\n",
      " 5   5       102598 non-null  float64\n",
      " 6   6       102598 non-null  float64\n",
      " 7   7       102598 non-null  float64\n",
      " 8   8       102598 non-null  float64\n",
      " 9   9       102598 non-null  float64\n",
      " 10  10      102598 non-null  float64\n",
      " 11  11      102598 non-null  float64\n",
      " 12  12      102598 non-null  float64\n",
      " 13  13      102598 non-null  float64\n",
      " 14  14      102598 non-null  float64\n",
      " 15  15      102598 non-null  float64\n",
      " 16  16      102598 non-null  float64\n",
      " 17  17      102598 non-null  float64\n",
      " 18  18      102598 non-null  float64\n",
      " 19  19      102598 non-null  float64\n",
      " 20  20      102598 non-null  float64\n",
      " 21  21      102598 non-null  float64\n",
      " 22  22      102598 non-null  float64\n",
      "dtypes: float64(23)\n",
      "memory usage: 18.8 MB\n"
     ]
    }
   ],
   "source": [
    "data_dirty.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5887b650",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 假设 data 已经是一个经过预处理的 DataFrame\n",
    "data_array = data.values.astype(np.float32)  # 转换为浮点数类型的 NumPy 数组\n",
    "\n",
    "# 分割数据为训练集和临时测试集（包括真正的测试集和验证集）\n",
    "train_data, val_test_data = train_test_split(data_array, test_size=0.5, random_state=42)\n",
    "\n",
    "# 将训练验证集进一步分割为训练集和验证集\n",
    "val_data, test_data = train_test_split(val_test_data, test_size=0.5, random_state=42)  # 0.25 x 0.8 = 0.2\n",
    "\n",
    "\n",
    "# 转换为PyTorch张量\n",
    "\n",
    "\n",
    "# 创建数据加载器\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "batch_size = 128  # 或者任何适合你GPU的大小\n",
    "\n",
    "train_tensor = torch.tensor(train_data) #0.6\n",
    "train_dataset = TensorDataset(train_tensor, train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "val_tensor = torch.tensor(val_data) #0.2\n",
    "val_dataset = TensorDataset(val_tensor, val_tensor)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "test_tensor = torch.tensor(test_data)  #20%\n",
    "test_dataset = TensorDataset(test_tensor, test_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)  # 50个批次\n",
    "#len(test_dataset)// 50\n",
    "\n",
    "data_dirty_array = data_dirty.values.astype(np.float32)  # 转换为浮点数类型的 NumPy 数组\n",
    "test_dirty_tensor = torch.tensor(data_dirty_array)  #20%\n",
    "test_dirty_dataset = TensorDataset(test_dirty_tensor, test_dirty_tensor)\n",
    "test_dirty_loader = DataLoader(test_dirty_dataset, batch_size=batch_size, shuffle=False)  # 50个批次\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3d31c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "92722c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 检查数据中是否有NaN或无穷大的值\n",
    "# if torch.isnan(train_tensor).any() or torch.isinf(train_tensor).any():\n",
    "#     print(\"Data contains NaNs or Infs.\")\n",
    "# # 检查数据中是否有NaN或无穷大的值\n",
    "# if torch.isnan(test_dirty_tensor).any() or torch.isinf(test_dirty_tensor).any():\n",
    "#     print(\"Data contains NaNs or Infs.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "63a8f2af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAE(\n",
      "  (fc1): Linear(in_features=23, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc31): Linear(in_features=64, out_features=20, bias=True)\n",
      "  (fc32): Linear(in_features=64, out_features=20, bias=True)\n",
      "  (fc4): Linear(in_features=20, out_features=64, bias=True)\n",
      "  (fc5): Linear(in_features=64, out_features=128, bias=True)\n",
      "  (fc6): Linear(in_features=128, out_features=23, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "        # Encoder\n",
    "        self.fc1 = nn.Linear(23, 128)  # Input layer\n",
    "        self.fc2 = nn.Linear(128, 64)  # Hidden layer\n",
    "        self.fc31 = nn.Linear(64, 20)  # Output layer for mu\n",
    "        self.fc32 = nn.Linear(64, 20)  # Output layer for logvar\n",
    "\n",
    "        # Decoder\n",
    "        self.fc4 = nn.Linear(20, 64)   # Input layer\n",
    "        self.fc5 = nn.Linear(64, 128)  # Hidden layer\n",
    "        self.fc6 = nn.Linear(128, 23)  # Output layer\n",
    "\n",
    "    def encode(self, x):\n",
    "        h1 = F.relu(self.fc1(x))\n",
    "        h2 = F.relu(self.fc2(h1))\n",
    "        return self.fc31(h2), self.fc32(h2)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar) + 1e-8  # Adding a small constant for numerical stability\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z):\n",
    "        h3 = F.relu(self.fc4(z))\n",
    "        h4 = F.relu(self.fc5(h3))\n",
    "        return torch.sigmoid(self.fc6(h4))  # Use sigmoid to ensure output is between 0 and 1\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n",
    "\n",
    "# Instantiate the model\n",
    "model = VAE()\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4db9f9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "\n",
    "\n",
    "# # 定义VAE的架构\n",
    "# class VAE(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(VAE, self).__init__()\n",
    "#         self.fc1 = nn.Linear(23, 12)  # 假设有23个特征\n",
    "#         self.fc21 = nn.Linear(12, 6)  # 均值输出\n",
    "#         self.fc22 = nn.Linear(12, 6)  # 方差输出\n",
    "#         self.fc3 = nn.Linear(6, 12)\n",
    "#         self.fc4 = nn.Linear(12, 23)\n",
    "\n",
    "#     def encode(self, x):\n",
    "#         h1 = F.relu(self.fc1(x))\n",
    "#         return self.fc21(h1), self.fc22(h1)\n",
    "\n",
    "#     def reparameterize(self, mu, logvar):\n",
    "#         std = torch.exp(0.5 * logvar) + 1e-8  # 添加一个小常数以提高数值稳定性\n",
    "#         eps = torch.randn_like(std)\n",
    "#         return mu + eps*std\n",
    "\n",
    "#     def decode(self, z):\n",
    "#         h3 = F.relu(self.fc3(z))\n",
    "#         return torch.sigmoid(self.fc4(h3))\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         mu, logvar = self.encode(x.view(-1, 23))\n",
    "#         z = self.reparameterize(mu, logvar)\n",
    "#         return self.decode(z), mu, logvar\n",
    "\n",
    "# # 实例化模型\n",
    "# model = VAE()\n",
    "# print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c1d03c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#device = torch.device(\"cpu\")\n",
    "model = VAE().to(device)\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "# 设置优化器\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    # 确保目标张量也是浮点类型且维度匹配\n",
    "    recon_x = torch.clamp(recon_x, 0, 1)  # 确保输出值在[0, 1]范围内\n",
    "    BCE = F.binary_cross_entropy(recon_x, x, reduction='none')  # 保留每个样本的损失\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp(), dim=1, keepdim=True)  # 保持维度\n",
    "    return BCE + KLD\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c4a9e966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Total Loss: 15.016054565891181\n",
      "Epoch: 2 Total Loss: 14.904005591581953\n",
      "Epoch: 3 Total Loss: 14.898292319784467\n",
      "Epoch: 4 Total Loss: 14.895097270004609\n",
      "Epoch: 5 Total Loss: 14.892928285828015\n",
      "Epoch: 6 Total Loss: 14.892401732323968\n",
      "Epoch: 7 Total Loss: 14.89168929206025\n",
      "Epoch: 8 Total Loss: 14.891398653323948\n",
      "Epoch: 9 Total Loss: 14.89072084973197\n",
      "Epoch: 10 Total Loss: 14.890612860255205\n",
      "Model saved to vae_model_airnbnb_graph.pth\n",
      "CUDA cache cleared.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    total_BCE = 0\n",
    "    total_KLD = 0\n",
    "    total_loss = 0\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):  # 使用TensorDataset，数据被重复用作输入和标签\n",
    "        data = data.to(device)\n",
    "        # 检查输入数据的范围\n",
    "        if (data < 0).any() or (data > 1).any():\n",
    "            raise ValueError(\"Input data contains values out of range [0, 1]\")\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        if (recon_batch < 0).any() or (recon_batch > 1).any():\n",
    "            raise ValueError(\"Warning: recon_batch contains values out of range [0, 1]\")\n",
    "        loss = loss_function(recon_batch, data, mu, logvar)  # 这里loss是每个样本的损失\n",
    "        loss.mean().backward()  # 使用.mean()在所有样本上平均损失然后进行反向传播\n",
    "        total_loss += loss.sum().item()  # 更新总损失\n",
    "        optimizer.step()\n",
    "        \n",
    "        # if batch_idx % 100 == 0:\n",
    "        #     print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "        #         epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "        #         100. * batch_idx / len(train_loader), loss.mean().item()))\n",
    "\n",
    "    # 打印每个epoch的平均损失\n",
    "    print(f'Epoch: {epoch} Total Loss: {total_loss / len(train_loader.dataset)}')\n",
    "\n",
    "# 训练模型\n",
    "num_epochs = 10  # 可根据需要调整\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    train(epoch)\n",
    "\n",
    "# 保存模型的状态字典\n",
    "torch.save(model.state_dict(), 'vae_model_airnbnb_graph.pth')\n",
    "print(\"Model saved to vae_model_airnbnb_graph.pth\")\n",
    "\n",
    "# 添加调试信息\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.synchronize()\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"CUDA cache cleared.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5b6b9ddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss on validation data: 14.896268011254085\n",
      "Average loss on test data: 14.898248455175747\n",
      "Average loss on test dirty data: 15.14082212836104\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, data_loader):\n",
    "    model.eval()  # 切换到评估模式\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():  # 关闭梯度计算\n",
    "        for inputs, _ in data_loader:  # 假设 data_loader 返回 inputs 和 targets，这里我们不需要 targets\n",
    "            inputs = inputs.to(device)  # 确保将 inputs 转移到正确的设备\n",
    "            recon, mu, logvar = model(inputs)\n",
    "            loss = loss_function(recon, inputs, mu, logvar)  # 每个样本的损失列表\n",
    "            total_loss += loss.sum().item()  # 累计所有样本的损失\n",
    "\n",
    "    average_loss = total_loss / len(data_loader.dataset)\n",
    "    return average_loss\n",
    "\n",
    "# 加载模型\n",
    "model = VAE().to(device)\n",
    "model.load_state_dict(torch.load('vae_model_airnbnb_graph.pth'))\n",
    "\n",
    "# 计算测试集和验证集上的平均损失\n",
    "val_loss = evaluate_model(model, val_loader)\n",
    "test_loss = evaluate_model(model, test_loader)\n",
    "test_dirty_loss = evaluate_model(model, test_dirty_loader)\n",
    "\n",
    "print(f\"Average loss on validation data: {val_loss}\")\n",
    "print(f\"Average loss on test data: {test_loss}\")\n",
    "print(f\"Average loss on test dirty data: {test_dirty_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8950bc94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss threshold for detecting data quality issues: 15.640409708023071\n",
      "Min validation error: 13.763216018676758\n",
      "Max validation error: 17.723398208618164\n",
      "Mean validation error: 14.895785944354213\n",
      "95th percentile of validation errors: 15.640409708023071\n",
      "Maximum validation error (100th percentile): 17.723398208618164\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def collect_reconstruction_errors(model, data_loader):\n",
    "    model.eval()\n",
    "    reconstruction_errors = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, _ in data_loader:  # 假设 data_loader 返回的是 inputs 和 labels，这里我们忽略 labels\n",
    "            inputs = inputs.to(device)  # 将输入数据移动到正确的设备\n",
    "            recon, mu, logvar = model(inputs)\n",
    "            loss = loss_function(recon, inputs, mu, logvar)  # 每个样本的损失列表\n",
    "            loss_per_sample = loss.sum(dim=1)\n",
    "            # print(\"Type of loss:\", type(loss_per_sample))\n",
    "            # print(\"Shape of loss:\", loss_per_sample.shape)\n",
    "            #print(\"First few loss values:\", loss[:10])  # 打印前10个损失值\n",
    "            reconstruction_errors.extend(loss_per_sample.tolist())  \n",
    "\n",
    "    return reconstruction_errors\n",
    "\n",
    "# 收集验证集的重构误差\n",
    "val_errors = collect_reconstruction_errors(model, val_loader)\n",
    "\n",
    "threshold = np.quantile(val_errors, 0.95)  # 计算95%分位数作为阈值\n",
    "print(f\"Loss threshold for detecting data quality issues: {threshold}\")\n",
    "\n",
    "min_val_error = min(val_errors)\n",
    "max_val_error = max(val_errors)\n",
    "mean_val_error = sum(val_errors) / len(val_errors)\n",
    "print(f\"Min validation error: {min_val_error}\")\n",
    "print(f\"Max validation error: {max_val_error}\")\n",
    "print(f\"Mean validation error: {mean_val_error}\")\n",
    "print(f\"95th percentile of validation errors: {np.quantile(val_errors, 0.95)}\")\n",
    "print(f\"Maximum validation error (100th percentile): {np.quantile(val_errors, 1)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af88823a",
   "metadata": {},
   "source": [
    "## Loop test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2738ff28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random sample 0 is ok: 167 out of 3466 samples are faulty (4.82%).\n",
      "Random sample 1 is ok: 162 out of 3466 samples are faulty (4.67%).\n",
      "Random sample 2 is ok: 162 out of 3466 samples are faulty (4.67%).\n",
      "Random sample 3 is ok: 163 out of 3466 samples are faulty (4.70%).\n",
      "Random sample 4 is ok: 191 out of 3466 samples are faulty (5.51%).\n",
      "Random sample 5 is ok: 165 out of 3466 samples are faulty (4.76%).\n",
      "Random sample 6 is ok: 165 out of 3466 samples are faulty (4.76%).\n",
      "Random sample 7 is ok: 197 out of 3466 samples are faulty (5.68%).\n",
      "Random sample 8 is ok: 171 out of 3466 samples are faulty (4.93%).\n",
      "Random sample 9 is ok: 172 out of 3466 samples are faulty (4.96%).\n",
      "Random sample 10 is ok: 184 out of 3466 samples are faulty (5.31%).\n",
      "Random sample 11 is ok: 166 out of 3466 samples are faulty (4.79%).\n",
      "Random sample 12 is ok: 177 out of 3466 samples are faulty (5.11%).\n",
      "Random sample 13 is ok: 160 out of 3466 samples are faulty (4.62%).\n",
      "Random sample 14 is ok: 159 out of 3466 samples are faulty (4.59%).\n",
      "Random sample 15 is ok: 164 out of 3466 samples are faulty (4.73%).\n",
      "Random sample 16 is ok: 168 out of 3466 samples are faulty (4.85%).\n",
      "Random sample 17 is ok: 173 out of 3466 samples are faulty (4.99%).\n",
      "Random sample 18 is ok: 151 out of 3466 samples are faulty (4.36%).\n",
      "Random sample 19 is ok: 182 out of 3466 samples are faulty (5.25%).\n",
      "Random sample 20 is ok: 176 out of 3466 samples are faulty (5.08%).\n",
      "Random sample 21 is ok: 183 out of 3466 samples are faulty (5.28%).\n",
      "Random sample 22 is ok: 168 out of 3466 samples are faulty (4.85%).\n",
      "Random sample 23 is ok: 179 out of 3466 samples are faulty (5.16%).\n",
      "Random sample 24 is ok: 183 out of 3466 samples are faulty (5.28%).\n",
      "Random sample 25 is ok: 194 out of 3466 samples are faulty (5.60%).\n",
      "Random sample 26 is ok: 150 out of 3466 samples are faulty (4.33%).\n",
      "Random sample 27 is ok: 170 out of 3466 samples are faulty (4.90%).\n",
      "Random sample 28 is ok: 184 out of 3466 samples are faulty (5.31%).\n",
      "Random sample 29 is ok: 177 out of 3466 samples are faulty (5.11%).\n",
      "Random sample 30 is ok: 171 out of 3466 samples are faulty (4.93%).\n",
      "Random sample 31 is ok: 148 out of 3466 samples are faulty (4.27%).\n",
      "Random sample 32 is ok: 183 out of 3466 samples are faulty (5.28%).\n",
      "Random sample 33 is ok: 163 out of 3466 samples are faulty (4.70%).\n",
      "Random sample 34 is ok: 164 out of 3466 samples are faulty (4.73%).\n",
      "Random sample 35 is ok: 184 out of 3466 samples are faulty (5.31%).\n",
      "Random sample 36 is ok: 171 out of 3466 samples are faulty (4.93%).\n",
      "Random sample 37 is ok: 176 out of 3466 samples are faulty (5.08%).\n",
      "Random sample 38 is ok: 154 out of 3466 samples are faulty (4.44%).\n",
      "Random sample 39 is ok: 173 out of 3466 samples are faulty (4.99%).\n",
      "Random sample 40 is ok: 174 out of 3466 samples are faulty (5.02%).\n",
      "Random sample 41 is ok: 162 out of 3466 samples are faulty (4.67%).\n",
      "Random sample 42 is ok: 183 out of 3466 samples are faulty (5.28%).\n",
      "Random sample 43 is ok: 171 out of 3466 samples are faulty (4.93%).\n",
      "Random sample 44 is ok: 171 out of 3466 samples are faulty (4.93%).\n",
      "Random sample 45 is ok: 182 out of 3466 samples are faulty (5.25%).\n",
      "Random sample 46 is ok: 172 out of 3466 samples are faulty (4.96%).\n",
      "Random sample 47 is ok: 172 out of 3466 samples are faulty (4.96%).\n",
      "Random sample 48 is ok: 164 out of 3466 samples are faulty (4.73%).\n",
      "Random sample 49 is ok: 148 out of 3466 samples are faulty (4.27%).\n",
      "Total problematic batches across all tests: 0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def detect_quality_issues(model, data_loader, threshold, seed):\n",
    "    model.eval()\n",
    "    current_issues_count = 0\n",
    "    total_samples = 0  # 累积处理的样本总数\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, _ in data_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            recon, mu, logvar = model(inputs)\n",
    "            losses = loss_function(recon, inputs, mu, logvar)  # 获取每个样本的损失\n",
    "            loss_per_sample = losses.sum(dim=1)\n",
    "            # 检查每个样本是否有问题\n",
    "            for loss in loss_per_sample:\n",
    "                if loss.item() > threshold:\n",
    "                    current_issues_count += 1\n",
    "            total_samples += inputs.size(0)  # 更新处理的样本总数\n",
    "\n",
    "    # 评估是否有超过10%的样本有问题\n",
    "    if current_issues_count > total_samples * 0.05*1.2:\n",
    "        print(f\"Random sample {seed} is problematic: {current_issues_count} out of {total_samples} samples are faulty ({(current_issues_count/total_samples * 100):.2f}%).\")\n",
    "        return True\n",
    "    else:\n",
    "        print(f\"Random sample {seed} is ok: {current_issues_count} out of {total_samples} samples are faulty ({(current_issues_count/total_samples * 100):.2f}%).\")\n",
    "        return False\n",
    "\n",
    "# 主函数：执行50次随机采样测试\n",
    "def test_random_samples(model, data, threshold):\n",
    "    problematic_batches = 0\n",
    "    for seed in range(50):\n",
    "        _, sample_data = train_test_split(data, test_size=0.2, random_state=seed)  # 随机采样20%\n",
    "        sample_tensor = torch.tensor(sample_data, dtype=torch.float32)  # 确保数据类型是float32\n",
    "        sample_dataset = TensorDataset(sample_tensor, sample_tensor)\n",
    "        test_dirty_loader = DataLoader(sample_dataset, batch_size=batch_size, shuffle=False)\n",
    "        if detect_quality_issues(model, test_dirty_loader, threshold, seed):\n",
    "            problematic_batches += 1\n",
    "\n",
    "    print(f\"Total problematic batches across all tests: {problematic_batches}\")\n",
    "\n",
    "# 假设 model, data_dirty, threshold, device 已经被正确定义和设置\n",
    "test_random_samples(model, test_data, threshold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "274ccd45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random sample 0 is problematic: 1440 out of 20520 samples are faulty (7.02%).\n",
      "Random sample 1 is problematic: 1459 out of 20520 samples are faulty (7.11%).\n",
      "Random sample 2 is problematic: 1386 out of 20520 samples are faulty (6.75%).\n",
      "Random sample 3 is problematic: 1450 out of 20520 samples are faulty (7.07%).\n",
      "Random sample 4 is problematic: 1387 out of 20520 samples are faulty (6.76%).\n",
      "Random sample 5 is problematic: 1430 out of 20520 samples are faulty (6.97%).\n",
      "Random sample 6 is problematic: 1409 out of 20520 samples are faulty (6.87%).\n",
      "Random sample 7 is problematic: 1462 out of 20520 samples are faulty (7.12%).\n",
      "Random sample 8 is problematic: 1446 out of 20520 samples are faulty (7.05%).\n",
      "Random sample 9 is problematic: 1459 out of 20520 samples are faulty (7.11%).\n",
      "Random sample 10 is problematic: 1452 out of 20520 samples are faulty (7.08%).\n",
      "Random sample 11 is problematic: 1412 out of 20520 samples are faulty (6.88%).\n",
      "Random sample 12 is problematic: 1459 out of 20520 samples are faulty (7.11%).\n",
      "Random sample 13 is problematic: 1398 out of 20520 samples are faulty (6.81%).\n",
      "Random sample 14 is problematic: 1427 out of 20520 samples are faulty (6.95%).\n",
      "Random sample 15 is problematic: 1398 out of 20520 samples are faulty (6.81%).\n",
      "Random sample 16 is problematic: 1362 out of 20520 samples are faulty (6.64%).\n",
      "Random sample 17 is problematic: 1469 out of 20520 samples are faulty (7.16%).\n",
      "Random sample 18 is problematic: 1466 out of 20520 samples are faulty (7.14%).\n",
      "Random sample 19 is problematic: 1456 out of 20520 samples are faulty (7.10%).\n",
      "Random sample 20 is problematic: 1420 out of 20520 samples are faulty (6.92%).\n",
      "Random sample 21 is problematic: 1434 out of 20520 samples are faulty (6.99%).\n",
      "Random sample 22 is problematic: 1426 out of 20520 samples are faulty (6.95%).\n",
      "Random sample 23 is problematic: 1472 out of 20520 samples are faulty (7.17%).\n",
      "Random sample 24 is problematic: 1451 out of 20520 samples are faulty (7.07%).\n",
      "Random sample 25 is problematic: 1421 out of 20520 samples are faulty (6.92%).\n",
      "Random sample 26 is problematic: 1411 out of 20520 samples are faulty (6.88%).\n",
      "Random sample 27 is problematic: 1415 out of 20520 samples are faulty (6.90%).\n",
      "Random sample 28 is problematic: 1412 out of 20520 samples are faulty (6.88%).\n",
      "Random sample 29 is problematic: 1403 out of 20520 samples are faulty (6.84%).\n",
      "Random sample 30 is problematic: 1428 out of 20520 samples are faulty (6.96%).\n",
      "Random sample 31 is problematic: 1435 out of 20520 samples are faulty (6.99%).\n",
      "Random sample 32 is problematic: 1467 out of 20520 samples are faulty (7.15%).\n",
      "Random sample 33 is problematic: 1446 out of 20520 samples are faulty (7.05%).\n",
      "Random sample 34 is problematic: 1403 out of 20520 samples are faulty (6.84%).\n",
      "Random sample 35 is problematic: 1407 out of 20520 samples are faulty (6.86%).\n",
      "Random sample 36 is problematic: 1444 out of 20520 samples are faulty (7.04%).\n",
      "Random sample 37 is problematic: 1449 out of 20520 samples are faulty (7.06%).\n",
      "Random sample 38 is problematic: 1452 out of 20520 samples are faulty (7.08%).\n",
      "Random sample 39 is problematic: 1386 out of 20520 samples are faulty (6.75%).\n",
      "Random sample 40 is problematic: 1479 out of 20520 samples are faulty (7.21%).\n",
      "Random sample 41 is problematic: 1372 out of 20520 samples are faulty (6.69%).\n",
      "Random sample 42 is problematic: 1445 out of 20520 samples are faulty (7.04%).\n",
      "Random sample 43 is problematic: 1420 out of 20520 samples are faulty (6.92%).\n",
      "Random sample 44 is problematic: 1418 out of 20520 samples are faulty (6.91%).\n",
      "Random sample 45 is problematic: 1461 out of 20520 samples are faulty (7.12%).\n",
      "Random sample 46 is problematic: 1401 out of 20520 samples are faulty (6.83%).\n",
      "Random sample 47 is problematic: 1439 out of 20520 samples are faulty (7.01%).\n",
      "Random sample 48 is problematic: 1480 out of 20520 samples are faulty (7.21%).\n",
      "Random sample 49 is problematic: 1426 out of 20520 samples are faulty (6.95%).\n",
      "Total problematic batches across all tests: 50\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def detect_quality_issues(model, data_loader, threshold, seed):\n",
    "    model.eval()\n",
    "    current_issues_count = 0\n",
    "    total_samples = 0  # 累积处理的样本总数\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, _ in data_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            recon, mu, logvar = model(inputs)\n",
    "            losses = loss_function(recon, inputs, mu, logvar)  # 获取每个样本的损失\n",
    "            loss_per_sample = losses.sum(dim=1)\n",
    "            # 检查每个样本是否有问题\n",
    "            for loss in loss_per_sample:\n",
    "                if loss.item() > threshold:\n",
    "                    current_issues_count += 1\n",
    "            total_samples += inputs.size(0)  # 更新处理的样本总数\n",
    "\n",
    "    # 评估是否有超过10%的样本有问题\n",
    "    if current_issues_count > total_samples * 0.06:\n",
    "        print(f\"Random sample {seed} is problematic: {current_issues_count} out of {total_samples} samples are faulty ({(current_issues_count/total_samples * 100):.2f}%).\")\n",
    "        return True\n",
    "    else:\n",
    "        print(f\"Random sample {seed} is ok: {current_issues_count} out of {total_samples} samples are faulty ({(current_issues_count/total_samples * 100):.2f}%).\")\n",
    "        return False\n",
    "\n",
    "# 主函数：执行50次随机采样测试\n",
    "def test_random_samples(model, data, threshold):\n",
    "    problematic_batches = 0\n",
    "    for seed in range(50):\n",
    "        _, sample_data = train_test_split(data, test_size=0.2, random_state=seed)  # 随机采样20%\n",
    "        sample_data_array = sample_data.values.astype(np.float32)  # 转换为浮点数类型的 NumPy 数组\n",
    "        sample_tensor = torch.tensor(sample_data_array)  # 确保数据类型是float32\n",
    "        sample_dataset = TensorDataset(sample_tensor, sample_tensor)\n",
    "        test_dirty_loader = DataLoader(sample_dataset, batch_size=batch_size, shuffle=False)\n",
    "        if detect_quality_issues(model, test_dirty_loader, threshold, seed):\n",
    "            problematic_batches += 1\n",
    "\n",
    "    print(f\"Total problematic batches across all tests: {problematic_batches}\")\n",
    "\n",
    "# 假设 model, data_dirty, threshold, device 已经被正确定义和设置\n",
    "test_random_samples(model, data_dirty, threshold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b7b267a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# def detect_quality_issues(model, data_loader, threshold):\n",
    "#     model.eval()\n",
    "#     total_issue_count = 0\n",
    "#     total_batches_with_issues = 0\n",
    "#     total_samples = 0\n",
    "#     current_batch_issues = 0\n",
    "#     batch_count = 0\n",
    "#     batch_size = len(data_loader.dataset) // 50  # 你希望的批次大小\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for inputs, _ in data_loader:\n",
    "#             inputs = inputs.to(device)  # 将输入数据移动到正确的设备\n",
    "#             recon, mu, logvar = model(inputs)\n",
    "#             BCE, KLD = loss_function(recon, inputs, mu, logvar)\n",
    "#             total_loss = BCE + KLD  # 计算当前样本的总损失\n",
    "#             total_samples += 1\n",
    "\n",
    "#             # 判断当前样本是否有问题\n",
    "#             if total_loss.item() > threshold:\n",
    "#                 current_batch_issues += 1\n",
    "\n",
    "#             # 当累积样本数达到你设定的批次大小时，评估这个批次\n",
    "#             if total_samples % batch_size == 0:\n",
    "#                 if current_batch_issues >= batch_size * 0.02:  # 判断这个批次是否有超过5%的样本有问题\n",
    "#                     print(f\"Batch {batch_count} is problematic: {current_batch_issues} out of {batch_size} samples are faulty ({(current_batch_issues/batch_size * 100):.2f}%).\")\n",
    "#                     total_batches_with_issues += 1\n",
    "#                     total_issue_count = total_issue_count + current_batch_issues\n",
    "#                 else:\n",
    "#                     total_issue_count = total_issue_count + current_batch_issues\n",
    "#                     print(f\"Batch {batch_count} is ok: {current_batch_issues} out of {batch_size} samples are faulty ({(current_batch_issues/batch_size * 100):.2f}%).\")\n",
    "#                 current_batch_issues = 0\n",
    "#                 batch_count += 1\n",
    "\n",
    "#     total_issue_rate = total_issue_count / total_samples\n",
    "#     print(f\"Total batches with issues: {total_batches_with_issues} out of {batch_count}\")\n",
    "#     print(f\"Total problematic samples: {total_issue_count} out of {total_samples} ({(total_issue_rate * 100):.2f}%)\")\n",
    "#     return total_issue_rate\n",
    "\n",
    "# # Example usage\n",
    "# # test_dataset = TensorDataset(test_tensor, test_tensor)\n",
    "# # test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "# issue_rate = detect_quality_issues(model, test_loader, threshold)\n",
    "# print(f\"Percentage of data quality issues detected in the test set: {issue_rate * 100:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c139b017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0 is ok: 12 out of 2051 samples are faulty (0.59%).\n",
      "Batch 1 is ok: 4 out of 2051 samples are faulty (0.20%).\n",
      "Batch 2 is ok: 2 out of 2051 samples are faulty (0.10%).\n",
      "Batch 3 is ok: 3 out of 2051 samples are faulty (0.15%).\n",
      "Batch 4 is ok: 2 out of 2051 samples are faulty (0.10%).\n",
      "Batch 5 is ok: 3 out of 2051 samples are faulty (0.15%).\n",
      "Batch 6 is ok: 2 out of 2051 samples are faulty (0.10%).\n",
      "Batch 7 is ok: 0 out of 2051 samples are faulty (0.00%).\n",
      "Batch 8 is ok: 0 out of 2051 samples are faulty (0.00%).\n",
      "Batch 9 is ok: 3 out of 2051 samples are faulty (0.15%).\n",
      "Batch 10 is ok: 5 out of 2051 samples are faulty (0.24%).\n",
      "Batch 11 is ok: 0 out of 2051 samples are faulty (0.00%).\n",
      "Batch 12 is ok: 1 out of 2051 samples are faulty (0.05%).\n",
      "Batch 13 is ok: 2 out of 2051 samples are faulty (0.10%).\n",
      "Batch 14 is ok: 1 out of 2051 samples are faulty (0.05%).\n",
      "Batch 15 is ok: 1 out of 2051 samples are faulty (0.05%).\n",
      "Batch 16 is ok: 1 out of 2051 samples are faulty (0.05%).\n",
      "Batch 17 is ok: 7 out of 2051 samples are faulty (0.34%).\n",
      "Batch 18 is problematic: 79 out of 2051 samples are faulty (3.85%).\n",
      "Batch 19 is ok: 28 out of 2051 samples are faulty (1.37%).\n",
      "Batch 20 is problematic: 152 out of 2051 samples are faulty (7.41%).\n",
      "Batch 21 is problematic: 132 out of 2051 samples are faulty (6.44%).\n",
      "Batch 22 is problematic: 44 out of 2051 samples are faulty (2.15%).\n",
      "Batch 23 is ok: 18 out of 2051 samples are faulty (0.88%).\n",
      "Batch 24 is problematic: 84 out of 2051 samples are faulty (4.10%).\n",
      "Batch 25 is ok: 37 out of 2051 samples are faulty (1.80%).\n",
      "Batch 26 is problematic: 79 out of 2051 samples are faulty (3.85%).\n",
      "Batch 27 is problematic: 52 out of 2051 samples are faulty (2.54%).\n",
      "Batch 28 is problematic: 44 out of 2051 samples are faulty (2.15%).\n",
      "Batch 29 is problematic: 95 out of 2051 samples are faulty (4.63%).\n",
      "Batch 30 is problematic: 88 out of 2051 samples are faulty (4.29%).\n",
      "Batch 31 is problematic: 113 out of 2051 samples are faulty (5.51%).\n",
      "Batch 32 is problematic: 102 out of 2051 samples are faulty (4.97%).\n",
      "Batch 33 is ok: 22 out of 2051 samples are faulty (1.07%).\n",
      "Batch 34 is ok: 18 out of 2051 samples are faulty (0.88%).\n",
      "Batch 35 is ok: 9 out of 2051 samples are faulty (0.44%).\n",
      "Batch 36 is problematic: 153 out of 2051 samples are faulty (7.46%).\n",
      "Batch 37 is ok: 4 out of 2051 samples are faulty (0.20%).\n",
      "Batch 38 is ok: 7 out of 2051 samples are faulty (0.34%).\n",
      "Batch 39 is ok: 0 out of 2051 samples are faulty (0.00%).\n",
      "Batch 40 is ok: 2 out of 2051 samples are faulty (0.10%).\n",
      "Batch 41 is ok: 1 out of 2051 samples are faulty (0.05%).\n",
      "Batch 42 is ok: 4 out of 2051 samples are faulty (0.20%).\n",
      "Batch 43 is ok: 2 out of 2051 samples are faulty (0.10%).\n",
      "Batch 44 is ok: 7 out of 2051 samples are faulty (0.34%).\n",
      "Batch 45 is ok: 3 out of 2051 samples are faulty (0.15%).\n",
      "Batch 46 is ok: 4 out of 2051 samples are faulty (0.20%).\n",
      "Batch 47 is ok: 4 out of 2051 samples are faulty (0.20%).\n",
      "Batch 48 is ok: 5 out of 2051 samples are faulty (0.24%).\n",
      "Batch 49 is ok: 8 out of 2051 samples are faulty (0.39%).\n",
      "Total batches with issues: 13 out of 50\n",
      "Total problematic samples: 1449 out of 102598 (1.41%)\n",
      "Percentage of data quality issues detected in the test set: 1.41%\n"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "# from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# def detect_quality_issues(model, data_loader, threshold):\n",
    "#     model.eval()\n",
    "#     total_issue_count = 0\n",
    "#     total_batches_with_issues = 0\n",
    "#     total_samples = 0\n",
    "#     current_batch_issues = 0\n",
    "#     batch_count = 0\n",
    "#     batch_size = len(data_loader.dataset) // 50  # 你希望的批次大小\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for inputs, _ in data_loader:\n",
    "#             inputs = inputs.to(device)  # 将输入数据移动到正确的设备\n",
    "#             recon, mu, logvar = model(inputs)\n",
    "#             BCE, KLD = loss_function(recon, inputs, mu, logvar)\n",
    "#             total_loss = BCE + KLD  # 计算当前样本的总损失\n",
    "#             total_samples += 1\n",
    "\n",
    "#             # 判断当前样本是否有问题\n",
    "#             if total_loss.item() > threshold:\n",
    "#                 current_batch_issues += 1\n",
    "\n",
    "#             # 当累积样本数达到你设定的批次大小时，评估这个批次\n",
    "#             if total_samples % batch_size == 0:\n",
    "#                 if current_batch_issues >= batch_size * 0.02:  # 判断这个批次是否有超过5%的样本有问题\n",
    "#                     print(f\"Batch {batch_count} is problematic: {current_batch_issues} out of {batch_size} samples are faulty ({(current_batch_issues/batch_size * 100):.2f}%).\")\n",
    "#                     total_batches_with_issues += 1\n",
    "#                     total_issue_count = total_issue_count + current_batch_issues\n",
    "#                 else:\n",
    "#                     total_issue_count = total_issue_count + current_batch_issues\n",
    "#                     print(f\"Batch {batch_count} is ok: {current_batch_issues} out of {batch_size} samples are faulty ({(current_batch_issues/batch_size * 100):.2f}%).\")\n",
    "#                 current_batch_issues = 0\n",
    "#                 batch_count += 1\n",
    "\n",
    "#     total_issue_rate = total_issue_count / total_samples\n",
    "#     print(f\"Total batches with issues: {total_batches_with_issues} out of {batch_count}\")\n",
    "#     print(f\"Total problematic samples: {total_issue_count} out of {total_samples} ({(total_issue_rate * 100):.2f}%)\")\n",
    "#     return total_issue_rate\n",
    "\n",
    "# # Example usage\n",
    "\n",
    "\n",
    "\n",
    "# # 假设 data_dirty 已经是一个经过预处理的 DataFrame\n",
    "# data_dirty_array = data_dirty.values.astype(np.float32)  # 转换为浮点数类型的 NumPy 数组\n",
    "# test_dirty_tensor = torch.tensor(data_dirty_array)  \n",
    "# test_dirty_dataset = TensorDataset(test_dirty_tensor, test_dirty_tensor)\n",
    "# test_dirty_loader = DataLoader(test_dirty_dataset, batch_size=1, shuffle=False) \n",
    "\n",
    "# issue_rate = detect_quality_issues(model, test_dirty_loader, threshold)\n",
    "# print(f\"Percentage of data quality issues detected in the test set: {issue_rate * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8bc294",
   "metadata": {},
   "source": [
    "## 保存loss 到文件中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c690c9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import numpy as np\n",
    "# from torch.utils.data import DataLoader, TensorDataset\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# import pandas as pd\n",
    "\n",
    "# def record_losses(model, data_loader, seed):\n",
    "#     model.eval()\n",
    "#     losses = []\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         for inputs, _ in data_loader:\n",
    "#             inputs = inputs.to(device)\n",
    "#             recon, mu, logvar = model(inputs)\n",
    "#             BCE, KLD = loss_function(recon, inputs, mu, logvar)\n",
    "#             total_loss = BCE + KLD\n",
    "#             losses.append(total_loss.item())\n",
    "    \n",
    "#     # Save the losses to a CSV file\n",
    "#     losses_df = pd.DataFrame(losses, columns=['Loss_dirty_graph'])\n",
    "#     losses_df.to_csv(f'loss_data_dirty_graph.csv', index=False)\n",
    "#     print(f\"Loss data for random sample data_dirty_graph saved.\")\n",
    "\n",
    "# # 主函数：测试2000个随机样本\n",
    "# def test_random_sample(model, data_dirty):\n",
    "#     seed = 42  # Use a fixed seed for reproducibility\n",
    "#     _, sample_data = train_test_split(data_dirty, test_size=2000, train_size=None, random_state=seed)\n",
    "#     sample_data_array = sample_data.values.astype(np.float32)\n",
    "#     sample_tensor = torch.tensor(sample_data_array)\n",
    "#     sample_dataset = TensorDataset(sample_tensor, sample_tensor)\n",
    "#     test_loader = DataLoader(sample_dataset, batch_size=1, shuffle=False) \n",
    "#     record_losses(model, test_loader, seed)\n",
    "\n",
    "# # 假设 model, data_dirty, device, loss_function 已经被正确定义和设置\n",
    "# test_random_sample(model, data_dirty)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "652dc382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined loss data saved to /home/sdong/experiments/VAE_method/results/combined_loss_data_gragh.csv.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def record_losses(model, data_loader, seed, device, loss_function):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, _ in data_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            recon, mu, logvar = model(inputs)\n",
    "            loss = loss_function(recon, inputs, mu, logvar)\n",
    "            loss_per_sample = loss.sum(dim=1)\n",
    "            losses.extend(loss_per_sample.tolist())  \n",
    "    \n",
    "    return losses\n",
    "\n",
    "# 主函数：测试两个数据集并将结果保存到同一个CSV文件\n",
    "def test_and_save_combined_losses(model, data_clean, data_dirty, file_name='/home/sdong/experiments/VAE_method/results/combined_loss_data_gragh.csv'):\n",
    "    seed = 42  # Use a fixed seed for reproducibility\n",
    "    # Process clean data\n",
    "    _, sample_data_clean = train_test_split(data_clean, test_size=2000, train_size=None, random_state=seed)\n",
    "    #sample_clean_array = sample_data_clean.values.astype(np.float32)\n",
    "    sample_clean_tensor = torch.tensor(sample_data_clean)\n",
    "    sample_clean_dataset = TensorDataset(sample_clean_tensor, sample_clean_tensor)\n",
    "    clean_loader = DataLoader(sample_clean_dataset, batch_size=1024, shuffle=False)\n",
    "    clean_losses = record_losses(model, clean_loader, seed, device, loss_function)\n",
    "    \n",
    "    # Process dirty data\n",
    "    _, sample_data_dirty = train_test_split(data_dirty, test_size=2000, train_size=None, random_state=seed)\n",
    "    sample_dirty_array = sample_data_dirty.values.astype(np.float32)\n",
    "    sample_dirty_tensor = torch.tensor(sample_dirty_array)\n",
    "    sample_dirty_dataset = TensorDataset(sample_dirty_tensor, sample_dirty_tensor)\n",
    "    dirty_loader = DataLoader(sample_dirty_dataset, batch_size=1024, shuffle=False)\n",
    "    dirty_losses = record_losses(model, dirty_loader, seed, device, loss_function)\n",
    "    \n",
    "    # Combine and save to CSV\n",
    "    combined_df = pd.DataFrame({\n",
    "        'Loss_clean_graph': clean_losses,\n",
    "        'Loss_dirty_graph': dirty_losses\n",
    "    })\n",
    "    combined_df.to_csv(file_name, index=False)\n",
    "    print(f\"Combined loss data saved to {file_name}.\")\n",
    "\n",
    "# 假设 model, data_clean, data_dirty, device, loss_function 已经被正确定义和设置\n",
    "test_and_save_combined_losses(model, test_data, data_dirty)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95383854",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c828937",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5158c12f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e70407d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c585cc0c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "huggingface_learn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
