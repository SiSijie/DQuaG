{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b91df9ab-79c4-46e0-a477-ac62672be489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 102599 entries, 0 to 102598\n",
      "Data columns (total 23 columns):\n",
      " #   Column                          Non-Null Count   Dtype  \n",
      "---  ------                          --------------   -----  \n",
      " 0   id                              102599 non-null  int64  \n",
      " 1   name                            102349 non-null  object \n",
      " 2   host_id                         102599 non-null  int64  \n",
      " 3   host_identity_verified          102310 non-null  object \n",
      " 4   host_name                       102193 non-null  object \n",
      " 5   neighbourhood_group             102570 non-null  object \n",
      " 6   neighbourhood                   102583 non-null  object \n",
      " 7   lat                             102591 non-null  float64\n",
      " 8   long                            102591 non-null  float64\n",
      " 9   instant_bookable                102599 non-null  bool   \n",
      " 10  cancellation_policy             102523 non-null  object \n",
      " 11  room_type                       102599 non-null  object \n",
      " 12  construction_year               102385 non-null  float64\n",
      " 13  price                           102352 non-null  float64\n",
      " 14  service_fee                     102326 non-null  float64\n",
      " 15  minimum_nights                  102190 non-null  float64\n",
      " 16  number_of_reviews               102416 non-null  float64\n",
      " 17  last_review                     86706 non-null   object \n",
      " 18  reviews_per_month               86720 non-null   float64\n",
      " 19  review_rate_number              102273 non-null  float64\n",
      " 20  calculated_host_listings_count  102280 non-null  float64\n",
      " 21  availability_365                102151 non-null  float64\n",
      " 22  house_rules                     50468 non-null   object \n",
      "dtypes: bool(1), float64(11), int64(2), object(9)\n",
      "memory usage: 17.3+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# 加载数据\n",
    "file_path_clean = '/home/sdong/data/airbnb/airbnb_nyc_clean.csv'\n",
    "file_path_origi = '/home/sdong/data/airbnb/Airbnb_Open_Data_Alignement.csv'\n",
    "data = pd.read_csv(file_path_clean)\n",
    "data_dirty = pd.read_csv(file_path_origi)\n",
    "# 设置显示所有列和部分行\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 10)  \n",
    "# 显示数据集的前几行和数据结构\n",
    "\n",
    "print(data_dirty.info())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "077a3df8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>host_id</th>\n",
       "      <th>host_identity_verified</th>\n",
       "      <th>host_name</th>\n",
       "      <th>neighbourhood_group</th>\n",
       "      <th>neighbourhood</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>instant_bookable</th>\n",
       "      <th>cancellation_policy</th>\n",
       "      <th>room_type</th>\n",
       "      <th>construction_year</th>\n",
       "      <th>price</th>\n",
       "      <th>service_fee</th>\n",
       "      <th>minimum_nights</th>\n",
       "      <th>number_of_reviews</th>\n",
       "      <th>last_review</th>\n",
       "      <th>reviews_per_month</th>\n",
       "      <th>review_rate_number</th>\n",
       "      <th>calculated_host_listings_count</th>\n",
       "      <th>availability_365</th>\n",
       "      <th>house_rules</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1001254</td>\n",
       "      <td>Clean &amp; quiet apt home by the park</td>\n",
       "      <td>80014485718</td>\n",
       "      <td>unconfirmed</td>\n",
       "      <td>Madaline</td>\n",
       "      <td>Brooklyn</td>\n",
       "      <td>Kensington</td>\n",
       "      <td>40.64749</td>\n",
       "      <td>-73.97237</td>\n",
       "      <td>False</td>\n",
       "      <td>strict</td>\n",
       "      <td>Private room</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>966.0</td>\n",
       "      <td>193.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2021-10-19</td>\n",
       "      <td>0.21</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>286.0</td>\n",
       "      <td>Clean up and treat the home the way you'd like...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1002102</td>\n",
       "      <td>Skylit Midtown Castle</td>\n",
       "      <td>52335172823</td>\n",
       "      <td>verified</td>\n",
       "      <td>Jenna</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>Midtown</td>\n",
       "      <td>40.75362</td>\n",
       "      <td>-73.98377</td>\n",
       "      <td>False</td>\n",
       "      <td>moderate</td>\n",
       "      <td>Entire home/apt</td>\n",
       "      <td>2007.0</td>\n",
       "      <td>142.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>2022-05-21</td>\n",
       "      <td>0.38</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>228.0</td>\n",
       "      <td>Pet friendly but please confirm with me if the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1002403</td>\n",
       "      <td>THE VILLAGE OF HARLEM....NEW YORK !</td>\n",
       "      <td>78829239556</td>\n",
       "      <td>unconfirmed</td>\n",
       "      <td>Elise</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>Harlem</td>\n",
       "      <td>40.80902</td>\n",
       "      <td>-73.94190</td>\n",
       "      <td>True</td>\n",
       "      <td>flexible</td>\n",
       "      <td>Private room</td>\n",
       "      <td>2005.0</td>\n",
       "      <td>620.0</td>\n",
       "      <td>124.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2019-06-14</td>\n",
       "      <td>0.79</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>352.0</td>\n",
       "      <td>I encourage you to use my kitchen, cooking and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1002755</td>\n",
       "      <td>blank</td>\n",
       "      <td>85098326012</td>\n",
       "      <td>unconfirmed</td>\n",
       "      <td>Garry</td>\n",
       "      <td>Brooklyn</td>\n",
       "      <td>Clinton Hill</td>\n",
       "      <td>40.68514</td>\n",
       "      <td>-73.95976</td>\n",
       "      <td>True</td>\n",
       "      <td>moderate</td>\n",
       "      <td>Entire home/apt</td>\n",
       "      <td>2005.0</td>\n",
       "      <td>368.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>270.0</td>\n",
       "      <td>2019-07-05</td>\n",
       "      <td>4.64</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>322.0</td>\n",
       "      <td>blank</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1003689</td>\n",
       "      <td>Entire Apt: Spacious Studio/Loft by central park</td>\n",
       "      <td>92037596077</td>\n",
       "      <td>verified</td>\n",
       "      <td>Lyndon</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>East Harlem</td>\n",
       "      <td>40.79851</td>\n",
       "      <td>-73.94399</td>\n",
       "      <td>False</td>\n",
       "      <td>moderate</td>\n",
       "      <td>Entire home/apt</td>\n",
       "      <td>2009.0</td>\n",
       "      <td>204.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2018-11-19</td>\n",
       "      <td>0.10</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>289.0</td>\n",
       "      <td>Please no smoking in the house, porch or on th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                              name      host_id  \\\n",
       "0  1001254                Clean & quiet apt home by the park  80014485718   \n",
       "1  1002102                             Skylit Midtown Castle  52335172823   \n",
       "2  1002403               THE VILLAGE OF HARLEM....NEW YORK !  78829239556   \n",
       "3  1002755                                             blank  85098326012   \n",
       "4  1003689  Entire Apt: Spacious Studio/Loft by central park  92037596077   \n",
       "\n",
       "  host_identity_verified host_name neighbourhood_group neighbourhood  \\\n",
       "0            unconfirmed  Madaline            Brooklyn    Kensington   \n",
       "1               verified     Jenna           Manhattan       Midtown   \n",
       "2            unconfirmed     Elise           Manhattan        Harlem   \n",
       "3            unconfirmed     Garry            Brooklyn  Clinton Hill   \n",
       "4               verified    Lyndon           Manhattan   East Harlem   \n",
       "\n",
       "        lat      long  instant_bookable cancellation_policy        room_type  \\\n",
       "0  40.64749 -73.97237             False              strict     Private room   \n",
       "1  40.75362 -73.98377             False            moderate  Entire home/apt   \n",
       "2  40.80902 -73.94190              True            flexible     Private room   \n",
       "3  40.68514 -73.95976              True            moderate  Entire home/apt   \n",
       "4  40.79851 -73.94399             False            moderate  Entire home/apt   \n",
       "\n",
       "   construction_year  price  service_fee  minimum_nights  number_of_reviews  \\\n",
       "0             2020.0  966.0        193.0            10.0                9.0   \n",
       "1             2007.0  142.0         28.0            13.0               45.0   \n",
       "2             2005.0  620.0        124.0             3.0                0.0   \n",
       "3             2005.0  368.0         74.0            13.0              270.0   \n",
       "4             2009.0  204.0         41.0            10.0                9.0   \n",
       "\n",
       "  last_review  reviews_per_month  review_rate_number  \\\n",
       "0  2021-10-19               0.21                 4.0   \n",
       "1  2022-05-21               0.38                 4.0   \n",
       "2  2019-06-14               0.79                 5.0   \n",
       "3  2019-07-05               4.64                 4.0   \n",
       "4  2018-11-19               0.10                 3.0   \n",
       "\n",
       "   calculated_host_listings_count  availability_365  \\\n",
       "0                             6.0             286.0   \n",
       "1                             2.0             228.0   \n",
       "2                             1.0             352.0   \n",
       "3                             1.0             322.0   \n",
       "4                             1.0             289.0   \n",
       "\n",
       "                                         house_rules  \n",
       "0  Clean up and treat the home the way you'd like...  \n",
       "1  Pet friendly but please confirm with me if the...  \n",
       "2  I encourage you to use my kitchen, cooking and...  \n",
       "3                                              blank  \n",
       "4  Please no smoking in the house, porch or on th...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a56b53dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['instant_bookable'] = data['instant_bookable'].astype(int)\n",
    "\n",
    "# 更新分类特征和数值特征的列表\n",
    "categorical_cols = ['host_identity_verified', 'neighbourhood_group', 'neighbourhood','name','host_name', \n",
    "                     'room_type', 'cancellation_policy', 'last_review', 'house_rules']\n",
    "\n",
    "numeric_cols = ['lat', 'long', 'construction_year', 'price', 'service_fee', \n",
    "                 'minimum_nights', 'number_of_reviews', 'reviews_per_month', \n",
    "                 'review_rate_number', 'calculated_host_listings_count', 'availability_365','house_rules','name','host_name','last_review','id','host_id', 'host_identity_verified','neighbourhood_group','neighbourhood','instant_bookable','cancellation_policy','room_type']\n",
    "\n",
    "\n",
    "# 使用Label Encoder处理分类数据\n",
    "for col in categorical_cols:\n",
    "    data[col] = LabelEncoder().fit_transform(data[col].astype(str))\n",
    "\n",
    "# # 使用StandardScaler处理数值型数据\n",
    "# data[numeric_cols] = StandardScaler().fit_transform(data[numeric_cols])\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# 假设 data 是你的数据框架，numeric_cols 是你想要标准化的数值型列的列表\n",
    "scaler = MinMaxScaler()\n",
    "data[numeric_cols] = scaler.fit_transform(data[numeric_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ac07562",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用 fillna() 方法替换所有的 NaN 值为 0\n",
    "data_dirty.fillna(0, inplace=True)\n",
    "# 处理布尔型数据：转换为整型\n",
    "data_dirty['instant_bookable'] = data_dirty['instant_bookable'].astype(int)\n",
    "\n",
    "\n",
    "# 使用Label Encoder处理分类数据\n",
    "for col in categorical_cols:\n",
    "    data_dirty[col] = LabelEncoder().fit_transform(data_dirty[col].astype(str))\n",
    "\n",
    "# 假设 data 是你的数据框架，numeric_cols 是你想要标准化的数值型列的列表\n",
    "scaler = MinMaxScaler()\n",
    "data_dirty[numeric_cols] = scaler.fit_transform(data_dirty[numeric_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8a628d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>host_id</th>\n",
       "      <th>host_identity_verified</th>\n",
       "      <th>host_name</th>\n",
       "      <th>neighbourhood_group</th>\n",
       "      <th>neighbourhood</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>instant_bookable</th>\n",
       "      <th>cancellation_policy</th>\n",
       "      <th>room_type</th>\n",
       "      <th>construction_year</th>\n",
       "      <th>price</th>\n",
       "      <th>service_fee</th>\n",
       "      <th>minimum_nights</th>\n",
       "      <th>number_of_reviews</th>\n",
       "      <th>last_review</th>\n",
       "      <th>reviews_per_month</th>\n",
       "      <th>review_rate_number</th>\n",
       "      <th>calculated_host_listings_count</th>\n",
       "      <th>availability_365</th>\n",
       "      <th>house_rules</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.257476</td>\n",
       "      <td>0.809915</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.562851</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.516807</td>\n",
       "      <td>0.354044</td>\n",
       "      <td>0.509474</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.894737</td>\n",
       "      <td>0.796522</td>\n",
       "      <td>0.795652</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.008789</td>\n",
       "      <td>0.943343</td>\n",
       "      <td>0.002222</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.015106</td>\n",
       "      <td>0.678899</td>\n",
       "      <td>0.224696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.782029</td>\n",
       "      <td>0.529285</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.416300</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.605042</td>\n",
       "      <td>0.608442</td>\n",
       "      <td>0.488542</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.210526</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>0.078261</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.043945</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.004112</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.003021</td>\n",
       "      <td>0.545872</td>\n",
       "      <td>0.616397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.903287</td>\n",
       "      <td>0.797898</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.252995</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.453782</td>\n",
       "      <td>0.741239</td>\n",
       "      <td>0.565422</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>0.495652</td>\n",
       "      <td>0.495652</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.696884</td>\n",
       "      <td>0.008668</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.830275</td>\n",
       "      <td>0.295040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.977006</td>\n",
       "      <td>0.861458</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.307961</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.197479</td>\n",
       "      <td>0.444293</td>\n",
       "      <td>0.532628</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>0.276522</td>\n",
       "      <td>0.278261</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.263672</td>\n",
       "      <td>0.705382</td>\n",
       "      <td>0.051450</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.761468</td>\n",
       "      <td>0.975709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.387095</td>\n",
       "      <td>0.931812</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.558074</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.294118</td>\n",
       "      <td>0.716046</td>\n",
       "      <td>0.561584</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.315789</td>\n",
       "      <td>0.133913</td>\n",
       "      <td>0.134783</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.008789</td>\n",
       "      <td>0.613112</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.685780</td>\n",
       "      <td>0.692308</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id      name   host_id  host_identity_verified  host_name  \\\n",
       "0  0.000000  0.257476  0.809915                     0.0   0.562851   \n",
       "1  0.000015  0.782029  0.529285                     1.0   0.416300   \n",
       "2  0.000020  0.903287  0.797898                     0.0   0.252995   \n",
       "3  0.000027  0.977006  0.861458                     0.0   0.307961   \n",
       "4  0.000043  0.387095  0.931812                     1.0   0.558074   \n",
       "\n",
       "   neighbourhood_group  neighbourhood       lat      long  instant_bookable  \\\n",
       "0                 0.25       0.516807  0.354044  0.509474               0.0   \n",
       "1                 0.50       0.605042  0.608442  0.488542               0.0   \n",
       "2                 0.50       0.453782  0.741239  0.565422               1.0   \n",
       "3                 0.25       0.197479  0.444293  0.532628               1.0   \n",
       "4                 0.50       0.294118  0.716046  0.561584               0.0   \n",
       "\n",
       "   cancellation_policy  room_type  construction_year     price  service_fee  \\\n",
       "0                  1.0   0.666667           0.894737  0.796522     0.795652   \n",
       "1                  0.5   0.000000           0.210526  0.080000     0.078261   \n",
       "2                  0.0   0.666667           0.105263  0.495652     0.495652   \n",
       "3                  0.5   0.000000           0.105263  0.276522     0.278261   \n",
       "4                  0.5   0.000000           0.315789  0.133913     0.134783   \n",
       "\n",
       "   minimum_nights  number_of_reviews  last_review  reviews_per_month  \\\n",
       "0        0.769231           0.008789     0.943343           0.002222   \n",
       "1        1.000000           0.043945     1.000000           0.004112   \n",
       "2        0.230769           0.000000     0.696884           0.008668   \n",
       "3        1.000000           0.263672     0.705382           0.051450   \n",
       "4        0.769231           0.008789     0.613112           0.001000   \n",
       "\n",
       "   review_rate_number  calculated_host_listings_count  availability_365  \\\n",
       "0                0.75                        0.015106          0.678899   \n",
       "1                0.75                        0.003021          0.545872   \n",
       "2                1.00                        0.000000          0.830275   \n",
       "3                0.75                        0.000000          0.761468   \n",
       "4                0.50                        0.000000          0.685780   \n",
       "\n",
       "   house_rules  \n",
       "0     0.224696  \n",
       "1     0.616397  \n",
       "2     0.295040  \n",
       "3     0.975709  \n",
       "4     0.692308  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "167f66e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>host_id</th>\n",
       "      <th>host_identity_verified</th>\n",
       "      <th>host_name</th>\n",
       "      <th>neighbourhood_group</th>\n",
       "      <th>neighbourhood</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>instant_bookable</th>\n",
       "      <th>cancellation_policy</th>\n",
       "      <th>room_type</th>\n",
       "      <th>construction_year</th>\n",
       "      <th>price</th>\n",
       "      <th>service_fee</th>\n",
       "      <th>minimum_nights</th>\n",
       "      <th>number_of_reviews</th>\n",
       "      <th>last_review</th>\n",
       "      <th>reviews_per_month</th>\n",
       "      <th>review_rate_number</th>\n",
       "      <th>calculated_host_listings_count</th>\n",
       "      <th>availability_365</th>\n",
       "      <th>house_rules</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.257453</td>\n",
       "      <td>0.809928</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.562926</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.993414</td>\n",
       "      <td>0.003737</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.999011</td>\n",
       "      <td>0.805000</td>\n",
       "      <td>0.804167</td>\n",
       "      <td>0.179528</td>\n",
       "      <td>0.008789</td>\n",
       "      <td>0.114251</td>\n",
       "      <td>0.002333</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.018072</td>\n",
       "      <td>0.080282</td>\n",
       "      <td>0.225202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.781988</td>\n",
       "      <td>0.529317</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.416376</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.584821</td>\n",
       "      <td>0.996008</td>\n",
       "      <td>0.003583</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.992582</td>\n",
       "      <td>0.118333</td>\n",
       "      <td>0.116667</td>\n",
       "      <td>0.182440</td>\n",
       "      <td>0.043945</td>\n",
       "      <td>0.606782</td>\n",
       "      <td>0.004222</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.006024</td>\n",
       "      <td>0.064551</td>\n",
       "      <td>0.616903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.903282</td>\n",
       "      <td>0.797912</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.253071</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>0.997362</td>\n",
       "      <td>0.004147</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.991592</td>\n",
       "      <td>0.516667</td>\n",
       "      <td>0.516667</td>\n",
       "      <td>0.178509</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.003012</td>\n",
       "      <td>0.098183</td>\n",
       "      <td>0.295547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.010574</td>\n",
       "      <td>0.861467</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.308036</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.191964</td>\n",
       "      <td>0.994334</td>\n",
       "      <td>0.003907</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.991592</td>\n",
       "      <td>0.306667</td>\n",
       "      <td>0.308333</td>\n",
       "      <td>0.182440</td>\n",
       "      <td>0.263672</td>\n",
       "      <td>0.817117</td>\n",
       "      <td>0.051556</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.003012</td>\n",
       "      <td>0.090046</td>\n",
       "      <td>0.104251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.387135</td>\n",
       "      <td>0.931817</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.558150</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.281250</td>\n",
       "      <td>0.997105</td>\n",
       "      <td>0.004119</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.993571</td>\n",
       "      <td>0.170000</td>\n",
       "      <td>0.170833</td>\n",
       "      <td>0.179528</td>\n",
       "      <td>0.008789</td>\n",
       "      <td>0.196205</td>\n",
       "      <td>0.001111</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.003012</td>\n",
       "      <td>0.081096</td>\n",
       "      <td>0.692814</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id      name   host_id  host_identity_verified  host_name  \\\n",
       "0  0.000000  0.257453  0.809928                     0.5   0.562926   \n",
       "1  0.000015  0.781988  0.529317                     1.0   0.416376   \n",
       "2  0.000020  0.903282  0.797912                     0.0   0.253071   \n",
       "3  0.000027  0.010574  0.861467                     0.5   0.308036   \n",
       "4  0.000043  0.387135  0.931817                     1.0   0.558150   \n",
       "\n",
       "   neighbourhood_group  neighbourhood       lat      long  instant_bookable  \\\n",
       "0             0.285714       0.500000  0.993414  0.003737               0.0   \n",
       "1             0.428571       0.584821  0.996008  0.003583               0.0   \n",
       "2             0.428571       0.437500  0.997362  0.004147               1.0   \n",
       "3             0.285714       0.191964  0.994334  0.003907               1.0   \n",
       "4             0.428571       0.281250  0.997105  0.004119               0.0   \n",
       "\n",
       "   cancellation_policy  room_type  construction_year     price  service_fee  \\\n",
       "0             1.000000   0.666667           0.999011  0.805000     0.804167   \n",
       "1             0.666667   0.000000           0.992582  0.118333     0.116667   \n",
       "2             0.333333   0.666667           0.991592  0.516667     0.516667   \n",
       "3             0.666667   0.000000           0.991592  0.306667     0.308333   \n",
       "4             0.666667   0.000000           0.993571  0.170000     0.170833   \n",
       "\n",
       "   minimum_nights  number_of_reviews  last_review  reviews_per_month  \\\n",
       "0        0.179528           0.008789     0.114251           0.002333   \n",
       "1        0.182440           0.043945     0.606782           0.004222   \n",
       "2        0.178509           0.000000     0.000000           0.000000   \n",
       "3        0.182440           0.263672     0.817117           0.051556   \n",
       "4        0.179528           0.008789     0.196205           0.001111   \n",
       "\n",
       "   review_rate_number  calculated_host_listings_count  availability_365  \\\n",
       "0                 0.8                        0.018072          0.080282   \n",
       "1                 0.8                        0.006024          0.064551   \n",
       "2                 1.0                        0.003012          0.098183   \n",
       "3                 0.8                        0.003012          0.090046   \n",
       "4                 0.6                        0.003012          0.081096   \n",
       "\n",
       "   house_rules  \n",
       "0     0.225202  \n",
       "1     0.616903  \n",
       "2     0.295547  \n",
       "3     0.104251  \n",
       "4     0.692814  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dirty.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14966a57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 69305 entries, 0 to 69304\n",
      "Data columns (total 23 columns):\n",
      " #   Column                          Non-Null Count  Dtype  \n",
      "---  ------                          --------------  -----  \n",
      " 0   id                              69305 non-null  float64\n",
      " 1   name                            69305 non-null  float64\n",
      " 2   host_id                         69305 non-null  float64\n",
      " 3   host_identity_verified          69305 non-null  float64\n",
      " 4   host_name                       69305 non-null  float64\n",
      " 5   neighbourhood_group             69305 non-null  float64\n",
      " 6   neighbourhood                   69305 non-null  float64\n",
      " 7   lat                             69305 non-null  float64\n",
      " 8   long                            69305 non-null  float64\n",
      " 9   instant_bookable                69305 non-null  float64\n",
      " 10  cancellation_policy             69305 non-null  float64\n",
      " 11  room_type                       69305 non-null  float64\n",
      " 12  construction_year               69305 non-null  float64\n",
      " 13  price                           69305 non-null  float64\n",
      " 14  service_fee                     69305 non-null  float64\n",
      " 15  minimum_nights                  69305 non-null  float64\n",
      " 16  number_of_reviews               69305 non-null  float64\n",
      " 17  last_review                     69305 non-null  float64\n",
      " 18  reviews_per_month               69305 non-null  float64\n",
      " 19  review_rate_number              69305 non-null  float64\n",
      " 20  calculated_host_listings_count  69305 non-null  float64\n",
      " 21  availability_365                69305 non-null  float64\n",
      " 22  house_rules                     69305 non-null  float64\n",
      "dtypes: float64(23)\n",
      "memory usage: 12.2 MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5887b650",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 假设 data 已经是一个经过预处理的 DataFrame\n",
    "data_array = data.values.astype(np.float32)  # 转换为浮点数类型的 NumPy 数组\n",
    "\n",
    "# 分割数据为训练集和临时测试集（包括真正的测试集和验证集）\n",
    "train_data, val_test_data = train_test_split(data_array, test_size=0.5, random_state=42)\n",
    "\n",
    "# 将训练验证集进一步分割为训练集和验证集\n",
    "val_data, test_data = train_test_split(val_test_data, test_size=0.5, random_state=42)  # 0.25 x 0.8 = 0.2\n",
    "\n",
    "\n",
    "# 转换为PyTorch张量\n",
    "\n",
    "\n",
    "# 创建数据加载器\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "batch_size = 32  # 或者任何适合你GPU的大小\n",
    "\n",
    "train_tensor = torch.tensor(train_data) #0.6\n",
    "train_dataset = TensorDataset(train_tensor, train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "val_tensor = torch.tensor(val_data) #0.2\n",
    "val_dataset = TensorDataset(val_tensor, val_tensor)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "test_tensor = torch.tensor(test_data)  #20%\n",
    "test_dataset = TensorDataset(test_tensor, test_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)  # 50个批次\n",
    "#len(test_dataset)// 50\n",
    "\n",
    "data_dirty_array = data_dirty.values.astype(np.float32)  # 转换为浮点数类型的 NumPy 数组\n",
    "test_dirty_tensor = torch.tensor(data_dirty_array)  #20%\n",
    "test_dirty_dataset = TensorDataset(test_dirty_tensor, test_dirty_tensor)\n",
    "test_dirty_loader = DataLoader(test_dirty_dataset, batch_size=1, shuffle=False)  # 50个批次\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3d31c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "92722c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 检查数据中是否有NaN或无穷大的值\n",
    "# if torch.isnan(train_tensor).any() or torch.isinf(train_tensor).any():\n",
    "#     print(\"Data contains NaNs or Infs.\")\n",
    "# # 检查数据中是否有NaN或无穷大的值\n",
    "# if torch.isnan(test_dirty_tensor).any() or torch.isinf(test_dirty_tensor).any():\n",
    "#     print(\"Data contains NaNs or Infs.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "63a8f2af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAE(\n",
      "  (fc1): Linear(in_features=23, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc31): Linear(in_features=64, out_features=20, bias=True)\n",
      "  (fc32): Linear(in_features=64, out_features=20, bias=True)\n",
      "  (fc4): Linear(in_features=20, out_features=64, bias=True)\n",
      "  (fc5): Linear(in_features=64, out_features=128, bias=True)\n",
      "  (fc6): Linear(in_features=128, out_features=23, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "        # Encoder\n",
    "        self.fc1 = nn.Linear(23, 128)  # Input layer\n",
    "        self.fc2 = nn.Linear(128, 64)  # Hidden layer\n",
    "        self.fc31 = nn.Linear(64, 20)  # Output layer for mu\n",
    "        self.fc32 = nn.Linear(64, 20)  # Output layer for logvar\n",
    "\n",
    "        # Decoder\n",
    "        self.fc4 = nn.Linear(20, 64)   # Input layer\n",
    "        self.fc5 = nn.Linear(64, 128)  # Hidden layer\n",
    "        self.fc6 = nn.Linear(128, 23)  # Output layer\n",
    "\n",
    "    def encode(self, x):\n",
    "        h1 = F.relu(self.fc1(x))\n",
    "        h2 = F.relu(self.fc2(h1))\n",
    "        return self.fc31(h2), self.fc32(h2)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar) + 1e-8  # Adding a small constant for numerical stability\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z):\n",
    "        h3 = F.relu(self.fc4(z))\n",
    "        h4 = F.relu(self.fc5(h3))\n",
    "        return torch.sigmoid(self.fc6(h4))  # Use sigmoid to ensure output is between 0 and 1\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n",
    "\n",
    "# Instantiate the model\n",
    "model = VAE()\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4db9f9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "\n",
    "\n",
    "# # 定义VAE的架构\n",
    "# class VAE(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(VAE, self).__init__()\n",
    "#         self.fc1 = nn.Linear(23, 12)  # 假设有23个特征\n",
    "#         self.fc21 = nn.Linear(12, 6)  # 均值输出\n",
    "#         self.fc22 = nn.Linear(12, 6)  # 方差输出\n",
    "#         self.fc3 = nn.Linear(6, 12)\n",
    "#         self.fc4 = nn.Linear(12, 23)\n",
    "\n",
    "#     def encode(self, x):\n",
    "#         h1 = F.relu(self.fc1(x))\n",
    "#         return self.fc21(h1), self.fc22(h1)\n",
    "\n",
    "#     def reparameterize(self, mu, logvar):\n",
    "#         std = torch.exp(0.5 * logvar) + 1e-8  # 添加一个小常数以提高数值稳定性\n",
    "#         eps = torch.randn_like(std)\n",
    "#         return mu + eps*std\n",
    "\n",
    "#     def decode(self, z):\n",
    "#         h3 = F.relu(self.fc3(z))\n",
    "#         return torch.sigmoid(self.fc4(h3))\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         mu, logvar = self.encode(x.view(-1, 23))\n",
    "#         z = self.reparameterize(mu, logvar)\n",
    "#         return self.decode(z), mu, logvar\n",
    "\n",
    "# # 实例化模型\n",
    "# model = VAE()\n",
    "# print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c1d03c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#device = torch.device(\"cpu\")\n",
    "model = VAE().to(device)\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "# 设置优化器\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# 定义损失函数\n",
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    # 确保目标张量也是浮点类型且维度匹配\n",
    "    recon_x = torch.clamp(recon_x, 0, 1)  # 确保输出值在[0, 1]范围内\n",
    "    BCE = F.binary_cross_entropy(recon_x, x.view(-1, 23).float(), reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return BCE, KLD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c4a9e966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/34652 (0%)]\tLoss: 16.028532\n",
      "Train Epoch: 1 [3200/34652 (9%)]\tLoss: 13.461390\n",
      "Train Epoch: 1 [6400/34652 (18%)]\tLoss: 13.834731\n",
      "Train Epoch: 1 [9600/34652 (28%)]\tLoss: 13.664021\n",
      "Train Epoch: 1 [12800/34652 (37%)]\tLoss: 13.732774\n",
      "Train Epoch: 1 [16000/34652 (46%)]\tLoss: 13.865624\n",
      "Train Epoch: 1 [19200/34652 (55%)]\tLoss: 13.698837\n",
      "Train Epoch: 1 [22400/34652 (65%)]\tLoss: 13.609385\n",
      "Train Epoch: 1 [25600/34652 (74%)]\tLoss: 13.836342\n",
      "Train Epoch: 1 [28800/34652 (83%)]\tLoss: 13.585758\n",
      "Train Epoch: 1 [32000/34652 (92%)]\tLoss: 13.718147\n",
      "Epoch: 1 Average BCE: 13.794372179898188 Average KLD: 0.0056142972264721395 Total Loss: 13.799986467534286\n",
      "Model saved to vae_model.pth\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# 定义训练函数\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    #train_loss = 0\n",
    "    total_BCE = 0\n",
    "    total_KLD = 0\n",
    "    total_loss = 0\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):  # 由于使用TensorDataset，数据被重复用作输入和标签\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        if (recon_batch < 0).any() or (recon_batch > 1).any():\n",
    "            print(\"Warning: recon_batch contains values out of range [0, 1]\")\n",
    "        BCE, KLD = loss_function(recon_batch, data, mu, logvar)\n",
    "        loss = BCE + KLD\n",
    "        loss.backward()\n",
    "        total_loss += loss.item()\n",
    "        total_BCE += BCE.item()\n",
    "        total_KLD += KLD.item()\n",
    "        optimizer.step()\n",
    "        # if epoch == 1:  # 只在第一个epoch检查\n",
    "        #     print(\"Sample recon_x:\", recon_batch[0].data)\n",
    "        #     print(\"Sample x:\", data[0].data)\n",
    "\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item() / len(data)))\n",
    "    #print('====> Epoch: {} Average loss: {:.4f}'.format(epoch, train_loss / len(train_loader.dataset)))\n",
    "    print(f'Epoch: {epoch} Average BCE: {total_BCE / len(train_loader.dataset)} Average KLD: {total_KLD / len(train_loader.dataset)} Total Loss: {total_loss / len(train_loader.dataset)}')\n",
    "\n",
    "# 训练模型\n",
    "num_epochs = 1  # 可根据需要调整\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    train(epoch)\n",
    "\n",
    "# 保存模型的状态字典\n",
    "torch.save(model.state_dict(), 'vae_model.pth')\n",
    "\n",
    "print(\"Model saved to vae_model.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5b6b9ddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17326\n",
      "17327\n",
      "102599\n",
      "Average loss on validation data: 13.749168468762885\n",
      "Average loss on test data: 13.750350388210823\n",
      "Average loss on test dirty data: 14.344290650656937\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, data_loader):\n",
    "    model.eval()  # 切换到评估模式\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():  # 关闭梯度计算\n",
    "        for inputs, _ in data_loader:  # 假设 data_loader 返回 inputs 和 targets，这里我们不需要 targets\n",
    "            inputs = inputs.to(device)  # 确保将 inputs 转移到正确的设备\n",
    "            recon, mu, logvar = model(inputs)\n",
    "            BCE, KLD = loss_function(recon, inputs, mu, logvar)\n",
    "            loss = BCE + KLD  # 将损失元组中的元素相加\n",
    "            total_loss += loss.item()  # 现在这是一个单一的数值\n",
    "    print(len(data_loader.dataset))\n",
    "    return total_loss / len(data_loader.dataset)\n",
    "\n",
    "\n",
    "model = VAE().to(device)\n",
    "model.load_state_dict(torch.load('vae_model.pth'))\n",
    "# 计算测试集上的平均损失\n",
    "# 计算测试集和验证集上的平均损失\n",
    "val_loss = evaluate_model(model, val_loader)\n",
    "test_loss = evaluate_model(model, test_loader)\n",
    "test_dirty_loss = evaluate_model(model, test_dirty_loader)\n",
    "print(f\"Average loss on validation data: {val_loss}\")\n",
    "print(f\"Average loss on test data: {test_loss}\")\n",
    "print(f\"Average loss on test dirty data: {test_dirty_loss}\")\n",
    "# 简单的基于阈值的数据质量问题判断\n",
    "# 这里我们需要设置一个阈值来决定什么样的重构误差被认为是“异常”的，此阈值可以基于训练集或验证集的性能来确定\n",
    "# 假设我们根据验证集确定阈值\n",
    "# threshold = np.quantile([loss_function(model(recon, data.to(device), mu, logvar).item() for data, _ in val_loader], 0.95)\n",
    "# print(f\"Loss threshold for detecting data quality issues: {threshold}\")\n",
    "\n",
    "# # 判断测试集\n",
    "# quality_issues = test_loss > threshold\n",
    "# print(f\"Data quality issues detected: {quality_issues}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8950bc94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss threshold for detecting data quality issues: 16.067871570587158\n",
      "Min validation error: 12.020641326904297\n",
      "Max validation error: 18.95651626586914\n",
      "Mean validation error: 13.749729047454037\n",
      "95th percentile of validation errors: 14.852011919021606\n",
      "Maximum validation error (100th percentile): 18.95651626586914\n"
     ]
    }
   ],
   "source": [
    "def collect_reconstruction_errors(model, data_loader):\n",
    "    model.eval()\n",
    "    reconstruction_errors = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, _ in data_loader:  # 假设 data_loader 返回的是 inputs 和 labels，这里我们忽略 labels\n",
    "            inputs = inputs.to(device)  # 将输入数据移动到正确的设备\n",
    "            recon, mu, logvar = model(inputs)\n",
    "            BCE, KLD = loss_function(recon, inputs, mu, logvar)\n",
    "            total_loss = BCE + KLD  # 计算总损失\n",
    "            average_loss = total_loss.item() / inputs.size(0)  # 计算平均损失\n",
    "            reconstruction_errors.append(average_loss)  # 添加单个损失值到列表中\n",
    "    return reconstruction_errors\n",
    "\n",
    "# 收集验证集的重构误差\n",
    "val_errors = collect_reconstruction_errors(model, val_loader)\n",
    "threshold = np.quantile(val_errors, 0.99)  # 计算95%分位数作为阈值\n",
    "threshold = threshold * 1\n",
    "print(f\"Loss threshold for detecting data quality issues: {threshold}\")\n",
    "\n",
    "min_val_error = min(val_errors)\n",
    "max_val_error = max(val_errors)\n",
    "mean_val_error = sum(val_errors) / len(val_errors)\n",
    "print(f\"Min validation error: {min_val_error}\")\n",
    "print(f\"Max validation error: {max_val_error}\")\n",
    "print(f\"Mean validation error: {mean_val_error}\")\n",
    "print(f\"95th percentile of validation errors: {np.quantile(val_errors, 0.95)}\")\n",
    "print(f\"Maximum validation error (100th percentile): {np.quantile(val_errors, 1)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b7b267a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0 is ok: 19 out of 346 samples are faulty (5.49%).\n",
      "Batch 1 is ok: 9 out of 346 samples are faulty (2.60%).\n",
      "Batch 2 is ok: 11 out of 346 samples are faulty (3.18%).\n",
      "Batch 3 is ok: 22 out of 346 samples are faulty (6.36%).\n",
      "Batch 4 is ok: 14 out of 346 samples are faulty (4.05%).\n",
      "Batch 5 is ok: 15 out of 346 samples are faulty (4.34%).\n",
      "Batch 6 is ok: 27 out of 346 samples are faulty (7.80%).\n",
      "Batch 7 is ok: 16 out of 346 samples are faulty (4.62%).\n",
      "Batch 8 is ok: 21 out of 346 samples are faulty (6.07%).\n",
      "Batch 9 is ok: 17 out of 346 samples are faulty (4.91%).\n",
      "Batch 10 is ok: 15 out of 346 samples are faulty (4.34%).\n",
      "Batch 11 is ok: 19 out of 346 samples are faulty (5.49%).\n",
      "Batch 12 is ok: 19 out of 346 samples are faulty (5.49%).\n",
      "Batch 13 is ok: 19 out of 346 samples are faulty (5.49%).\n",
      "Batch 14 is ok: 13 out of 346 samples are faulty (3.76%).\n",
      "Batch 15 is ok: 14 out of 346 samples are faulty (4.05%).\n",
      "Batch 16 is ok: 18 out of 346 samples are faulty (5.20%).\n",
      "Batch 17 is ok: 23 out of 346 samples are faulty (6.65%).\n",
      "Batch 18 is ok: 11 out of 346 samples are faulty (3.18%).\n",
      "Batch 19 is ok: 16 out of 346 samples are faulty (4.62%).\n",
      "Batch 20 is ok: 20 out of 346 samples are faulty (5.78%).\n",
      "Batch 21 is ok: 15 out of 346 samples are faulty (4.34%).\n",
      "Batch 22 is ok: 15 out of 346 samples are faulty (4.34%).\n",
      "Batch 23 is ok: 12 out of 346 samples are faulty (3.47%).\n",
      "Batch 24 is ok: 21 out of 346 samples are faulty (6.07%).\n",
      "Batch 25 is ok: 14 out of 346 samples are faulty (4.05%).\n",
      "Batch 26 is ok: 17 out of 346 samples are faulty (4.91%).\n",
      "Batch 27 is ok: 21 out of 346 samples are faulty (6.07%).\n",
      "Batch 28 is ok: 18 out of 346 samples are faulty (5.20%).\n",
      "Batch 29 is ok: 14 out of 346 samples are faulty (4.05%).\n",
      "Batch 30 is ok: 9 out of 346 samples are faulty (2.60%).\n",
      "Batch 31 is ok: 12 out of 346 samples are faulty (3.47%).\n",
      "Batch 32 is ok: 26 out of 346 samples are faulty (7.51%).\n",
      "Batch 33 is ok: 20 out of 346 samples are faulty (5.78%).\n",
      "Batch 34 is ok: 24 out of 346 samples are faulty (6.94%).\n",
      "Batch 35 is ok: 16 out of 346 samples are faulty (4.62%).\n",
      "Batch 36 is ok: 24 out of 346 samples are faulty (6.94%).\n",
      "Batch 37 is ok: 21 out of 346 samples are faulty (6.07%).\n",
      "Batch 38 is ok: 6 out of 346 samples are faulty (1.73%).\n",
      "Batch 39 is ok: 13 out of 346 samples are faulty (3.76%).\n",
      "Batch 40 is ok: 19 out of 346 samples are faulty (5.49%).\n",
      "Batch 41 is ok: 16 out of 346 samples are faulty (4.62%).\n",
      "Batch 42 is ok: 30 out of 346 samples are faulty (8.67%).\n",
      "Batch 43 is ok: 17 out of 346 samples are faulty (4.91%).\n",
      "Batch 44 is ok: 18 out of 346 samples are faulty (5.20%).\n",
      "Batch 45 is ok: 17 out of 346 samples are faulty (4.91%).\n",
      "Batch 46 is ok: 14 out of 346 samples are faulty (4.05%).\n",
      "Batch 47 is ok: 17 out of 346 samples are faulty (4.91%).\n",
      "Batch 48 is ok: 14 out of 346 samples are faulty (4.05%).\n",
      "Batch 49 is ok: 14 out of 346 samples are faulty (4.05%).\n",
      "Total batches with issues: 0 out of 50\n",
      "Total problematic samples: 852 out of 17327 (4.92%)\n",
      "Percentage of data quality issues detected in the test set: 4.92%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "def detect_quality_issues(model, data_loader, threshold):\n",
    "    model.eval()\n",
    "    total_issue_count = 0\n",
    "    total_batches_with_issues = 0\n",
    "    total_samples = 0\n",
    "    current_batch_issues = 0\n",
    "    batch_count = 0\n",
    "    batch_size = len(data_loader.dataset) // 50  # 你希望的批次大小\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, _ in data_loader:\n",
    "            inputs = inputs.to(device)  # 将输入数据移动到正确的设备\n",
    "            recon, mu, logvar = model(inputs)\n",
    "            BCE, KLD = loss_function(recon, inputs, mu, logvar)\n",
    "            total_loss = BCE + KLD  # 计算当前样本的总损失\n",
    "            total_samples += 1\n",
    "\n",
    "            # 判断当前样本是否有问题\n",
    "            if total_loss.item() > threshold:\n",
    "                current_batch_issues += 1\n",
    "\n",
    "            # 当累积样本数达到你设定的批次大小时，评估这个批次\n",
    "            if total_samples % batch_size == 0:\n",
    "                if current_batch_issues >= batch_size * 0.1:  # 判断这个批次是否有超过5%的样本有问题\n",
    "                    print(f\"Batch {batch_count} is problematic: {current_batch_issues} out of {batch_size} samples are faulty ({(current_batch_issues/batch_size * 100):.2f}%).\")\n",
    "                    total_batches_with_issues += 1\n",
    "                    total_issue_count = total_issue_count + current_batch_issues\n",
    "                else:\n",
    "                    total_issue_count = total_issue_count + current_batch_issues\n",
    "                    print(f\"Batch {batch_count} is ok: {current_batch_issues} out of {batch_size} samples are faulty ({(current_batch_issues/batch_size * 100):.2f}%).\")\n",
    "                current_batch_issues = 0\n",
    "                batch_count += 1\n",
    "\n",
    "    total_issue_rate = total_issue_count / total_samples\n",
    "    print(f\"Total batches with issues: {total_batches_with_issues} out of {batch_count}\")\n",
    "    print(f\"Total problematic samples: {total_issue_count} out of {total_samples} ({(total_issue_rate * 100):.2f}%)\")\n",
    "    return total_issue_rate\n",
    "\n",
    "# Example usage\n",
    "# test_dataset = TensorDataset(test_tensor, test_tensor)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "issue_rate = detect_quality_issues(model, test_loader, threshold)\n",
    "print(f\"Percentage of data quality issues detected in the test set: {issue_rate * 100:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c139b017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0 is ok: 88 out of 2051 samples are faulty (4.29%).\n",
      "Batch 1 is ok: 76 out of 2051 samples are faulty (3.71%).\n",
      "Batch 2 is ok: 67 out of 2051 samples are faulty (3.27%).\n",
      "Batch 3 is ok: 50 out of 2051 samples are faulty (2.44%).\n",
      "Batch 4 is ok: 52 out of 2051 samples are faulty (2.54%).\n",
      "Batch 5 is ok: 79 out of 2051 samples are faulty (3.85%).\n",
      "Batch 6 is ok: 83 out of 2051 samples are faulty (4.05%).\n",
      "Batch 7 is ok: 102 out of 2051 samples are faulty (4.97%).\n",
      "Batch 8 is ok: 80 out of 2051 samples are faulty (3.90%).\n",
      "Batch 9 is ok: 104 out of 2051 samples are faulty (5.07%).\n",
      "Batch 10 is ok: 80 out of 2051 samples are faulty (3.90%).\n",
      "Batch 11 is ok: 71 out of 2051 samples are faulty (3.46%).\n",
      "Batch 12 is ok: 76 out of 2051 samples are faulty (3.71%).\n",
      "Batch 13 is ok: 95 out of 2051 samples are faulty (4.63%).\n",
      "Batch 14 is ok: 93 out of 2051 samples are faulty (4.53%).\n",
      "Batch 15 is ok: 89 out of 2051 samples are faulty (4.34%).\n",
      "Batch 16 is ok: 125 out of 2051 samples are faulty (6.09%).\n",
      "Batch 17 is ok: 123 out of 2051 samples are faulty (6.00%).\n",
      "Batch 18 is problematic: 278 out of 2051 samples are faulty (13.55%).\n",
      "Batch 19 is ok: 174 out of 2051 samples are faulty (8.48%).\n",
      "Batch 20 is problematic: 307 out of 2051 samples are faulty (14.97%).\n",
      "Batch 21 is problematic: 293 out of 2051 samples are faulty (14.29%).\n",
      "Batch 22 is ok: 183 out of 2051 samples are faulty (8.92%).\n",
      "Batch 23 is problematic: 340 out of 2051 samples are faulty (16.58%).\n",
      "Batch 24 is problematic: 682 out of 2051 samples are faulty (33.25%).\n",
      "Batch 25 is problematic: 610 out of 2051 samples are faulty (29.74%).\n",
      "Batch 26 is problematic: 814 out of 2051 samples are faulty (39.69%).\n",
      "Batch 27 is problematic: 946 out of 2051 samples are faulty (46.12%).\n",
      "Batch 28 is problematic: 1005 out of 2051 samples are faulty (49.00%).\n",
      "Batch 29 is problematic: 771 out of 2051 samples are faulty (37.59%).\n",
      "Batch 30 is problematic: 847 out of 2051 samples are faulty (41.30%).\n",
      "Batch 31 is problematic: 297 out of 2051 samples are faulty (14.48%).\n",
      "Batch 32 is problematic: 560 out of 2051 samples are faulty (27.30%).\n",
      "Batch 33 is problematic: 587 out of 2051 samples are faulty (28.62%).\n",
      "Batch 34 is problematic: 509 out of 2051 samples are faulty (24.82%).\n",
      "Batch 35 is problematic: 567 out of 2051 samples are faulty (27.65%).\n",
      "Batch 36 is problematic: 602 out of 2051 samples are faulty (29.35%).\n",
      "Batch 37 is problematic: 401 out of 2051 samples are faulty (19.55%).\n",
      "Batch 38 is problematic: 350 out of 2051 samples are faulty (17.06%).\n",
      "Batch 39 is problematic: 342 out of 2051 samples are faulty (16.67%).\n",
      "Batch 40 is problematic: 375 out of 2051 samples are faulty (18.28%).\n",
      "Batch 41 is problematic: 421 out of 2051 samples are faulty (20.53%).\n",
      "Batch 42 is problematic: 420 out of 2051 samples are faulty (20.48%).\n",
      "Batch 43 is problematic: 469 out of 2051 samples are faulty (22.87%).\n",
      "Batch 44 is problematic: 465 out of 2051 samples are faulty (22.67%).\n",
      "Batch 45 is problematic: 418 out of 2051 samples are faulty (20.38%).\n",
      "Batch 46 is problematic: 370 out of 2051 samples are faulty (18.04%).\n",
      "Batch 47 is problematic: 504 out of 2051 samples are faulty (24.57%).\n",
      "Batch 48 is problematic: 557 out of 2051 samples are faulty (27.16%).\n",
      "Batch 49 is problematic: 503 out of 2051 samples are faulty (24.52%).\n",
      "Total batches with issues: 30 out of 50\n",
      "Total problematic samples: 17500 out of 102599 (17.06%)\n",
      "Percentage of data quality issues detected in the test set: 17.06%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "def detect_quality_issues(model, data_loader, threshold):\n",
    "    model.eval()\n",
    "    total_issue_count = 0\n",
    "    total_batches_with_issues = 0\n",
    "    total_samples = 0\n",
    "    current_batch_issues = 0\n",
    "    batch_count = 0\n",
    "    batch_size = len(data_loader.dataset) // 50  # 你希望的批次大小\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, _ in data_loader:\n",
    "            inputs = inputs.to(device)  # 将输入数据移动到正确的设备\n",
    "            recon, mu, logvar = model(inputs)\n",
    "            BCE, KLD = loss_function(recon, inputs, mu, logvar)\n",
    "            total_loss = BCE + KLD  # 计算当前样本的总损失\n",
    "            total_samples += 1\n",
    "\n",
    "            # 判断当前样本是否有问题\n",
    "            if total_loss.item() > threshold:\n",
    "                current_batch_issues += 1\n",
    "\n",
    "            # 当累积样本数达到你设定的批次大小时，评估这个批次\n",
    "            if total_samples % batch_size == 0:\n",
    "                if current_batch_issues >= batch_size * 0.1:  # 判断这个批次是否有超过5%的样本有问题\n",
    "                    print(f\"Batch {batch_count} is problematic: {current_batch_issues} out of {batch_size} samples are faulty ({(current_batch_issues/batch_size * 100):.2f}%).\")\n",
    "                    total_batches_with_issues += 1\n",
    "                    total_issue_count = total_issue_count + current_batch_issues\n",
    "                else:\n",
    "                    total_issue_count = total_issue_count + current_batch_issues\n",
    "                    print(f\"Batch {batch_count} is ok: {current_batch_issues} out of {batch_size} samples are faulty ({(current_batch_issues/batch_size * 100):.2f}%).\")\n",
    "                current_batch_issues = 0\n",
    "                batch_count += 1\n",
    "\n",
    "    total_issue_rate = total_issue_count / total_samples\n",
    "    print(f\"Total batches with issues: {total_batches_with_issues} out of {batch_count}\")\n",
    "    print(f\"Total problematic samples: {total_issue_count} out of {total_samples} ({(total_issue_rate * 100):.2f}%)\")\n",
    "    return total_issue_rate\n",
    "\n",
    "# Example usage\n",
    "\n",
    "\n",
    "\n",
    "# 假设 data_dirty 已经是一个经过预处理的 DataFrame\n",
    "data_dirty_array = data_dirty.values.astype(np.float32)  # 转换为浮点数类型的 NumPy 数组\n",
    "test_dirty_tensor = torch.tensor(data_dirty_array)  \n",
    "test_dirty_dataset = TensorDataset(test_dirty_tensor, test_dirty_tensor)\n",
    "test_dirty_loader = DataLoader(test_dirty_dataset, batch_size=1, shuffle=False) \n",
    "\n",
    "issue_rate = detect_quality_issues(model, test_dirty_loader, threshold)\n",
    "print(f\"Percentage of data quality issues detected in the test set: {issue_rate * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68bd6172",
   "metadata": {},
   "source": [
    "##loop test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "73c55af4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random sample 0 is ok: 329 out of 20520 samples are faulty (1.60%).\n",
      "Random sample 1 is ok: 304 out of 20520 samples are faulty (1.48%).\n",
      "Random sample 2 is ok: 309 out of 20520 samples are faulty (1.51%).\n",
      "Random sample 3 is ok: 315 out of 20520 samples are faulty (1.54%).\n",
      "Random sample 4 is ok: 313 out of 20520 samples are faulty (1.53%).\n",
      "Random sample 5 is ok: 339 out of 20520 samples are faulty (1.65%).\n",
      "Random sample 6 is ok: 319 out of 20520 samples are faulty (1.55%).\n",
      "Random sample 7 is ok: 364 out of 20520 samples are faulty (1.77%).\n",
      "Random sample 8 is ok: 301 out of 20520 samples are faulty (1.47%).\n",
      "Random sample 9 is ok: 328 out of 20520 samples are faulty (1.60%).\n",
      "Random sample 10 is ok: 321 out of 20520 samples are faulty (1.56%).\n",
      "Random sample 11 is ok: 324 out of 20520 samples are faulty (1.58%).\n",
      "Random sample 12 is ok: 339 out of 20520 samples are faulty (1.65%).\n",
      "Random sample 13 is ok: 327 out of 20520 samples are faulty (1.59%).\n",
      "Random sample 14 is ok: 340 out of 20520 samples are faulty (1.66%).\n",
      "Random sample 15 is ok: 339 out of 20520 samples are faulty (1.65%).\n",
      "Random sample 16 is ok: 306 out of 20520 samples are faulty (1.49%).\n",
      "Random sample 17 is ok: 298 out of 20520 samples are faulty (1.45%).\n",
      "Random sample 18 is ok: 332 out of 20520 samples are faulty (1.62%).\n",
      "Random sample 19 is ok: 312 out of 20520 samples are faulty (1.52%).\n",
      "Random sample 20 is ok: 315 out of 20520 samples are faulty (1.54%).\n",
      "Random sample 21 is ok: 345 out of 20520 samples are faulty (1.68%).\n",
      "Random sample 22 is ok: 322 out of 20520 samples are faulty (1.57%).\n",
      "Random sample 23 is ok: 346 out of 20520 samples are faulty (1.69%).\n",
      "Random sample 24 is ok: 336 out of 20520 samples are faulty (1.64%).\n",
      "Random sample 25 is ok: 350 out of 20520 samples are faulty (1.71%).\n",
      "Random sample 26 is ok: 279 out of 20520 samples are faulty (1.36%).\n",
      "Random sample 27 is ok: 333 out of 20520 samples are faulty (1.62%).\n",
      "Random sample 28 is ok: 347 out of 20520 samples are faulty (1.69%).\n",
      "Random sample 29 is ok: 293 out of 20520 samples are faulty (1.43%).\n",
      "Random sample 30 is ok: 334 out of 20520 samples are faulty (1.63%).\n",
      "Random sample 31 is ok: 324 out of 20520 samples are faulty (1.58%).\n",
      "Random sample 32 is ok: 342 out of 20520 samples are faulty (1.67%).\n",
      "Random sample 33 is ok: 322 out of 20520 samples are faulty (1.57%).\n",
      "Random sample 34 is ok: 310 out of 20520 samples are faulty (1.51%).\n",
      "Random sample 35 is ok: 305 out of 20520 samples are faulty (1.49%).\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def detect_quality_issues(model, data_loader, threshold, seed):\n",
    "    model.eval()\n",
    "    current_batch_issues = 0\n",
    "    batch_size = len(data_loader.dataset)  # 这里的批次大小是整个数据集的大小\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, _ in data_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            recon, mu, logvar = model(inputs)\n",
    "            BCE, KLD = loss_function(recon, inputs, mu, logvar)\n",
    "            total_loss = BCE + KLD\n",
    "            # 判断当前样本是否有问题\n",
    "            if total_loss.item() > threshold:\n",
    "                current_batch_issues += 1\n",
    "\n",
    "    # 评估是否有超过5%的样本有问题\n",
    "    if current_batch_issues > batch_size * 0.02:\n",
    "        print(f\"Random sample {seed} is problematic: {current_batch_issues} out of {batch_size} samples are faulty ({(current_batch_issues/batch_size * 100):.2f}%).\")\n",
    "        return True\n",
    "    else:\n",
    "        print(f\"Random sample {seed} is ok: {current_batch_issues} out of {batch_size} samples are faulty ({(current_batch_issues/batch_size * 100):.2f}%).\")\n",
    "        return False\n",
    "\n",
    "# 主函数：执行50次随机采样测试\n",
    "def test_random_samples(model, data_dirty, threshold):\n",
    "    problematic_batches = 0\n",
    "    for seed in range(50):\n",
    "        _, sample_data = train_test_split(data_dirty, test_size=0.2, random_state=seed)  # 随机采样20%\n",
    "        sample_data_array = sample_data.values.astype(np.float32)\n",
    "        sample_tensor = torch.tensor(sample_data_array)\n",
    "        sample_dataset = TensorDataset(sample_tensor, sample_tensor)\n",
    "        test_dirty_loader = DataLoader(sample_dataset, batch_size=1, shuffle=False) \n",
    "        if detect_quality_issues(model, test_dirty_loader, threshold, seed):\n",
    "            problematic_batches += 1\n",
    "\n",
    "    print(f\"Total problematic batches across all tests: {problematic_batches}\")\n",
    "\n",
    "# 假设 model, data_dirty, threshold, device 已经被正确定义和设置\n",
    "test_random_samples(model, data_dirty, threshold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "74364557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random sample 0 is ok: 37 out of 3466 samples are faulty (1.07%).\n",
      "Random sample 1 is ok: 21 out of 3466 samples are faulty (0.61%).\n",
      "Random sample 2 is ok: 35 out of 3466 samples are faulty (1.01%).\n",
      "Random sample 3 is ok: 25 out of 3466 samples are faulty (0.72%).\n",
      "Random sample 4 is ok: 26 out of 3466 samples are faulty (0.75%).\n",
      "Random sample 5 is ok: 32 out of 3466 samples are faulty (0.92%).\n",
      "Random sample 6 is ok: 29 out of 3466 samples are faulty (0.84%).\n",
      "Random sample 7 is ok: 33 out of 3466 samples are faulty (0.95%).\n",
      "Random sample 8 is ok: 23 out of 3466 samples are faulty (0.66%).\n",
      "Random sample 9 is ok: 24 out of 3466 samples are faulty (0.69%).\n",
      "Random sample 10 is ok: 33 out of 3466 samples are faulty (0.95%).\n",
      "Random sample 11 is ok: 27 out of 3466 samples are faulty (0.78%).\n",
      "Random sample 12 is ok: 30 out of 3466 samples are faulty (0.87%).\n",
      "Random sample 13 is ok: 29 out of 3466 samples are faulty (0.84%).\n",
      "Random sample 14 is ok: 27 out of 3466 samples are faulty (0.78%).\n",
      "Random sample 15 is ok: 30 out of 3466 samples are faulty (0.87%).\n",
      "Random sample 16 is ok: 28 out of 3466 samples are faulty (0.81%).\n",
      "Random sample 17 is ok: 28 out of 3466 samples are faulty (0.81%).\n",
      "Random sample 18 is ok: 33 out of 3466 samples are faulty (0.95%).\n",
      "Random sample 19 is ok: 24 out of 3466 samples are faulty (0.69%).\n",
      "Random sample 20 is ok: 35 out of 3466 samples are faulty (1.01%).\n",
      "Random sample 21 is ok: 34 out of 3466 samples are faulty (0.98%).\n",
      "Random sample 22 is ok: 28 out of 3466 samples are faulty (0.81%).\n",
      "Random sample 23 is ok: 31 out of 3466 samples are faulty (0.89%).\n",
      "Random sample 24 is ok: 31 out of 3466 samples are faulty (0.89%).\n",
      "Random sample 25 is ok: 27 out of 3466 samples are faulty (0.78%).\n",
      "Random sample 26 is ok: 31 out of 3466 samples are faulty (0.89%).\n",
      "Random sample 27 is ok: 26 out of 3466 samples are faulty (0.75%).\n",
      "Random sample 28 is ok: 30 out of 3466 samples are faulty (0.87%).\n",
      "Random sample 29 is ok: 22 out of 3466 samples are faulty (0.63%).\n",
      "Random sample 30 is ok: 34 out of 3466 samples are faulty (0.98%).\n",
      "Random sample 31 is ok: 28 out of 3466 samples are faulty (0.81%).\n",
      "Random sample 32 is ok: 35 out of 3466 samples are faulty (1.01%).\n",
      "Random sample 33 is ok: 32 out of 3466 samples are faulty (0.92%).\n",
      "Random sample 34 is ok: 30 out of 3466 samples are faulty (0.87%).\n",
      "Random sample 35 is ok: 27 out of 3466 samples are faulty (0.78%).\n",
      "Random sample 36 is ok: 36 out of 3466 samples are faulty (1.04%).\n",
      "Random sample 37 is ok: 28 out of 3466 samples are faulty (0.81%).\n",
      "Random sample 38 is ok: 38 out of 3466 samples are faulty (1.10%).\n",
      "Random sample 39 is ok: 38 out of 3466 samples are faulty (1.10%).\n",
      "Random sample 40 is ok: 33 out of 3466 samples are faulty (0.95%).\n",
      "Random sample 41 is ok: 27 out of 3466 samples are faulty (0.78%).\n",
      "Random sample 42 is ok: 34 out of 3466 samples are faulty (0.98%).\n",
      "Random sample 43 is ok: 32 out of 3466 samples are faulty (0.92%).\n",
      "Random sample 44 is ok: 27 out of 3466 samples are faulty (0.78%).\n",
      "Random sample 45 is ok: 48 out of 3466 samples are faulty (1.38%).\n",
      "Random sample 46 is ok: 29 out of 3466 samples are faulty (0.84%).\n",
      "Random sample 47 is ok: 31 out of 3466 samples are faulty (0.89%).\n",
      "Random sample 48 is ok: 28 out of 3466 samples are faulty (0.81%).\n",
      "Random sample 49 is ok: 32 out of 3466 samples are faulty (0.92%).\n",
      "Total problematic batches across all tests: 0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def detect_quality_issues(model, data_loader, threshold, seed):\n",
    "    model.eval()\n",
    "    current_batch_issues = 0\n",
    "    batch_size = len(data_loader.dataset)  # 这里的批次大小是整个数据集的大小\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, _ in data_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            recon, mu, logvar = model(inputs)\n",
    "            BCE, KLD = loss_function(recon, inputs, mu, logvar)\n",
    "            total_loss = BCE + KLD\n",
    "            # 判断当前样本是否有问题\n",
    "            if total_loss.item() > threshold:\n",
    "                current_batch_issues += 1\n",
    "\n",
    "    # 评估是否有超过5%的样本有问题\n",
    "    if current_batch_issues > batch_size * 0.02:\n",
    "        print(f\"Random sample {seed} is problematic: {current_batch_issues} out of {batch_size} samples are faulty ({(current_batch_issues/batch_size * 100):.2f}%).\")\n",
    "        return True\n",
    "    else:\n",
    "        print(f\"Random sample {seed} is ok: {current_batch_issues} out of {batch_size} samples are faulty ({(current_batch_issues/batch_size * 100):.2f}%).\")\n",
    "        return False\n",
    "\n",
    "# 主函数：执行50次随机采样测试\n",
    "def test_random_samples(model, data, threshold):\n",
    "    problematic_batches = 0\n",
    "    for seed in range(50):\n",
    "        _, sample_data = train_test_split(data, test_size=0.2, random_state=seed)  # 随机采样20%\n",
    "        ##sample_data_array = sample_data.values.astype(np.float32)\n",
    "        sample_tensor = torch.tensor(sample_data)\n",
    "        sample_dataset = TensorDataset(sample_tensor, sample_tensor)\n",
    "        test_dirty_loader = DataLoader(sample_dataset, batch_size=1, shuffle=False) \n",
    "        if detect_quality_issues(model, test_dirty_loader, threshold, seed):\n",
    "            problematic_batches += 1\n",
    "\n",
    "    print(f\"Total problematic batches across all tests: {problematic_batches}\")\n",
    "\n",
    "# 假设 model, data_dirty, threshold, device 已经被正确定义和设置\n",
    "test_random_samples(model, test_data, threshold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c690c9ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined loss data saved to /home/sdong/experiments/VAE_method/results/combined_loss_data_original.csv.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def record_losses(model, data_loader, seed, device, loss_function):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, _ in data_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            recon, mu, logvar = model(inputs)\n",
    "            BCE, KLD = loss_function(recon, inputs, mu, logvar)\n",
    "            total_loss = BCE + KLD\n",
    "            losses.append(total_loss.item())\n",
    "    \n",
    "    return losses\n",
    "\n",
    "# 主函数：测试两个数据集并将结果保存到同一个CSV文件\n",
    "def test_and_save_combined_losses(model, data_clean, data_dirty, file_name='/home/sdong/experiments/VAE_method/results/combined_loss_data_original.csv'):\n",
    "    seed = 42  # Use a fixed seed for reproducibility\n",
    "    # Process clean data\n",
    "    _, sample_data_clean = train_test_split(data_clean, test_size=2000, train_size=None, random_state=seed)\n",
    "    #sample_clean_array = sample_data_clean.values.astype(np.float32)\n",
    "    sample_clean_tensor = torch.tensor(sample_data_clean)\n",
    "    sample_clean_dataset = TensorDataset(sample_clean_tensor, sample_clean_tensor)\n",
    "    clean_loader = DataLoader(sample_clean_dataset, batch_size=1, shuffle=False)\n",
    "    clean_losses = record_losses(model, clean_loader, seed, device, loss_function)\n",
    "    \n",
    "    # Process dirty data\n",
    "    _, sample_data_dirty = train_test_split(data_dirty, test_size=2000, train_size=None, random_state=seed)\n",
    "    sample_dirty_array = sample_data_dirty.values.astype(np.float32)\n",
    "    sample_dirty_tensor = torch.tensor(sample_dirty_array)\n",
    "    sample_dirty_dataset = TensorDataset(sample_dirty_tensor, sample_dirty_tensor)\n",
    "    dirty_loader = DataLoader(sample_dirty_dataset, batch_size=1, shuffle=False)\n",
    "    dirty_losses = record_losses(model, dirty_loader, seed, device, loss_function)\n",
    "    \n",
    "    # Combine and save to CSV\n",
    "    combined_df = pd.DataFrame({\n",
    "        'Loss_clean_original': clean_losses,\n",
    "        'Loss_dirty_original': dirty_losses\n",
    "    })\n",
    "    combined_df.to_csv(file_name, index=False)\n",
    "    print(f\"Combined loss data saved to {file_name}.\")\n",
    "\n",
    "# 假设 model, data_clean, data_dirty, device, loss_function 已经被正确定义和设置\n",
    "test_and_save_combined_losses(model, test_data, data_dirty)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11473197",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "huggingface_learn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
